import sys
from typing import Dict, Any
from langchain_core.messages import HumanMessage, AIMessage
from langchain_core.prompts import PromptTemplate

from .. import Model, State, logger


def human_reviewer(state: State) -> State:
    """
    Human-in-the-loop reviewer agent for final summary quality assurance.
    Provides bidirectional Q&A functionality for iterative improvement.
    """
    logger.info("human_reviewer")

    llm = Model().llm()
    
    # Get current data
    final_summary = state.get("final_summary", "")
    overview = state.get("overview", "")
    url = state.get("url", "")
    target_report_summaries = state.get("target_report_summaries", [])
    
    # Initialize review session if not exists
    if "review_session" not in state:
        state["review_session"] = {
            "iteration": 0,
            "qa_history": [],
            "original_summary": final_summary,
            "improvements": []
        }
    
    review_session = state["review_session"]
    
    # Display current summary for human review
    print("\n" + "="*80)
    print("ğŸ” HUMAN REVIEW SESSION - FINAL SUMMARY QUALITY CHECK")
    print("="*80)
    _display_current_summary(final_summary, url=url)
    
    if review_session["iteration"] > 0:
        print(f"\nğŸ“Š Review Iteration: {review_session['iteration']}")
        print(f"ğŸ’¬ Previous Q&A exchanges: {len(review_session['qa_history'])}")
    
    # Interactive review loop
    print("\nğŸ’¡ Tips: æ”¹è¡Œã®ã¿ï¼ˆç©ºå…¥åŠ›ï¼‰ã§å…¨ç”»é¢ã‚¨ãƒ‡ã‚£ã‚¿ãŒèµ·å‹•ã—ã¾ã™")
    
    while True:
        try:
            user_input = _enhanced_input("\nYou>")
            
            # Check for empty input (just Enter) - launch fullscreen editor
            if not user_input.strip():
                print("\nğŸ“ å…¨ç”»é¢ã‚¨ãƒ‡ã‚£ã‚¿ã‚’èµ·å‹•ã—ã¾ã™ã€‚è©³ç´°ãªæ”¹å–„è¦æ±‚ã‚„ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚")
                user_input = _enhanced_input("è©³ç´°ãªæ”¹å–„è¦æ±‚", fullscreen=True)
                if not user_input.strip():
                    print("âŒ Please provide some input.")
                    continue
            
            # Check for help request
            if user_input.lower() in ["help", "h", "ãƒ˜ãƒ«ãƒ—"]:
                _display_help()
                continue
            
            # Use LLM to classify the user's intent
            action_type = _classify_user_intent(llm, user_input)
            
            if action_type == "approve":
                # Check character limit before approval
                total_chars = len(final_summary) + len(url) + 1
                if total_chars <= 300:
                    # Approve and finish
                    print("\nâœ… Summary approved! Finishing review session.")
                    state["review_approved"] = True
                    break
                else:
                    # Generate shortened version
                    print(f"\nğŸ“ Summary is {total_chars} chars (exceeds 300 limit).")
                    print("âœ¨ Generating shortened version...")
                    shortened_summary = _generate_shortened_summary(
                        llm, final_summary, overview, target_report_summaries, url
                    )
                    
                    # Update the summary
                    final_summary = shortened_summary
                    state["final_summary"] = final_summary
                    review_session["improvements"].append({
                        "request": f"Auto-shorten from {total_chars} to fit 300 char limit",
                        "result": shortened_summary
                    })
                    
                    _display_current_summary(final_summary, url)
                    print("âœ… Shortened version generated!")
                    print("\nğŸ’¬ Please review the shortened version. You can approve, improve further, or provide feedback.")
                
            elif action_type == "question":
                # Extract question or ask for clarification
                question = _extract_question_from_input(llm, user_input)
                if question:
                    ai_response = _ask_ai_question(llm, question, final_summary, overview, target_report_summaries)
                    print(f"\nAI> {ai_response}")
                    
                    # Record Q&A
                    review_session["qa_history"].append({
                        "type": "human_question",
                        "content": question,
                        "ai_response": ai_response
                    })
                else:
                    print("â“ Could not extract a clear question.")
                
            elif action_type == "improve":
                # Extract improvement request
                improvement_request = _extract_improvement_request(llm, user_input)
                if improvement_request:
                    print("âœ¨ Generating improved summary...")
                    improved_summary = _generate_improved_summary(
                        llm, final_summary, improvement_request, overview, target_report_summaries, url
                    )
                    
                    # Apply the improvement immediately
                    final_summary = improved_summary
                    state["final_summary"] = final_summary
                    review_session["improvements"].append({
                        "request": improvement_request,
                        "result": improved_summary
                    })
                    
                    _display_current_summary(final_summary, url)
                    print("âœ… Improvement applied!")
                else:
                    print("ğŸ“ Could not extract a clear improvement request.")
                
            elif action_type == "source":
                # Review source materials
                _display_source_materials(overview, target_report_summaries)
                

            elif action_type == "cancel":
                # Cancel review
                print("\nâŒ Review cancelled. Using current summary.")
                state["review_approved"] = False
                break
                
            else:
                print("âŒ Could not understand your request.")
                
        except KeyboardInterrupt:
            print("\n\nâš ï¸  Review interrupted. Using current summary.")
            state["review_approved"] = False
            break
        except EOFError:
            print("\n\nâš ï¸  Input ended. Using current summary.")
            state["review_approved"] = False
            break
    
    # Update review session
    review_session["iteration"] += 1
    state["review_session"] = review_session
    
    # Display final confirmed summary
    print("\nâœ… Review completed!")
    _display_current_summary(final_summary, url=url)
    
    # Update messages with final reviewed summary
    message = HumanMessage(content=f"{final_summary}\n{url}")
    
    # Add review metadata to state
    state["review_completed"] = True
    state["final_review_summary"] = final_summary
    
    return {**state, "messages": [message]}


def _ask_ai_question(llm, question: str, summary: str, overview: str, summaries: list) -> str:
    """Ask AI a specific question about the summary"""
    
    source_context = ""
    if summaries:
        source_context = "\n\n".join([
            f"ã€{s.name}ã€‘\n{s.content}" for s in summaries if s.content
        ])
    
    prompt = PromptTemplate(
        input_variables=["question", "summary", "overview", "source_context"],
        template="""
        äººé–“ã‹ã‚‰ä»¥ä¸‹ã®è³ªå•ã‚’å—ã‘ã¾ã—ãŸã€‚è¦ç´„ã®å†…å®¹ã¨å…ƒè³‡æ–™ã‚’å‚ç…§ã—ã¦ã€æ­£ç¢ºã§è©³ç´°ãªå›ç­”ã‚’ã—ã¦ãã ã•ã„ã€‚

        **è³ªå•:** {question}

        **ç¾åœ¨ã®è¦ç´„:**
        {summary}

        **æ¦‚è¦æƒ…å ±:**
        {overview}

        **å…ƒè³‡æ–™ã®è¦ç´„:**
        {source_context}

        **å›ç­”è¦ä»¶:**
        - è³ªå•ã«å¯¾ã—ã¦å…·ä½“çš„ã§æ­£ç¢ºãªå›ç­”ã‚’ã™ã‚‹
        - æ ¹æ‹ ã¨ãªã‚‹è³‡æ–™ã‚„æƒ…å ±ã‚’æ˜ç¤ºã™ã‚‹
        - ä¸æ˜ãªç‚¹ãŒã‚ã‚Œã°ç´ ç›´ã«ã€Œä¸æ˜ã€ã¨ç­”ãˆã‚‹
        - æ¨æ¸¬ã‚„å‰µä½œã¯è¡Œã‚ãªã„
        - å¿…è¦ã«å¿œã˜ã¦è¿½åŠ ã®è³ªå•ã‚’ææ¡ˆã™ã‚‹
        """
    )
    
    try:
        response = llm.invoke(prompt.format(
            question=question,
            summary=summary,
            overview=overview,
            source_context=source_context
        ))
        return response.content.strip()
    except Exception as e:
        logger.error(f"Error in AI question answering: {str(e)}")
        return f"ç”³ã—è¨³ã‚ã‚Šã¾ã›ã‚“ãŒã€è³ªå•ã¸ã®å›ç­”ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}"


def _generate_improved_summary(llm, current_summary: str, improvement_request: str, 
                             overview: str, summaries: list, url: str) -> str:
    """Generate an improved summary based on human feedback"""
    
    source_context = ""
    if summaries:
        source_context = "\n\n".join([
            f"ã€{s.name}ã€‘\n{s.content}" for s in summaries if s.content
        ])
    
    # Calculate max characters based on URL length
    url_length = len(url)
    max_chars = max(50, 300 - url_length - 1)
    
    prompt = PromptTemplate(
        input_variables=["current_summary", "improvement_request", "overview", "source_context", "max_chars"],
        template="""
        ç¾åœ¨ã®è¦ç´„ã«å¯¾ã—ã¦æ”¹å–„è¦æ±‚ãŒã‚ã‚Šã¾ã—ãŸã€‚è¦æ±‚ã«å¾“ã£ã¦è¦ç´„ã‚’æ”¹å–„ã—ã¦ãã ã•ã„ã€‚

        **æ”¹å–„è¦æ±‚:**
        {improvement_request}

        **ç¾åœ¨ã®è¦ç´„:**
        {current_summary}

        **æ¦‚è¦æƒ…å ±:**
        {overview}

        **å…ƒè³‡æ–™ã®è¦ç´„:**
        {source_context}

        **æ”¹å–„è¦ä»¶:**
        - æ”¹å–„è¦æ±‚ã«å…·ä½“çš„ã«å¯¾å¿œã™ã‚‹
        - {max_chars}æ–‡å­—ä»¥ä¸‹ã§ä½œæˆã™ã‚‹
        - å®Ÿéš›ã«æ›¸ã‹ã‚Œã¦ã„ã‚‹å†…å®¹ã®ã¿ã‚’ä½¿ç”¨ã™ã‚‹
        - æ¨æ¸¬ã‚„å‰µä½œã¯è¡Œã‚ãªã„
        - é‡è¦ãªæƒ…å ±ã‚’æ¼ã‚‰ã•ãªã„
        - èª­ã¿ã‚„ã™ãè«–ç†çš„ãªæ§‹æˆã«ã™ã‚‹
        - ä¼šè­°åã‚„è³‡æ–™åã‚’é©åˆ‡ã«å«ã‚ã‚‹
        """
    )
    
    try:
        response = llm.invoke(prompt.format(
            current_summary=current_summary,
            improvement_request=improvement_request,
            overview=overview,
            source_context=source_context,
            max_chars=max_chars
        ))
        improved_summary = response.content.strip()
        
        return improved_summary
    except Exception as e:
        logger.error(f"Error in summary improvement: {str(e)}")
        return current_summary


def _generate_shortened_summary(llm, current_summary: str, overview: str, summaries: list, url: str) -> str:
    """Generate a shortened version of the summary to fit 300 character limit"""
    
    source_context = ""
    if summaries:
        source_context = "\n\n".join([
            f"ã€{s.name}ã€‘\n{s.content}" for s in summaries if s.content
        ])
    
    # Calculate max characters based on URL length
    url_length = len(url)
    max_chars = max(50, 300 - url_length - 1)
    
    prompt = PromptTemplate(
        input_variables=["current_summary", "overview", "source_context", "max_chars"],
        template="""
        æ‰¿èªã•ã‚ŒãŸè¦ç´„ãŒæ–‡å­—æ•°åˆ¶é™ã‚’è¶…ãˆã¦ã„ã‚‹ãŸã‚ã€çŸ­ç¸®ç‰ˆã‚’ä½œæˆã—ã¦ãã ã•ã„ã€‚
        äººé–“ãŒæ‰¿èªã—ãŸå†…å®¹ã®æ„å›³ã¨é‡è¦ãªæƒ…å ±ã‚’ä¿æŒã—ãªãŒã‚‰ã€æ–‡å­—æ•°åˆ¶é™å†…ã«åã‚ã¦ãã ã•ã„ã€‚

        **æ‰¿èªã•ã‚ŒãŸè¦ç´„:**
        {current_summary}

        **æ¦‚è¦æƒ…å ±:**
        {overview}

        **å…ƒè³‡æ–™ã®è¦ç´„:**
        {source_context}

        **çŸ­ç¸®è¦ä»¶:**
        - {max_chars}æ–‡å­—ä»¥ä¸‹ã§ä½œæˆã™ã‚‹ï¼ˆå³å®ˆï¼‰
        - æ‰¿èªã•ã‚ŒãŸè¦ç´„ã®ä¸»è¦ãªå†…å®¹ã¨æ„å›³ã‚’ä¿æŒã™ã‚‹
        - æœ€ã‚‚é‡è¦ãªæƒ…å ±ã‚’å„ªå…ˆçš„ã«å«ã‚ã‚‹
        - å®Ÿéš›ã«æ›¸ã‹ã‚Œã¦ã„ã‚‹å†…å®¹ã®ã¿ã‚’ä½¿ç”¨ã™ã‚‹
        - æ¨æ¸¬ã‚„å‰µä½œã¯è¡Œã‚ãªã„
        - èª­ã¿ã‚„ã™ãè«–ç†çš„ãªæ§‹æˆã«ã™ã‚‹
        - ä¼šè­°åã‚„è³‡æ–™åã‚’é©åˆ‡ã«å«ã‚ã‚‹
        - äººé–“ã®æ”¹å–„æ„å›³ã‚’å¯èƒ½ãªé™ã‚Šåæ˜ ã™ã‚‹
        """
    )
    
    try:
        response = llm.invoke(prompt.format(
            current_summary=current_summary,
            overview=overview,
            source_context=source_context,
            max_chars=max_chars
        ))
        shortened_summary = response.content.strip()
        
        return shortened_summary
    except Exception as e:
        logger.error(f"Error in summary shortening: {str(e)}")
        return current_summary





def _classify_user_intent(llm, user_input: str) -> str:
    """Classify user's natural language input into action categories"""
    
    prompt = PromptTemplate(
        input_variables=["user_input"],
        template="""
        ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è‡ªç„¶è¨€èªå…¥åŠ›ã‚’æ…é‡ã«åˆ†æã—ã¦ã€æœ€ã‚‚é©åˆ‡ãªã‚¢ã‚¯ã‚·ãƒ§ãƒ³åˆ†é¡ã‚’æ±ºå®šã—ã¦ãã ã•ã„ã€‚

        **å…¥åŠ›:** {user_input}

        **åˆ†ææ‰‹é †:**
        1. ã¾ãšå…¥åŠ›ã®ä¸»è¦ãªæ„å›³ã‚’ç‰¹å®šã™ã‚‹
        2. å„ã‚«ãƒ†ã‚´ãƒªã®å®šç¾©ã¨ç…§ã‚‰ã—åˆã‚ã›ã‚‹
        3. æœ€ã‚‚é©åˆåº¦ã®é«˜ã„ã‚«ãƒ†ã‚´ãƒªã‚’é¸æŠã™ã‚‹

        **åˆ†é¡ã‚«ãƒ†ã‚´ãƒªï¼ˆå„ªå…ˆåº¦é †ï¼‰:**
        
        **approve** - è¦ç´„ã‚’æ‰¿èªãƒ»å®Œäº†ã™ã‚‹
        ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰: ã€Œæ‰¿èªã€ã€ŒOKã€ã€Œè‰¯ã„ã€ã€Œå®Œäº†ã€ã€Œçµ‚äº†ã€ã€Œã¯ã„ã€ã€Œã„ã„ã­ã€ã€Œå¤§ä¸ˆå¤«ã€ã€Œå•é¡Œãªã„ã€ã€Œæ¡ç”¨ã€
        åˆ¤å®šåŸºæº–: ç¾åœ¨ã®è¦ç´„ã«æº€è¶³ã—ã€ä½œæ¥­ã‚’å®Œäº†ã—ãŸã„æ„å›³ãŒæ˜ç¢º
        
        **question** - AIã«è³ªå•ã™ã‚‹
        ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰: ã€Œè³ªå•ã€ã€Œãªãœã€ã€Œã©ã†ã—ã¦ã€ã€Œæ•™ãˆã¦ã€ã€Œï¼Ÿã€ã€Œæ ¹æ‹ ã€ã€Œç†ç”±ã€ã€Œè©³ã—ãã€
        åˆ¤å®šåŸºæº–: ç–‘å•ç¬¦ãŒã‚ã‚‹ã‹ã€èª¬æ˜ã‚„è©³ç´°ã‚’æ±‚ã‚ã‚‹æ„å›³ãŒæ˜ç¢º
        
        **source** - å…ƒè³‡æ–™ã‚’ç¢ºèªã™ã‚‹
        ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰: ã€Œè³‡æ–™ã€ã€Œã‚½ãƒ¼ã‚¹ã€ã€Œå…ƒã€ã€Œç¢ºèªã€ã€Œè¦‹ãŸã„ã€ã€ŒåŸæ–‡ã€ã€Œå‡ºå…¸ã€
        åˆ¤å®šåŸºæº–: å…ƒã¨ãªã‚‹è³‡æ–™ã‚„æ–‡æ›¸ã‚’è¦‹ãŸã„æ„å›³ãŒæ˜ç¢º
        
        **cancel** - ãƒ¬ãƒ“ãƒ¥ãƒ¼ã‚’ã‚­ãƒ£ãƒ³ã‚»ãƒ«ã™ã‚‹
        ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰: ã€Œã‚­ãƒ£ãƒ³ã‚»ãƒ«ã€ã€Œä¸­æ­¢ã€ã€Œã‚„ã‚ã‚‹ã€ã€Œçµ‚ã‚ã‚Šã€ã€Œåœæ­¢ã€ã€Œä¸­æ–­ã€
        åˆ¤å®šåŸºæº–: ä½œæ¥­ã‚’ä¸­æ­¢ã—ãŸã„æ„å›³ãŒæ˜ç¢º
        
                 **improve** - æ”¹å–„ã‚’è¦æ±‚ã™ã‚‹ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆï¼‰
         ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰: ã€Œæ”¹å–„ã€ã€Œä¿®æ­£ã€ã€Œå¤‰æ›´ã€ã€Œç›´ã—ã¦ã€ã€Œã‚‚ã£ã¨ã€ã€Œä½œã‚Šç›´ã—ã€ã€Œå…¨é¢çš„ã«ã€ã€Œè¿½åŠ ã€ã€Œå‰Šé™¤ã€
         ã‚¢ãƒ‰ãƒã‚¤ã‚¹: ã€Œã€‡ã€‡ã¯Ã—Ã—ã§ã™ã€ã€Œå®Ÿéš›ã«ã¯ã€ã€Œæ­£ç¢ºã«ã¯ã€ã€Œè£œè¶³ã™ã‚‹ã¨ã€ã€Œã¡ãªã¿ã«ã€
         åˆ¤å®šåŸºæº–: ä¸Šè¨˜ä»¥å¤–ã®ã™ã¹ã¦ã€å¤‰æ›´ãƒ»æ”¹å–„ã‚’æ±‚ã‚ã‚‹æ„å›³ã€ã¾ãŸã¯æƒ…å ±æä¾›ãƒ»ã‚¢ãƒ‰ãƒã‚¤ã‚¹

                 **åˆ¤å®šãƒ«ãƒ¼ãƒ«:**
         - è¤‡æ•°ã®æ„å›³ãŒæ··åœ¨ã™ã‚‹å ´åˆã¯ã€æœ€ã‚‚å¼·ã„æ„å›³ã‚’é¸ã¶
         - æ›–æ˜§ãªå ´åˆã‚„åˆ¤æ–­ã«è¿·ã†å ´åˆã¯ "improve" ã‚’é¸ã¶
         - å˜èªã ã‘ã§ãªãã€æ–‡è„ˆã‚„å…¨ä½“çš„ãªæ„å›³ã‚’é‡è¦–ã™ã‚‹
         - ãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒä½•ã‹ã‚’å¤‰ãˆãŸã„ãƒ»è‰¯ãã—ãŸã„ã¨æ€ã£ã¦ã„ã‚‹å ´åˆã¯ "improve"
         - ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‹ã‚‰ã®æƒ…å ±æä¾›ã‚„ã‚¢ãƒ‰ãƒã‚¤ã‚¹ã€äº‹å®Ÿã®è¨‚æ­£ã‚‚ "improve" ã¨ã—ã¦æ‰±ã†
         - ã€Œã€‡ã€‡ã¯Ã—Ã—ã§ã™ã€ã®ã‚ˆã†ãªæƒ…å ±æä¾›ã¯è¦ç´„æ”¹å–„ã®ãŸã‚ã®ææ–™ã¨ã—ã¦ "improve"

        **å‡ºåŠ›:** 5ã¤ã®ã‚«ãƒ†ã‚´ãƒªï¼ˆapprove, question, improve, source, cancelï¼‰ã®ã†ã¡1ã¤ã®ã¿ã‚’è‹±èªå°æ–‡å­—ã§è¿”ã™
        """
    )
    
    try:
        response = llm.invoke(prompt.format(user_input=user_input))
        action_type = response.content.strip().lower()
        
        # Validate the response
        valid_actions = ["approve", "question", "improve", "source", "cancel"]
        if action_type in valid_actions:
            return action_type
        else:
            return "improve"  # Default to improve for unclear inputs
    except Exception as e:
        logger.error(f"Error in intent classification: {str(e)}")
        return "unknown"  # Return unknown on error


def _extract_question_from_input(llm, user_input: str) -> str:
    """Extract a clear question from user's natural language input"""
    
    prompt = PromptTemplate(
        input_variables=["user_input"],
        template="""
        ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®å…¥åŠ›ã‹ã‚‰è³ªå•ã‚’æŠ½å‡ºã—ã¦ãã ã•ã„ã€‚

        **å…¥åŠ›:** {user_input}

        **æŠ½å‡ºè¦ä»¶:**
        - æ˜ç¢ºãªè³ªå•æ–‡ã¨ã—ã¦æ•´ç†ã™ã‚‹
        - ã€Œï¼Ÿã€ã§çµ‚ã‚ã‚‹ç–‘å•æ–‡ã«ã™ã‚‹
        - è¦ç´„ã«é–¢ã™ã‚‹è³ªå•ã¨ã—ã¦é©åˆ‡ã«æ•´å½¢ã™ã‚‹
        - è³ªå•ãŒä¸æ˜ç¢ºãªå ´åˆã¯ç©ºæ–‡å­—åˆ—ã‚’è¿”ã™

        **ä¾‹:**
        - å…¥åŠ›: "ã“ã®éƒ¨åˆ†ã«ã¤ã„ã¦è©³ã—ãæ•™ãˆã¦" â†’ å‡ºåŠ›: "ã“ã®éƒ¨åˆ†ã«ã¤ã„ã¦è©³ã—ãæ•™ãˆã¦ãã ã•ã„ï¼Ÿ"
        - å…¥åŠ›: "ãªãœã“ã®çµè«–ã«ãªã£ãŸã®" â†’ å‡ºåŠ›: "ãªãœã“ã®çµè«–ã«ãªã£ãŸã®ã§ã™ã‹ï¼Ÿ"
        - å…¥åŠ›: "æ ¹æ‹ ã¯" â†’ å‡ºåŠ›: "ã“ã®è¦ç´„ã®æ ¹æ‹ ã¯ä½•ã§ã™ã‹ï¼Ÿ"

        **å‡ºåŠ›:** è³ªå•æ–‡ã®ã¿ï¼ˆèª¬æ˜ä¸è¦ï¼‰
        """
    )
    
    try:
        response = llm.invoke(prompt.format(user_input=user_input))
        question = response.content.strip()
        return question if question and question != "ç©ºæ–‡å­—åˆ—" else ""
    except Exception as e:
        logger.error(f"Error in question extraction: {str(e)}")
        return ""


def _extract_improvement_request(llm, user_input: str) -> str:
    """Extract improvement request from user's natural language input"""
    
    prompt = PromptTemplate(
        input_variables=["user_input"],
        template="""
        ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®å…¥åŠ›ã‹ã‚‰æ”¹å–„è¦æ±‚ã‚’æŠ½å‡ºã—ã¦ãã ã•ã„ã€‚

        **å…¥åŠ›:** {user_input}

        **æŠ½å‡ºè¦ä»¶:**
        - å…·ä½“çš„ãªæ”¹å–„æŒ‡ç¤ºã¨ã—ã¦æ•´ç†ã™ã‚‹
        - ä½•ã‚’ã©ã®ã‚ˆã†ã«æ”¹å–„ã—ãŸã„ã‹ã‚’æ˜ç¢ºã«ã™ã‚‹
        - è¦ç´„ã®æ”¹å–„ã«é–¢ã™ã‚‹æŒ‡ç¤ºã¨ã—ã¦é©åˆ‡ã«æ•´å½¢ã™ã‚‹
        - æ”¹å–„è¦æ±‚ãŒä¸æ˜ç¢ºãªå ´åˆã¯ç©ºæ–‡å­—åˆ—ã‚’è¿”ã™

        **ä¾‹:**
        - å…¥åŠ›: "ã‚‚ã£ã¨ç°¡æ½”ã«ã—ã¦" â†’ å‡ºåŠ›: "è¦ç´„ã‚’ã‚ˆã‚Šç°¡æ½”ã§èª­ã¿ã‚„ã™ãã—ã¦ãã ã•ã„"
        - å…¥åŠ›: "æ•°å­—ã‚’è¿½åŠ ã—ã¦" â†’ å‡ºåŠ›: "å…·ä½“çš„ãªæ•°å­—ã‚„ãƒ‡ãƒ¼ã‚¿ã‚’è¿½åŠ ã—ã¦ãã ã•ã„"
        - å…¥åŠ›: "çµè«–ã‚’æ˜ç¢ºã«" â†’ å‡ºåŠ›: "çµè«–éƒ¨åˆ†ã‚’ã‚ˆã‚Šæ˜ç¢ºã«è¡¨ç¾ã—ã¦ãã ã•ã„"

        **å‡ºåŠ›:** æ”¹å–„è¦æ±‚æ–‡ã®ã¿ï¼ˆèª¬æ˜ä¸è¦ï¼‰
        """
    )
    
    try:
        response = llm.invoke(prompt.format(user_input=user_input))
        improvement = response.content.strip()
        return improvement if improvement and improvement != "ç©ºæ–‡å­—åˆ—" else ""
    except Exception as e:
        logger.error(f"Error in improvement extraction: {str(e)}")
        return ""





def _display_current_summary(final_summary: str, url: str) -> None:
    """ç¾åœ¨ã®ã‚µãƒãƒªãƒ¼ã‚’è¡¨ç¤ºã™ã‚‹"""
    summary_chars = len(final_summary)
    url_chars = len(url)
    # è¦ç´„ + æ”¹è¡Œ1æ–‡å­— + URL = åˆè¨ˆæ–‡å­—æ•°
    total_chars = summary_chars + url_chars + 1
    
    print(f"\nğŸ“„ Current Summary (summary: {summary_chars}, URL: {url_chars}, total: {total_chars} chars):")
    print("-" * 50)
    print(final_summary)
    print("-" * 50)
    print(f"ğŸ”— URL: {url}")






def _fullscreen_editor(prompt_text: str, default: str = "") -> str:
    """Full-screen editor using prompt_toolkit"""
    try:
        from prompt_toolkit.application import Application
        from prompt_toolkit.buffer import Buffer
        from prompt_toolkit.layout.containers import HSplit, Window
        from prompt_toolkit.layout.controls import BufferControl, FormattedTextControl
        from prompt_toolkit.layout.layout import Layout
        from prompt_toolkit.key_binding import KeyBindings
        from prompt_toolkit.formatted_text import HTML
        import os
        
        # Create buffer for text input
        from prompt_toolkit.document import Document
        buffer = Buffer(multiline=True, document=Document(default))
        
        # Create key bindings
        kb = KeyBindings()
        
        @kb.add('c-s')  # Ctrl+S to save and exit
        def _(event):
            event.app.exit(result=buffer.text)
        
        @kb.add('c-q')  # Ctrl+Q to quit without saving
        def _(event):
            event.app.exit(result=default)
        
        @kb.add('c-x', 'c-c')  # Ctrl+X Ctrl+C to save and exit
        def _(event):
            event.app.exit(result=buffer.text)
        
        @kb.add('c-c')  # Ctrl+C to raise KeyboardInterrupt
        def _(event):
            raise KeyboardInterrupt()
        
        # Help overlay state
        help_visible = [False]  # Use list to make it mutable in nested function
        
        @kb.add('c-g')  # Ctrl+G for help
        def _(event):
            # Toggle help overlay
            help_visible[0] = not help_visible[0]
            event.app.invalidate()  # Refresh display
        
        # Create dynamic status line
        def get_status_text():
            line_count = buffer.document.line_count
            cursor_line = buffer.document.cursor_position_row + 1
            cursor_col = buffer.document.cursor_position_col + 1
            char_count = len(buffer.text)
            return f'è¡Œ {cursor_line}/{line_count}  åˆ— {cursor_col}  æ–‡å­—æ•° {char_count}'
        
        # Help content function
        def get_help_content():
            if help_visible[0]:
                return HTML('''<style bg="ansiyellow" fg="ansiblack">
=== å…¨ç”»é¢ã‚¨ãƒ‡ã‚£ã‚¿ ãƒ˜ãƒ«ãƒ— ===

ã‚­ãƒ¼ãƒã‚¤ãƒ³ãƒ‰:
^S (Ctrl+S)     : ä¿å­˜ã—ã¦çµ‚äº†
^Q (Ctrl+Q)     : ã‚­ãƒ£ãƒ³ã‚»ãƒ«ï¼ˆä¿å­˜ã—ãªã„ï¼‰
^G (Ctrl+G)     : ã“ã®ãƒ˜ãƒ«ãƒ—ã‚’è¡¨ç¤º/éè¡¨ç¤º
^C (Ctrl+C)     : å¼·åˆ¶çµ‚äº†

ç·¨é›†æ©Ÿèƒ½:
- é€šå¸¸ã®æ–‡å­—å…¥åŠ›ã€å‰Šé™¤ã€æ”¹è¡ŒãŒå¯èƒ½
- æ—¥æœ¬èªå…¥åŠ›å¯¾å¿œ
- ä¸Šä¸‹å·¦å³çŸ¢å°ã‚­ãƒ¼ã§ã‚«ãƒ¼ã‚½ãƒ«ç§»å‹•
- Home/End ã‚­ãƒ¼ã§è¡Œã®å§‹ç«¯/çµ‚ç«¯ã¸ç§»å‹•

ã“ã®ã‚¨ãƒ‡ã‚£ã‚¿ã§é•·æ–‡ã®æ”¹å–„è¦æ±‚ã‚„
ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ã‚’å¿«é©ã«å…¥åŠ›ã§ãã¾ã™ã€‚

ã‚‚ã†ä¸€åº¦ ^G ã‚’æŠ¼ã™ã¨ãƒ˜ãƒ«ãƒ—ã‚’é–‰ã˜ã¾ã™
=== ãƒ˜ãƒ«ãƒ—çµ‚äº† ===
</style>''')
            else:
                return HTML('')
        
        # Create layout with nano-style interface
        main_content = [
            # Header with title
            Window(
                content=FormattedTextControl(
                    HTML(f'<style bg="ansiblue" fg="ansiwhite"><b> å…¨ç”»é¢ã‚¨ãƒ‡ã‚£ã‚¿ - {prompt_text} </b></style>')
                ),
                height=1,
                dont_extend_height=True,
            ),
            # Main editing area - takes remaining space
            Window(
                content=BufferControl(buffer=buffer),
                wrap_lines=True,
            ),
        ]
        
        # Add help overlay if visible
        help_window = Window(
            content=FormattedTextControl(get_help_content),
            height=lambda: 18 if help_visible[0] else 0,
            dont_extend_height=True,
        )
        main_content.append(help_window)
        
        # Add status and help bar
        main_content.extend([
            # Status line
            Window(
                content=FormattedTextControl(
                    lambda: HTML(f'<style bg="ansigray" fg="ansiwhite"> {get_status_text()} </style>')
                ),
                height=1,
                dont_extend_height=True,
            ),
            # Bottom help bar (nano-style)
            Window(
                content=FormattedTextControl(
                    HTML('<style bg="ansiwhite" fg="ansiblack">'
                         ' ^S ä¿å­˜çµ‚äº†   ^Q ã‚­ãƒ£ãƒ³ã‚»ãƒ«   ^G ãƒ˜ãƒ«ãƒ—   ^C ä¸­æ–­ '
                         '</style>')
                ),
                height=1,
                dont_extend_height=True,
            ),
        ])
        
        root_container = HSplit(main_content)
        
        layout = Layout(root_container)
        
        # Create application
        app = Application(
            layout=layout,
            key_bindings=kb,
            full_screen=True,
            mouse_support=False  # WSL2ç’°å¢ƒã§ã®å®‰å®šæ€§ã®ãŸã‚ç„¡åŠ¹åŒ–
        )
        
        # Run the application
        result = app.run()
        return result.strip() if result else default
        
    except ImportError as e:
        print(f"âš ï¸  prompt_toolkitãŒåˆ©ç”¨ã§ãã¾ã›ã‚“: {e}")
        print("ğŸ“ æ¨™æº–å…¥åŠ›ã‚’ä½¿ç”¨ã—ã¾ã™ã€‚")
        return _safe_input_fallback(prompt_text, default)
    except Exception as e:
        print(f"âš ï¸  å…¨ç”»é¢ã‚¨ãƒ‡ã‚£ã‚¿ã§ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
        print(f"   ã‚¨ãƒ©ãƒ¼ã®ç¨®é¡: {type(e).__name__}")
        print(f"   è©³ç´°: {str(e)}")
        print("ğŸ“ æ¨™æº–å…¥åŠ›ã‚’ä½¿ç”¨ã—ã¾ã™ã€‚")
        return _safe_input_fallback(prompt_text, default)


def _is_wsl_environment() -> bool:
    """Check if running in WSL environment"""
    import os
    try:
        # Check multiple WSL indicators
        return (
            'microsoft' in os.uname().release.lower() or
            'wsl' in os.environ.get('WSL_DISTRO_NAME', '').lower() or
            os.path.exists('/proc/version') and 'microsoft' in open('/proc/version').read().lower()
        )
    except:
        return False


def _enhanced_input(prompt_text: str, fullscreen: bool = False, default: str = "") -> str:
    """Enhanced input with prompt_toolkit support for Japanese input"""
    
    # Full-screen editor mode
    if fullscreen:
        return _fullscreen_editor(prompt_text, default)
    
    try:
        from prompt_toolkit import prompt
        from prompt_toolkit.history import InMemoryHistory
        
        # Create history for this session
        history = InMemoryHistory()
        result = prompt(f"{prompt_text} ", history=history, default=default)
        
        # If empty input, launch fullscreen editor
        if not result.strip():
            editor_result = _fullscreen_editor(prompt_text.rstrip(': '), default)
            if editor_result is not None:
                return editor_result
            else:
                print("ã‚­ãƒ£ãƒ³ã‚»ãƒ«ã•ã‚Œã¾ã—ãŸã€‚")
                return ""
        
        return result.strip()
        
    except ImportError:
        # prompt_toolkitãŒåˆ©ç”¨ã§ããªã„å ´åˆã®ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
        print("âš ï¸  prompt_toolkitãŒåˆ©ç”¨ã§ãã¾ã›ã‚“ã€‚æ¨™æº–å…¥åŠ›ã‚’ä½¿ç”¨ã—ã¾ã™ã€‚")
        return _safe_input_fallback(prompt_text, default)
    except (EOFError, KeyboardInterrupt):
        # Re-raise these as they should be handled by the main loop
        raise


def _safe_input_fallback(prompt: str, default: str = "") -> str:
    """Fallback input function when prompt_toolkit is not available"""
    try:
        return input(prompt).strip()
    except UnicodeDecodeError as e:
        print(f"âŒ æ–‡å­—ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
        print("ğŸ’¡ å…¥åŠ›ã«ä½¿ç”¨ã§ããªã„æ–‡å­—ãŒå«ã¾ã‚Œã¦ã„ã¾ã™ã€‚")
        return default
    except (EOFError, KeyboardInterrupt):
        # Re-raise these as they should be handled by the main loop
        raise


def _safe_input(prompt: str, default: str = "") -> str:
    """Wrapper function for backward compatibility"""
    return _enhanced_input(prompt, fullscreen=False, default=default)


def _classify_confirmation_with_feedback(llm, user_input: str) -> dict:
    """Classify user response to accept/reject with potential additional feedback
    
    Returns:
        dict: {
            "action": "accept" | "reject" | "improve",
            "feedback": str | None
        }
    """
    
    prompt = PromptTemplate(
        input_variables=["user_input"],
        template="""
        ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®å…¥åŠ›ã‚’åˆ†æã—ã¦ã€ä»¥ä¸‹ã®ã„ãšã‚Œã‹ã«åˆ†é¡ã—ã¦ãã ã•ã„ï¼š

        1. "accept" - æ‰¿èªãƒ»å—ã‘å…¥ã‚Œï¼ˆyes, ok, ã„ã„ã­ã€æ‰¿èªã€ãªã©ï¼‰
        2. "reject" - æ‹’å¦ãƒ»å´ä¸‹ï¼ˆno, ã ã‚ã€å´ä¸‹ã€ã‚„ã‚Šç›´ã—ã€ãªã©ï¼‰  
        3. "improve" - è¿½åŠ ã®æ”¹å–„ææ¡ˆãŒå«ã¾ã‚Œã¦ã„ã‚‹

        ãƒ¦ãƒ¼ã‚¶ãƒ¼å…¥åŠ›: "{user_input}"

        **åˆ†é¡ãƒ«ãƒ¼ãƒ«:**
        - æ˜ç¢ºã«å—ã‘å…¥ã‚Œã‚‹æ„æ€ãŒç¤ºã•ã‚Œã¦ã„ã‚‹å ´åˆã¯ "accept"
        - æ˜ç¢ºã«æ‹’å¦ã™ã‚‹æ„æ€ãŒç¤ºã•ã‚Œã¦ã„ã‚‹å ´åˆã¯ "reject"
        - å…·ä½“çš„ãªæ”¹å–„ç‚¹ã‚„å¤‰æ›´è¦æ±‚ãŒå«ã¾ã‚Œã¦ã„ã‚‹å ´åˆã¯ "improve"
        - æ›–æ˜§ãªå ´åˆã¯ "reject" ã‚’é¸ã¶

        **å‡ºåŠ›å½¢å¼:**
        action: [accept/reject/improve]
        feedback: [æ”¹å–„ææ¡ˆãŒã‚ã‚‹å ´åˆã®ã¿ã€ãã®å†…å®¹ã‚’æŠ½å‡º]

        ä¾‹ï¼š
        - "yes" â†’ action: accept, feedback: 
        - "ã‚‚ã†å°‘ã—è©³ã—ã" â†’ action: improve, feedback: ã‚‚ã†å°‘ã—è©³ã—ãèª¬æ˜ã—ã¦ã»ã—ã„
        - "æ•°å­—ã‚’å…·ä½“çš„ã«ã—ã¦ã€ã‚ã¨çµè«–ã‚’æœ€åˆã«" â†’ action: improve, feedback: æ•°å­—ã‚’å…·ä½“çš„ã«ã—ã¦ã€çµè«–ã‚’æœ€åˆã«æŒã£ã¦ãã¦ã»ã—ã„
        """
    )
    
    try:
        response = llm.invoke(prompt.format(user_input=user_input))
        content = response.content.strip()
        
        action = "reject"  # default
        feedback = None
        
        for line in content.split("\n"):
            line = line.strip()
            if line.startswith("action:"):
                action_part = line.split(":", 1)[1].strip()
                if action_part in ["accept", "reject", "improve"]:
                    action = action_part
            elif line.startswith("feedback:"):
                feedback_part = line.split(":", 1)[1].strip()
                if feedback_part:
                    feedback = feedback_part
        
        return {"action": action, "feedback": feedback}
        
    except Exception as e:
        logger.error(f"Error in confirmation classification: {str(e)}")
        return {"action": "reject", "feedback": None}


def _classify_confirmation(llm, user_input: str) -> bool:
    """Classify user's natural language input as acceptance or rejection"""
    
    prompt = PromptTemplate(
        input_variables=["user_input"],
        template="""
        ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è‡ªç„¶è¨€èªå…¥åŠ›ãŒã€Œæ‰¿èªãƒ»å—ã‘å…¥ã‚Œã€ã‹ã€Œæ‹’å¦ãƒ»å´ä¸‹ã€ã‹ã‚’åˆ¤å®šã—ã¦ãã ã•ã„ã€‚

        **å…¥åŠ›:** {user_input}

        **åˆ¤å®šåŸºæº–:**
        
        **æ‰¿èªãƒ»å—ã‘å…¥ã‚Œã®ä¾‹:**
        - ã€Œã¯ã„ã€ã€Œã„ã„ãˆã€ã€ŒOKã€ã€Œè‰¯ã„ã€ã€Œæ‰¿èªã€ã€Œå—ã‘å…¥ã‚Œã¾ã™ã€
        - ã€Œãã‚Œã§è‰¯ã„ã€ã€Œå•é¡Œãªã„ã€ã€Œå¤§ä¸ˆå¤«ã€ã€Œæ¡ç”¨ã€
        - ã€Œyesã€ã€Œacceptã€ã€Œapproveã€ã€Œgoodã€ã€Œfineã€
        - ã€Œãã†ã—ã¦ãã ã•ã„ã€ã€ŒãŠé¡˜ã„ã—ã¾ã™ã€ã€Œé€²ã‚ã¦ã€
        
        **æ‹’å¦ãƒ»å´ä¸‹ã®ä¾‹:**
        - ã€Œã„ã„ãˆã€ã€Œã ã‚ã€ã€ŒNGã€ã€Œæ‹’å¦ã€ã€Œå´ä¸‹ã€ã€Œã‚„ã‚ã¦ã€
        - ã€Œè‰¯ããªã„ã€ã€Œå•é¡ŒãŒã‚ã‚‹ã€ã€Œä¸é©åˆ‡ã€ã€Œé•ã†ã€
        - ã€Œnoã€ã€Œrejectã€ã€Œdenyã€ã€Œbadã€ã€Œwrongã€
        - ã€Œã‚„ã‚Šç›´ã—ã€ã€Œåˆ¥ã®æ–¹æ³•ã§ã€ã€Œå¤‰æ›´ã—ã¦ã€

        **å‡ºåŠ›è¦ä»¶:**
        - æ‰¿èªã®å ´åˆ: "true"
        - æ‹’å¦ã®å ´åˆ: "false"
        - åˆ¤æ–­ã«è¿·ã†å ´åˆã¯æ–‡è„ˆã‹ã‚‰æœ€ã‚‚é©åˆ‡ãªæ–¹ã‚’é¸ã¶
        - ä¸æ˜ç¢ºãªå ´åˆã¯ "false" ã‚’è¿”ã™ï¼ˆå®‰å…¨å´ã«å€’ã™ï¼‰

        **å‡ºåŠ›:** "true" ã¾ãŸã¯ "false" ã®ã¿ï¼ˆèª¬æ˜ä¸è¦ï¼‰
        """
    )
    
    try:
        response = llm.invoke(prompt.format(user_input=user_input))
        result = response.content.strip().lower()
        
        # Validate and convert to boolean
        if result == "true":
            return True
        elif result == "false":
            return False
        else:
            # If LLM returns unexpected format, default to False (safe side)
            logger.warning(f"Unexpected confirmation classification result: {result}")
            return False
    except Exception as e:
        logger.error(f"Error in confirmation classification: {str(e)}")
        return False


def _display_help() -> None:
    """Display help information for the human reviewer"""
    print("\n" + "="*60)
    print("ğŸ“– HUMAN REVIEWER HELP")
    print("="*60)
    print("\nğŸ¯ åˆ©ç”¨å¯èƒ½ãªã‚¢ã‚¯ã‚·ãƒ§ãƒ³:")
    print("   â€¢ è³ªå•ã™ã‚‹: è¦ç´„ã«ã¤ã„ã¦è©³ã—ãèã")
    print("   â€¢ æ”¹å–„è¦æ±‚: å…·ä½“çš„ãªæ”¹å–„ç‚¹ã‚’æŒ‡æ‘˜")
    print("   â€¢ ã‚½ãƒ¼ã‚¹ç¢ºèª: å…ƒè³‡æ–™ã‚’ç¢ºèª")
    print("   â€¢ æ‰¿èª: è¦ç´„ã‚’æ‰¿èªã—ã¦å®Œäº†")
    print("   â€¢ ã‚­ãƒ£ãƒ³ã‚»ãƒ«: ãƒ¬ãƒ“ãƒ¥ãƒ¼ã‚’ä¸­æ­¢")
    print("   â€¢ å…¨ç”»é¢ã‚¨ãƒ‡ã‚£ã‚¿: ç©ºå…¥åŠ›ï¼ˆEnterã®ã¿ï¼‰ã§èµ·å‹•")
    
    print("\nğŸ“ å…¥åŠ›æ–¹æ³•:")
    print("   â€¢ é€šå¸¸å…¥åŠ›: ãã®ã¾ã¾å…¥åŠ›ã—ã¦Enter")
    print("   â€¢ å…¨ç”»é¢ã‚¨ãƒ‡ã‚£ã‚¿: ç©ºå…¥åŠ›ï¼ˆEnterã®ã¿ï¼‰ã§è‡ªå‹•èµ·å‹•")
    print("   â€¢ æ—¥æœ¬èªå…¥åŠ›: WSLç’°å¢ƒã§ã‚‚å¿«é©ã«å…¥åŠ›å¯èƒ½")
    print("   â€¢ å±¥æ­´å‘¼ã³å‡ºã—: ä¸Šä¸‹çŸ¢å°ã‚­ãƒ¼ã§éå»ã®å…¥åŠ›ã‚’å‘¼ã³å‡ºã—")
    print("   â€¢ å…¨ç”»é¢çµ‚äº†: Ctrl+S (ä¿å­˜), Ctrl+Q (ã‚­ãƒ£ãƒ³ã‚»ãƒ«)")
    
    print("\nğŸ’¡ å…¥åŠ›ä¾‹:")
    print("   â€¢ ã€Œã“ã®éƒ¨åˆ†ã‚’ã‚‚ã£ã¨è©³ã—ãèª¬æ˜ã—ã¦ã€")
    print("   â€¢ ã€Œæ•°å­—ã‚’å…·ä½“çš„ã«ã—ã¦ã€çµè«–ã‚’æœ€åˆã«ã€")
    print("   â€¢ ã€Œå…¨ä½“çš„ã«ä½œã‚Šç›´ã—ã¦æ™‚ç³»åˆ—ã§æ•´ç†ã€")
    print("   â€¢ ã€ŒOKã€ã€Œæ‰¿èªã€ã€Œã„ã„ã­ã€(æ‰¿èª)")
    print("   â€¢ ã€Œsourceã€ã€Œã‚½ãƒ¼ã‚¹ã€(å…ƒè³‡æ–™ç¢ºèª)")
    
    print("\nâŒ¨ï¸  ã‚·ãƒ§ãƒ¼ãƒˆã‚«ãƒƒãƒˆ:")
    print("   â€¢ Ctrl+C: ãƒ¬ãƒ“ãƒ¥ãƒ¼ä¸­æ–­")
    print("   â€¢ Enter: ç©ºå…¥åŠ›ã§å…¨ç”»é¢ã‚¨ãƒ‡ã‚£ã‚¿èµ·å‹•")
    print("   â€¢ Ctrl+S: å…¨ç”»é¢ã‚¨ãƒ‡ã‚£ã‚¿ã§ä¿å­˜ã—ã¦çµ‚äº†")
    print("   â€¢ Ctrl+Q: å…¨ç”»é¢ã‚¨ãƒ‡ã‚£ã‚¿ã§ã‚­ãƒ£ãƒ³ã‚»ãƒ«")
    print("="*60)


def _display_source_materials(overview: str, summaries: list) -> None:
    """Display source materials for human review"""
    
    print("\n" + "="*60)
    print("ğŸ“š SOURCE MATERIALS REVIEW")
    print("="*60)
    
    if overview:
        print(f"\nğŸ“‹ Overview:\n{'-'*30}\n{overview}\n")
    
    if summaries:
        print(f"ğŸ“„ Document Summaries ({len(summaries)} documents):")
        for i, summary in enumerate(summaries, 1):
            print(f"\n{i}. ã€{summary.name}ã€‘")
            print(f"   URL: {summary.url}")
            print(f"   Content: {summary.content[:200]}{'...' if len(summary.content) > 200 else ''}")
    else:
        print("\nâŒ No document summaries available.")
    
    _enhanced_input("\nğŸ“– Press Enter to continue...", default="") 
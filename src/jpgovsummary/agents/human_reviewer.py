import sys
from typing import Dict, Any
from langchain_core.messages import HumanMessage, AIMessage
from langchain_core.prompts import PromptTemplate

from .. import Model, State, logger


def human_reviewer(state: State) -> State:
    """
    Human-in-the-loop reviewer agent for final summary quality assurance.
    Provides bidirectional Q&A functionality for iterative improvement.
    """
    logger.info("human_reviewer")

    llm = Model().llm()
    
    # Get current data
    final_summary = state.get("final_summary", "")
    overview = state.get("overview", "")
    url = state.get("url", "")
    target_report_summaries = state.get("target_report_summaries", [])
    
    # Initialize review session if not exists
    if "review_session" not in state:
        state["review_session"] = {
            "iteration": 0,
            "qa_history": [],
            "original_summary": final_summary,
            "improvements": []
        }
    
    review_session = state["review_session"]
    
    # Display current summary for human review
    print("\n" + "="*80)
    print("ğŸ” HUMAN REVIEW SESSION - FINAL SUMMARY QUALITY CHECK")
    print("="*80)
    print(f"\nğŸ“„ Current Summary ({len(final_summary)} characters):")
    print("-" * 50)
    print(final_summary)
    print("-" * 50)
    print(f"ğŸ”— URL: {url}")
    
    if review_session["iteration"] > 0:
        print(f"\nğŸ“Š Review Iteration: {review_session['iteration']}")
        print(f"ğŸ’¬ Previous Q&A exchanges: {len(review_session['qa_history'])}")
    
    # Interactive review loop
    while True:
        try:
            user_input = _safe_input("\nYou> ")
            
            if not user_input:
                print("âŒ Please provide some input.")
                continue
            
            # Use LLM to classify the user's intent
            action_type = _classify_user_intent(llm, user_input)
            
            if action_type == "approve":
                # Approve and finish
                print("\nâœ… Summary approved! Finishing review session.")
                state["review_approved"] = True
                break
                
            elif action_type == "question":
                # Extract question or ask for clarification
                question = _extract_question_from_input(llm, user_input)
                if question:
                    ai_response = _ask_ai_question(llm, question, final_summary, overview, target_report_summaries)
                    print(f"\nAI> {ai_response}")
                    
                    # Record Q&A
                    review_session["qa_history"].append({
                        "type": "human_question",
                        "content": question,
                        "ai_response": ai_response
                    })
                else:
                    print("â“ Could not extract a clear question.")
                
            elif action_type == "improve":
                # Extract improvement request
                improvement_request = _extract_improvement_request(llm, user_input)
                if improvement_request:
                    improved_summary = _generate_improved_summary(
                        llm, final_summary, improvement_request, overview, target_report_summaries, url
                    )
                    
                    print(f"\nAI> Improved Summary:\n{'-'*50}\n{improved_summary}\n{'-'*50}")
                    
                    # Ask for confirmation in natural language
                    confirmation_input = _safe_input("\nYou> ", default="")
                    
                    if confirmation_input:
                        result = _classify_confirmation_with_feedback(llm, confirmation_input)
                        
                        if result["action"] == "accept":
                            final_summary = improved_summary
                            state["final_summary"] = final_summary
                            review_session["improvements"].append({
                                "request": improvement_request,
                                "result": improved_summary
                            })
                            print("âœ… Improvement applied!")
                        elif result["action"] == "improve" and result["feedback"]:
                            # Apply additional improvement
                            further_improved_summary = _generate_improved_summary(
                                llm, improved_summary, result["feedback"], overview, target_report_summaries, url
                            )
                            final_summary = further_improved_summary
                            state["final_summary"] = final_summary
                            review_session["improvements"].append({
                                "request": f"{improvement_request} + {result['feedback']}",
                                "result": further_improved_summary
                            })
                            print(f"\nAI> Further Improved Summary:\n{'-'*50}\n{further_improved_summary}\n{'-'*50}")
                            print("âœ… Additional improvement applied!")
                        else:
                            print("âŒ Improvement rejected, keeping current summary.")
                    else:
                        print("âŒ No response provided, keeping current summary.")
                else:
                    print("ğŸ“ Could not extract a clear improvement request.")
                
            elif action_type == "source":
                # Review source materials
                _display_source_materials(overview, target_report_summaries)
                
            elif action_type == "regenerate":
                # Extract feedback for regeneration
                feedback = _extract_regeneration_feedback(llm, user_input)
                if feedback:
                    regenerated_summary = _regenerate_summary_with_feedback(
                        llm, final_summary, feedback, overview, target_report_summaries, url
                    )
                    
                    print(f"\nAI> Regenerated Summary:\n{'-'*50}\n{regenerated_summary}\n{'-'*50}")
                    
                    # Ask for confirmation in natural language
                    confirmation_input = _safe_input("\nYou> ", default="")
                    
                    if confirmation_input:
                        result = _classify_confirmation_with_feedback(llm, confirmation_input)
                        
                        if result["action"] == "accept":
                            final_summary = regenerated_summary
                            state["final_summary"] = final_summary
                            review_session["improvements"].append({
                                "request": f"Regeneration: {feedback}",
                                "result": regenerated_summary
                            })
                            print("âœ… Regenerated summary applied!")
                        elif result["action"] == "improve" and result["feedback"]:
                            # Apply additional improvement to regenerated summary
                            further_improved_summary = _generate_improved_summary(
                                llm, regenerated_summary, result["feedback"], overview, target_report_summaries, url
                            )
                            final_summary = further_improved_summary
                            state["final_summary"] = final_summary
                            review_session["improvements"].append({
                                "request": f"Regeneration: {feedback} + {result['feedback']}",
                                "result": further_improved_summary
                            })
                            print(f"\nAI> Further Improved Summary:\n{'-'*50}\n{further_improved_summary}\n{'-'*50}")
                            print("âœ… Additional improvement applied!")
                        else:
                            print("âŒ Regeneration rejected, keeping current summary.")
                    else:
                        print("âŒ No response provided, keeping current summary.")
                else:
                    print("ğŸ“ Could not extract clear feedback.")
                
            elif action_type == "cancel":
                # Cancel review
                print("\nâŒ Review cancelled. Using current summary.")
                state["review_approved"] = False
                break
                
            else:
                print("âŒ Could not understand your request.")
                
        except KeyboardInterrupt:
            print("\n\nâš ï¸  Review interrupted. Using current summary.")
            state["review_approved"] = False
            break
        except EOFError:
            print("\n\nâš ï¸  Input ended. Using current summary.")
            state["review_approved"] = False
            break
    
    # Update review session
    review_session["iteration"] += 1
    state["review_session"] = review_session
    
    # Update messages with final reviewed summary
    message = HumanMessage(content=f"{final_summary}\n{url}")
    
    # Add review metadata to state
    state["review_completed"] = True
    state["final_review_summary"] = final_summary
    
    return {**state, "messages": [message]}


def _ask_ai_question(llm, question: str, summary: str, overview: str, summaries: list) -> str:
    """Ask AI a specific question about the summary"""
    
    source_context = ""
    if summaries:
        source_context = "\n\n".join([
            f"ã€{s.name}ã€‘\n{s.content}" for s in summaries if s.content
        ])
    
    prompt = PromptTemplate(
        input_variables=["question", "summary", "overview", "source_context"],
        template="""
        äººé–“ã‹ã‚‰ä»¥ä¸‹ã®è³ªå•ã‚’å—ã‘ã¾ã—ãŸã€‚è¦ç´„ã®å†…å®¹ã¨å…ƒè³‡æ–™ã‚’å‚ç…§ã—ã¦ã€æ­£ç¢ºã§è©³ç´°ãªå›ç­”ã‚’ã—ã¦ãã ã•ã„ã€‚

        **è³ªå•:** {question}

        **ç¾åœ¨ã®è¦ç´„:**
        {summary}

        **æ¦‚è¦æƒ…å ±:**
        {overview}

        **å…ƒè³‡æ–™ã®è¦ç´„:**
        {source_context}

        **å›ç­”è¦ä»¶:**
        - è³ªå•ã«å¯¾ã—ã¦å…·ä½“çš„ã§æ­£ç¢ºãªå›ç­”ã‚’ã™ã‚‹
        - æ ¹æ‹ ã¨ãªã‚‹è³‡æ–™ã‚„æƒ…å ±ã‚’æ˜ç¤ºã™ã‚‹
        - ä¸æ˜ãªç‚¹ãŒã‚ã‚Œã°ç´ ç›´ã«ã€Œä¸æ˜ã€ã¨ç­”ãˆã‚‹
        - æ¨æ¸¬ã‚„å‰µä½œã¯è¡Œã‚ãªã„
        - å¿…è¦ã«å¿œã˜ã¦è¿½åŠ ã®è³ªå•ã‚’ææ¡ˆã™ã‚‹
        """
    )
    
    try:
        response = llm.invoke(prompt.format(
            question=question,
            summary=summary,
            overview=overview,
            source_context=source_context
        ))
        return response.content.strip()
    except Exception as e:
        logger.error(f"Error in AI question answering: {str(e)}")
        return f"ç”³ã—è¨³ã‚ã‚Šã¾ã›ã‚“ãŒã€è³ªå•ã¸ã®å›ç­”ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}"


def _generate_improved_summary(llm, current_summary: str, improvement_request: str, 
                             overview: str, summaries: list, url: str) -> str:
    """Generate an improved summary based on human feedback"""
    
    source_context = ""
    if summaries:
        source_context = "\n\n".join([
            f"ã€{s.name}ã€‘\n{s.content}" for s in summaries if s.content
        ])
    
    # Calculate max characters based on URL length
    url_length = len(url)
    max_chars = max(50, 300 - url_length - 1)
    
    prompt = PromptTemplate(
        input_variables=["current_summary", "improvement_request", "overview", "source_context", "max_chars"],
        template="""
        ç¾åœ¨ã®è¦ç´„ã«å¯¾ã—ã¦æ”¹å–„è¦æ±‚ãŒã‚ã‚Šã¾ã—ãŸã€‚è¦æ±‚ã«å¾“ã£ã¦è¦ç´„ã‚’æ”¹å–„ã—ã¦ãã ã•ã„ã€‚

        **æ”¹å–„è¦æ±‚:**
        {improvement_request}

        **ç¾åœ¨ã®è¦ç´„:**
        {current_summary}

        **æ¦‚è¦æƒ…å ±:**
        {overview}

        **å…ƒè³‡æ–™ã®è¦ç´„:**
        {source_context}

        **æ”¹å–„è¦ä»¶:**
        - æ”¹å–„è¦æ±‚ã«å…·ä½“çš„ã«å¯¾å¿œã™ã‚‹
        - {max_chars}æ–‡å­—ä»¥ä¸‹ã§ä½œæˆã™ã‚‹
        - å®Ÿéš›ã«æ›¸ã‹ã‚Œã¦ã„ã‚‹å†…å®¹ã®ã¿ã‚’ä½¿ç”¨ã™ã‚‹
        - æ¨æ¸¬ã‚„å‰µä½œã¯è¡Œã‚ãªã„
        - é‡è¦ãªæƒ…å ±ã‚’æ¼ã‚‰ã•ãªã„
        - èª­ã¿ã‚„ã™ãè«–ç†çš„ãªæ§‹æˆã«ã™ã‚‹
        - ä¼šè­°åã‚„è³‡æ–™åã‚’é©åˆ‡ã«å«ã‚ã‚‹
        """
    )
    
    try:
        response = llm.invoke(prompt.format(
            current_summary=current_summary,
            improvement_request=improvement_request,
            overview=overview,
            source_context=source_context,
            max_chars=max_chars
        ))
        return response.content.strip()
    except Exception as e:
        logger.error(f"Error in summary improvement: {str(e)}")
        return current_summary


def _regenerate_summary_with_feedback(llm, current_summary: str, feedback: str,
                                    overview: str, summaries: list, url: str) -> str:
    """Completely regenerate summary with comprehensive feedback"""
    
    source_context = ""
    if summaries:
        source_context = "\n\n".join([
            f"ã€{s.name}ã€‘\n{s.content}" for s in summaries if s.content
        ])
    
    # Calculate max characters based on URL length
    url_length = len(url)
    max_chars = max(50, 300 - url_length - 1)
    
    prompt = PromptTemplate(
        input_variables=["feedback", "current_summary", "overview", "source_context", "max_chars"],
        template="""
        åŒ…æ‹¬çš„ãªãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ã«åŸºã¥ã„ã¦è¦ç´„ã‚’å®Œå…¨ã«å†ç”Ÿæˆã—ã¦ãã ã•ã„ã€‚

        **ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯:**
        {feedback}

        **å‚è€ƒï¼ˆç¾åœ¨ã®è¦ç´„ï¼‰:**
        {current_summary}

        **æ¦‚è¦æƒ…å ±:**
        {overview}

        **å…ƒè³‡æ–™ã®è¦ç´„:**
        {source_context}

        **å†ç”Ÿæˆè¦ä»¶:**
        - ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ã‚’å…¨é¢çš„ã«åæ˜ ã™ã‚‹
        - {max_chars}æ–‡å­—ä»¥ä¸‹ã§ä½œæˆã™ã‚‹
        - å®Ÿéš›ã«æ›¸ã‹ã‚Œã¦ã„ã‚‹å†…å®¹ã®ã¿ã‚’ä½¿ç”¨ã™ã‚‹
        - æ¨æ¸¬ã‚„å‰µä½œã¯è¡Œã‚ãªã„
        - ã‚ˆã‚Šè‰¯ã„æ§‹æˆã¨è¡¨ç¾ã‚’å¿ƒãŒã‘ã‚‹
        - é‡è¦ãªæƒ…å ±ã‚’æ¼ã‚‰ã•ãªã„
        - ä¼šè­°åã‚„è³‡æ–™åã‚’é©åˆ‡ã«å«ã‚ã‚‹
        - èª­ã¿æ‰‹ã«ã¨ã£ã¦ä¾¡å€¤ã®ã‚ã‚‹è¦ç´„ã«ã™ã‚‹
        """
    )
    
    try:
        response = llm.invoke(prompt.format(
            feedback=feedback,
            current_summary=current_summary,
            overview=overview,
            source_context=source_context,
            max_chars=max_chars
        ))
        return response.content.strip()
    except Exception as e:
        logger.error(f"Error in summary regeneration: {str(e)}")
        return current_summary


def _classify_user_intent(llm, user_input: str) -> str:
    """Classify user's natural language input into action categories"""
    
    prompt = PromptTemplate(
        input_variables=["user_input"],
        template="""
        ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è‡ªç„¶è¨€èªå…¥åŠ›ã‚’ä»¥ä¸‹ã®ã‚¢ã‚¯ã‚·ãƒ§ãƒ³åˆ†é¡ã®ã„ãšã‚Œã‹ã«åˆ†é¡ã—ã¦ãã ã•ã„ã€‚

        **å…¥åŠ›:** {user_input}

        **åˆ†é¡ã‚«ãƒ†ã‚´ãƒª:**
        - approve: è¦ç´„ã‚’æ‰¿èªãƒ»å®Œäº†ã™ã‚‹ï¼ˆä¾‹ï¼šã€Œæ‰¿èªã€ã€ŒOKã€ã€Œè‰¯ã„ã€ã€Œå®Œäº†ã€ã€Œçµ‚äº†ã€ï¼‰
        - question: AIã«è³ªå•ã™ã‚‹ï¼ˆä¾‹ï¼šã€Œè³ªå•ã€ã€Œãªãœã€ã€Œã©ã†ã—ã¦ã€ã€Œæ•™ãˆã¦ã€ã€Œï¼Ÿã€ï¼‰
        - improve: æ”¹å–„ã‚’è¦æ±‚ã™ã‚‹ï¼ˆä¾‹ï¼šã€Œæ”¹å–„ã€ã€Œä¿®æ­£ã€ã€Œå¤‰æ›´ã€ã€Œç›´ã—ã¦ã€ã€Œã‚‚ã£ã¨ã€ï¼‰
        - source: å…ƒè³‡æ–™ã‚’ç¢ºèªã™ã‚‹ï¼ˆä¾‹ï¼šã€Œè³‡æ–™ã€ã€Œã‚½ãƒ¼ã‚¹ã€ã€Œå…ƒã€ã€Œç¢ºèªã€ã€Œè¦‹ãŸã„ã€ï¼‰
        - regenerate: å®Œå…¨ã«å†ç”Ÿæˆã™ã‚‹ï¼ˆä¾‹ï¼šã€Œå†ç”Ÿæˆã€ã€Œä½œã‚Šç›´ã—ã€ã€Œæœ€åˆã‹ã‚‰ã€ã€Œå…¨é¢çš„ã«ã€ï¼‰
        - cancel: ãƒ¬ãƒ“ãƒ¥ãƒ¼ã‚’ã‚­ãƒ£ãƒ³ã‚»ãƒ«ã™ã‚‹ï¼ˆä¾‹ï¼šã€Œã‚­ãƒ£ãƒ³ã‚»ãƒ«ã€ã€Œä¸­æ­¢ã€ã€Œã‚„ã‚ã‚‹ã€ã€Œçµ‚ã‚ã‚Šã€ï¼‰

        **å‡ºåŠ›è¦ä»¶:**
        - ä¸Šè¨˜6ã¤ã®ã‚«ãƒ†ã‚´ãƒªã®ã†ã¡1ã¤ã ã‘ã‚’è¿”ã™
        - è‹±èªã®å°æ–‡å­—ã§è¿”ã™ï¼ˆapprove, question, improve, source, regenerate, cancelï¼‰
        - åˆ¤æ–­ã«è¿·ã†å ´åˆã¯æœ€ã‚‚è¿‘ã„ã‚«ãƒ†ã‚´ãƒªã‚’é¸ã¶
        - åˆ†é¡ã§ããªã„å ´åˆã¯ "unknown" ã‚’è¿”ã™
        """
    )
    
    try:
        response = llm.invoke(prompt.format(user_input=user_input))
        action_type = response.content.strip().lower()
        
        # Validate the response
        valid_actions = ["approve", "question", "improve", "source", "regenerate", "cancel"]
        if action_type in valid_actions:
            return action_type
        else:
            return "unknown"
    except Exception as e:
        logger.error(f"Error in intent classification: {str(e)}")
        return "unknown"


def _extract_question_from_input(llm, user_input: str) -> str:
    """Extract a clear question from user's natural language input"""
    
    prompt = PromptTemplate(
        input_variables=["user_input"],
        template="""
        ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®å…¥åŠ›ã‹ã‚‰è³ªå•ã‚’æŠ½å‡ºã—ã¦ãã ã•ã„ã€‚

        **å…¥åŠ›:** {user_input}

        **æŠ½å‡ºè¦ä»¶:**
        - æ˜ç¢ºãªè³ªå•æ–‡ã¨ã—ã¦æ•´ç†ã™ã‚‹
        - ã€Œï¼Ÿã€ã§çµ‚ã‚ã‚‹ç–‘å•æ–‡ã«ã™ã‚‹
        - è¦ç´„ã«é–¢ã™ã‚‹è³ªå•ã¨ã—ã¦é©åˆ‡ã«æ•´å½¢ã™ã‚‹
        - è³ªå•ãŒä¸æ˜ç¢ºãªå ´åˆã¯ç©ºæ–‡å­—åˆ—ã‚’è¿”ã™

        **ä¾‹:**
        - å…¥åŠ›: "ã“ã®éƒ¨åˆ†ã«ã¤ã„ã¦è©³ã—ãæ•™ãˆã¦" â†’ å‡ºåŠ›: "ã“ã®éƒ¨åˆ†ã«ã¤ã„ã¦è©³ã—ãæ•™ãˆã¦ãã ã•ã„ï¼Ÿ"
        - å…¥åŠ›: "ãªãœã“ã®çµè«–ã«ãªã£ãŸã®" â†’ å‡ºåŠ›: "ãªãœã“ã®çµè«–ã«ãªã£ãŸã®ã§ã™ã‹ï¼Ÿ"
        - å…¥åŠ›: "æ ¹æ‹ ã¯" â†’ å‡ºåŠ›: "ã“ã®è¦ç´„ã®æ ¹æ‹ ã¯ä½•ã§ã™ã‹ï¼Ÿ"

        **å‡ºåŠ›:** è³ªå•æ–‡ã®ã¿ï¼ˆèª¬æ˜ä¸è¦ï¼‰
        """
    )
    
    try:
        response = llm.invoke(prompt.format(user_input=user_input))
        question = response.content.strip()
        return question if question and question != "ç©ºæ–‡å­—åˆ—" else ""
    except Exception as e:
        logger.error(f"Error in question extraction: {str(e)}")
        return ""


def _extract_improvement_request(llm, user_input: str) -> str:
    """Extract improvement request from user's natural language input"""
    
    prompt = PromptTemplate(
        input_variables=["user_input"],
        template="""
        ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®å…¥åŠ›ã‹ã‚‰æ”¹å–„è¦æ±‚ã‚’æŠ½å‡ºã—ã¦ãã ã•ã„ã€‚

        **å…¥åŠ›:** {user_input}

        **æŠ½å‡ºè¦ä»¶:**
        - å…·ä½“çš„ãªæ”¹å–„æŒ‡ç¤ºã¨ã—ã¦æ•´ç†ã™ã‚‹
        - ä½•ã‚’ã©ã®ã‚ˆã†ã«æ”¹å–„ã—ãŸã„ã‹ã‚’æ˜ç¢ºã«ã™ã‚‹
        - è¦ç´„ã®æ”¹å–„ã«é–¢ã™ã‚‹æŒ‡ç¤ºã¨ã—ã¦é©åˆ‡ã«æ•´å½¢ã™ã‚‹
        - æ”¹å–„è¦æ±‚ãŒä¸æ˜ç¢ºãªå ´åˆã¯ç©ºæ–‡å­—åˆ—ã‚’è¿”ã™

        **ä¾‹:**
        - å…¥åŠ›: "ã‚‚ã£ã¨ç°¡æ½”ã«ã—ã¦" â†’ å‡ºåŠ›: "è¦ç´„ã‚’ã‚ˆã‚Šç°¡æ½”ã§èª­ã¿ã‚„ã™ãã—ã¦ãã ã•ã„"
        - å…¥åŠ›: "æ•°å­—ã‚’è¿½åŠ ã—ã¦" â†’ å‡ºåŠ›: "å…·ä½“çš„ãªæ•°å­—ã‚„ãƒ‡ãƒ¼ã‚¿ã‚’è¿½åŠ ã—ã¦ãã ã•ã„"
        - å…¥åŠ›: "çµè«–ã‚’æ˜ç¢ºã«" â†’ å‡ºåŠ›: "çµè«–éƒ¨åˆ†ã‚’ã‚ˆã‚Šæ˜ç¢ºã«è¡¨ç¾ã—ã¦ãã ã•ã„"

        **å‡ºåŠ›:** æ”¹å–„è¦æ±‚æ–‡ã®ã¿ï¼ˆèª¬æ˜ä¸è¦ï¼‰
        """
    )
    
    try:
        response = llm.invoke(prompt.format(user_input=user_input))
        improvement = response.content.strip()
        return improvement if improvement and improvement != "ç©ºæ–‡å­—åˆ—" else ""
    except Exception as e:
        logger.error(f"Error in improvement extraction: {str(e)}")
        return ""


def _extract_regeneration_feedback(llm, user_input: str) -> str:
    """Extract comprehensive feedback for summary regeneration"""
    
    prompt = PromptTemplate(
        input_variables=["user_input"],
        template="""
        ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®å…¥åŠ›ã‹ã‚‰è¦ç´„ã®å†ç”Ÿæˆã«å¿…è¦ãªåŒ…æ‹¬çš„ãªãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ã‚’æŠ½å‡ºã—ã¦ãã ã•ã„ã€‚

        **å…¥åŠ›:** {user_input}

        **æŠ½å‡ºè¦ä»¶:**
        - è¦ç´„ã®å†ç”Ÿæˆã«å¿…è¦ãªè©³ç´°ãªæŒ‡ç¤ºã¨ã—ã¦æ•´ç†ã™ã‚‹
        - æ§‹æˆã€å†…å®¹ã€ã‚¹ã‚¿ã‚¤ãƒ«ç­‰ã®æ”¹å–„ç‚¹ã‚’æ˜ç¢ºã«ã™ã‚‹
        - å®Œå…¨ãªä½œã‚Šç›´ã—ã«å¿…è¦ãªæƒ…å ±ã‚’å«ã‚ã‚‹
        - ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ãŒä¸æ˜ç¢ºãªå ´åˆã¯ç©ºæ–‡å­—åˆ—ã‚’è¿”ã™

        **ä¾‹:**
        - å…¥åŠ›: "å…¨ä½“çš„ã«ä½œã‚Šç›´ã—ã¦ã€ã‚‚ã£ã¨è©³ã—ã" â†’ å‡ºåŠ›: "è¦ç´„å…¨ä½“ã‚’ä½œã‚Šç›´ã—ã€ã‚ˆã‚Šè©³ç´°ã§å…·ä½“çš„ãªå†…å®¹ã«ã—ã¦ãã ã•ã„"
        - å…¥åŠ›: "æ§‹æˆã‚’å¤‰ãˆã¦æ™‚ç³»åˆ—ã§" â†’ å‡ºåŠ›: "è¦ç´„ã®æ§‹æˆã‚’æ™‚ç³»åˆ—é †ã«å¤‰æ›´ã—ã¦å†ç”Ÿæˆã—ã¦ãã ã•ã„"

        **å‡ºåŠ›:** ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯æ–‡ã®ã¿ï¼ˆèª¬æ˜ä¸è¦ï¼‰
        """
    )
    
    try:
        response = llm.invoke(prompt.format(user_input=user_input))
        feedback = response.content.strip()
        return feedback if feedback and feedback != "ç©ºæ–‡å­—åˆ—" else ""
    except Exception as e:
        logger.error(f"Error in feedback extraction: {str(e)}")
        return ""


def _safe_input(prompt: str, default: str = "") -> str:
    """Safely get user input with Unicode error handling"""
    try:
        return input(prompt).strip()
    except UnicodeDecodeError as e:
        print(f"âŒ æ–‡å­—ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
        print("ğŸ’¡ å…¥åŠ›ã«ä½¿ç”¨ã§ããªã„æ–‡å­—ãŒå«ã¾ã‚Œã¦ã„ã¾ã™ã€‚")
        return default
    except (EOFError, KeyboardInterrupt):
        # Re-raise these as they should be handled by the main loop
        raise


def _classify_confirmation_with_feedback(llm, user_input: str) -> dict:
    """Classify user response to accept/reject with potential additional feedback
    
    Returns:
        dict: {
            "action": "accept" | "reject" | "improve",
            "feedback": str | None
        }
    """
    
    prompt = PromptTemplate(
        input_variables=["user_input"],
        template="""
        ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®å…¥åŠ›ã‚’åˆ†æã—ã¦ã€ä»¥ä¸‹ã®ã„ãšã‚Œã‹ã«åˆ†é¡ã—ã¦ãã ã•ã„ï¼š

        1. "accept" - æ‰¿èªãƒ»å—ã‘å…¥ã‚Œï¼ˆyes, ok, ã„ã„ã­ã€æ‰¿èªã€ãªã©ï¼‰
        2. "reject" - æ‹’å¦ãƒ»å´ä¸‹ï¼ˆno, ã ã‚ã€å´ä¸‹ã€ã‚„ã‚Šç›´ã—ã€ãªã©ï¼‰  
        3. "improve" - è¿½åŠ ã®æ”¹å–„ææ¡ˆãŒå«ã¾ã‚Œã¦ã„ã‚‹

        ãƒ¦ãƒ¼ã‚¶ãƒ¼å…¥åŠ›: "{user_input}"

        **åˆ†é¡ãƒ«ãƒ¼ãƒ«:**
        - æ˜ç¢ºã«å—ã‘å…¥ã‚Œã‚‹æ„æ€ãŒç¤ºã•ã‚Œã¦ã„ã‚‹å ´åˆã¯ "accept"
        - æ˜ç¢ºã«æ‹’å¦ã™ã‚‹æ„æ€ãŒç¤ºã•ã‚Œã¦ã„ã‚‹å ´åˆã¯ "reject"
        - å…·ä½“çš„ãªæ”¹å–„ç‚¹ã‚„å¤‰æ›´è¦æ±‚ãŒå«ã¾ã‚Œã¦ã„ã‚‹å ´åˆã¯ "improve"
        - æ›–æ˜§ãªå ´åˆã¯ "reject" ã‚’é¸ã¶

        **å‡ºåŠ›å½¢å¼:**
        action: [accept/reject/improve]
        feedback: [æ”¹å–„ææ¡ˆãŒã‚ã‚‹å ´åˆã®ã¿ã€ãã®å†…å®¹ã‚’æŠ½å‡º]

        ä¾‹ï¼š
        - "yes" â†’ action: accept, feedback: 
        - "ã‚‚ã†å°‘ã—è©³ã—ã" â†’ action: improve, feedback: ã‚‚ã†å°‘ã—è©³ã—ãèª¬æ˜ã—ã¦ã»ã—ã„
        - "æ•°å­—ã‚’å…·ä½“çš„ã«ã—ã¦ã€ã‚ã¨çµè«–ã‚’æœ€åˆã«" â†’ action: improve, feedback: æ•°å­—ã‚’å…·ä½“çš„ã«ã—ã¦ã€çµè«–ã‚’æœ€åˆã«æŒã£ã¦ãã¦ã»ã—ã„
        """
    )
    
    try:
        response = llm.invoke(prompt.format(user_input=user_input))
        content = response.content.strip()
        
        action = "reject"  # default
        feedback = None
        
        for line in content.split("\n"):
            line = line.strip()
            if line.startswith("action:"):
                action_part = line.split(":", 1)[1].strip()
                if action_part in ["accept", "reject", "improve"]:
                    action = action_part
            elif line.startswith("feedback:"):
                feedback_part = line.split(":", 1)[1].strip()
                if feedback_part:
                    feedback = feedback_part
        
        return {"action": action, "feedback": feedback}
        
    except Exception as e:
        logger.error(f"Error in confirmation classification: {str(e)}")
        return {"action": "reject", "feedback": None}


def _classify_confirmation(llm, user_input: str) -> bool:
    """Classify user's natural language input as acceptance or rejection"""
    
    prompt = PromptTemplate(
        input_variables=["user_input"],
        template="""
        ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è‡ªç„¶è¨€èªå…¥åŠ›ãŒã€Œæ‰¿èªãƒ»å—ã‘å…¥ã‚Œã€ã‹ã€Œæ‹’å¦ãƒ»å´ä¸‹ã€ã‹ã‚’åˆ¤å®šã—ã¦ãã ã•ã„ã€‚

        **å…¥åŠ›:** {user_input}

        **åˆ¤å®šåŸºæº–:**
        
        **æ‰¿èªãƒ»å—ã‘å…¥ã‚Œã®ä¾‹:**
        - ã€Œã¯ã„ã€ã€Œã„ã„ãˆã€ã€ŒOKã€ã€Œè‰¯ã„ã€ã€Œæ‰¿èªã€ã€Œå—ã‘å…¥ã‚Œã¾ã™ã€
        - ã€Œãã‚Œã§è‰¯ã„ã€ã€Œå•é¡Œãªã„ã€ã€Œå¤§ä¸ˆå¤«ã€ã€Œæ¡ç”¨ã€
        - ã€Œyesã€ã€Œacceptã€ã€Œapproveã€ã€Œgoodã€ã€Œfineã€
        - ã€Œãã†ã—ã¦ãã ã•ã„ã€ã€ŒãŠé¡˜ã„ã—ã¾ã™ã€ã€Œé€²ã‚ã¦ã€
        
        **æ‹’å¦ãƒ»å´ä¸‹ã®ä¾‹:**
        - ã€Œã„ã„ãˆã€ã€Œã ã‚ã€ã€ŒNGã€ã€Œæ‹’å¦ã€ã€Œå´ä¸‹ã€ã€Œã‚„ã‚ã¦ã€
        - ã€Œè‰¯ããªã„ã€ã€Œå•é¡ŒãŒã‚ã‚‹ã€ã€Œä¸é©åˆ‡ã€ã€Œé•ã†ã€
        - ã€Œnoã€ã€Œrejectã€ã€Œdenyã€ã€Œbadã€ã€Œwrongã€
        - ã€Œã‚„ã‚Šç›´ã—ã€ã€Œåˆ¥ã®æ–¹æ³•ã§ã€ã€Œå¤‰æ›´ã—ã¦ã€

        **å‡ºåŠ›è¦ä»¶:**
        - æ‰¿èªã®å ´åˆ: "true"
        - æ‹’å¦ã®å ´åˆ: "false"
        - åˆ¤æ–­ã«è¿·ã†å ´åˆã¯æ–‡è„ˆã‹ã‚‰æœ€ã‚‚é©åˆ‡ãªæ–¹ã‚’é¸ã¶
        - ä¸æ˜ç¢ºãªå ´åˆã¯ "false" ã‚’è¿”ã™ï¼ˆå®‰å…¨å´ã«å€’ã™ï¼‰

        **å‡ºåŠ›:** "true" ã¾ãŸã¯ "false" ã®ã¿ï¼ˆèª¬æ˜ä¸è¦ï¼‰
        """
    )
    
    try:
        response = llm.invoke(prompt.format(user_input=user_input))
        result = response.content.strip().lower()
        
        # Validate and convert to boolean
        if result == "true":
            return True
        elif result == "false":
            return False
        else:
            # If LLM returns unexpected format, default to False (safe side)
            logger.warning(f"Unexpected confirmation classification result: {result}")
            return False
    except Exception as e:
        logger.error(f"Error in confirmation classification: {str(e)}")
        return False


def _display_source_materials(overview: str, summaries: list) -> None:
    """Display source materials for human review"""
    
    print("\n" + "="*60)
    print("ğŸ“š SOURCE MATERIALS REVIEW")
    print("="*60)
    
    if overview:
        print(f"\nğŸ“‹ Overview:\n{'-'*30}\n{overview}\n")
    
    if summaries:
        print(f"ğŸ“„ Document Summaries ({len(summaries)} documents):")
        for i, summary in enumerate(summaries, 1):
            print(f"\n{i}. ã€{summary.name}ã€‘")
            print(f"   URL: {summary.url}")
            print(f"   Content: {summary.content[:200]}{'...' if len(summary.content) > 200 else ''}")
    else:
        print("\nâŒ No document summaries available.")
    
    _safe_input("\nğŸ“– Press Enter to continue...", default="") 
from langchain.chains.summarize import load_summarize_chain
from langchain.prompts import PromptTemplate
from langchain_core.documents import Document
from langchain_core.messages import HumanMessage, AIMessage
from langchain_core.output_parsers import JsonOutputParser

from .. import Model, State, Summary, TargetReportList, logger
from ..tools import load_pdf_as_text


def extract_document_structure(texts: list[str]) -> tuple[str, str]:
    """p10ã¾ã§ã‹ã‚‰æ–‡æ›¸æ§‹æˆãƒ»è¦ç‚¹ã‚’æŠ½å‡º
    
    Returns:
        tuple[str, str]: (æŠ½å‡ºå†…å®¹, è©²å½“ãƒšãƒ¼ã‚¸æƒ…å ±)
    """
    llm = Model().llm()
    
    # p10ã¾ã§ã®ãƒ†ã‚­ã‚¹ãƒˆã‚’ãƒãƒ¼ã‚¸ï¼ˆæœ€å¤§10ãƒšãƒ¼ã‚¸ï¼‰
    pages_to_analyze = min(10, len(texts))
    merged_pages = []
    
    for i in range(pages_to_analyze):
        merged_pages.append(f"--- ãƒšãƒ¼ã‚¸ {i+1} ---\n{texts[i]}")
    
    merged_text = "\n\n".join(merged_pages)
    
    structure_prompt = PromptTemplate(
        input_variables=["text", "pages_count"],
        template="""ä»¥ä¸‹ã¯PDFæ–‡æ›¸ã®p1-p{pages_count}ã®å†…å®¹ã§ã™ã€‚
æ–‡æ›¸ã®æ§‹æˆã‚„è¦ç‚¹ã‚’ç¤ºã™éƒ¨åˆ†ã‚’ç‰¹å®šã—ã€è©²å½“ç®‡æ‰€ãŒã‚ã‚Œã°æŠ½å‡ºã—ã¦ãã ã•ã„ã€‚

**æŠ½å‡ºå¯¾è±¡ï¼š**
- ç›®æ¬¡ã€Contentsã€æ§‹æˆï¼ˆè¤‡æ•°ãƒšãƒ¼ã‚¸ã«ã‚ãŸã‚‹å ´åˆã‚‚å«ã‚€ï¼‰
- ç›®æ¬¡ï¼ˆç¶šãï¼‰ã€Contents (continued)
- æœ¬æ—¥ã®è«–ç‚¹ã€è«–ç‚¹ã€æ¤œè¨äº‹é …
- è­°é¡Œã€ã‚¢ã‚¸ã‚§ãƒ³ãƒ€ã€Agenda
- æ¦‚è¦ã€è¦ç‚¹ã€ãƒã‚¤ãƒ³ãƒˆ
- ä»Šå›ã®å†…å®¹ã€æœ¬æ—¥ã®å†…å®¹
- ãã®ä»–ã€æ–‡æ›¸ã®å…¨ä½“æ§‹æˆã‚„ä¸»è¦è«–ç‚¹ã‚’ç¤ºã™é …ç›®ãƒªã‚¹ãƒˆ

**æŠ½å‡ºãƒ«ãƒ¼ãƒ«ï¼š**
- è©²å½“ç®‡æ‰€ã®å†…å®¹ã‚’ãã®ã¾ã¾æŠ½å‡ºï¼ˆè¦‹å‡ºã—å«ã‚€ï¼‰
- **è¤‡æ•°ãƒšãƒ¼ã‚¸ã«ã‚ãŸã‚‹ç›®æ¬¡ã¯çµ±åˆã—ã¦æŠ½å‡º**
- ç›®æ¬¡ã®ä¸€éƒ¨åˆ†ã®ã¿ã®å ´åˆã‚‚æŠ½å‡ºå¯¾è±¡ã¨ã™ã‚‹
- è¤‡æ•°ç®‡æ‰€ã‚ã‚‹å ´åˆã¯ã™ã¹ã¦æŠ½å‡º
- è¦ç´„ã‚„è§£é‡ˆã¯è¡Œã‚ãªã„
- è©²å½“ç®‡æ‰€ãŒãªã„å ´åˆã¯ã€Œãªã—ã€ã¨å›ç­”

**ç‰¹åˆ¥æ³¨æ„ï¼š**
- ç›®æ¬¡ãŒè¤‡æ•°ãƒšãƒ¼ã‚¸ã«åˆ†ã‹ã‚Œã¦ã„ã‚‹å ´åˆã€ã™ã¹ã¦ã®ãƒšãƒ¼ã‚¸ã®ç›®æ¬¡éƒ¨åˆ†ã‚’çµ±åˆã—ã¦æŠ½å‡º
- ã€Œç›®æ¬¡ï¼ˆç¶šãï¼‰ã€ã€ŒContents (continued)ã€ãªã©ã‚‚ç›®æ¬¡ã¨ã—ã¦èªè­˜
- ãƒšãƒ¼ã‚¸ç•ªå·ã‚„ç« ç•ªå·ã®é€£ç¶šæ€§ã‹ã‚‰ç›®æ¬¡ã®ç¶™ç¶šã‚’åˆ¤æ–­
- å„ãƒšãƒ¼ã‚¸ã®ç›®æ¬¡éƒ¨åˆ†ã‚’è¦‹ã¤ã‘ãŸå ´åˆã¯ã€ãƒšãƒ¼ã‚¸å¢ƒç•Œã‚’è¶…ãˆã¦çµ±åˆ

**å‡ºåŠ›å½¢å¼ï¼š**
è©²å½“ç®‡æ‰€ãŒã‚ã‚‹å ´åˆï¼š
PAGES: [è©²å½“ãƒšãƒ¼ã‚¸ç•ªå·ã‚’ã‚«ãƒ³ãƒåŒºåˆ‡ã‚Šã§åˆ—æŒ™ï¼ˆä¾‹ï¼š1,2,3ï¼‰]
CONTENT: [æŠ½å‡ºå†…å®¹ï¼ˆè¤‡æ•°ãƒšãƒ¼ã‚¸ã®å ´åˆã¯çµ±åˆï¼‰]

è©²å½“ç®‡æ‰€ãŒãªã„å ´åˆï¼š
PAGES: ãªã—
CONTENT: ãªã—

æ–‡æ›¸å†…å®¹ï¼š
```
{text}
```
    """)
    
    chain = structure_prompt | llm
    result = chain.invoke({"text": merged_text, "pages_count": pages_to_analyze})
    
    # çµæœã‚’ãƒ‘ãƒ¼ã‚¹
    result_text = result.content.strip()
    
    # PAGES: ã¨CONTENT: ã‚’æŠ½å‡º
    pages_info = "ä¸æ˜"
    content = "ãªã—"
    
    lines = result_text.split('\n')
    for line in lines:
        if line.startswith('PAGES:'):
            pages_info = line.replace('PAGES:', '').strip()
        elif line.startswith('CONTENT:'):
            content = line.replace('CONTENT:', '').strip()
            # æ®‹ã‚Šã®è¡Œã‚‚ content ã«å«ã‚ã‚‹
            idx = lines.index(line)
            if idx + 1 < len(lines):
                remaining_lines = lines[idx + 1:]
                content += '\n' + '\n'.join(remaining_lines)
            break
    
    return content, pages_info


def generate_structure_based_summary(structure_content: str, document_name: str) -> str:
    """æŠ½å‡ºã—ãŸæ§‹æˆãƒ»è¦ç‚¹ã‹ã‚‰å…¨ä½“è¦ç´„ã‚’ç”Ÿæˆ"""
    llm = Model().llm()
    
    summary_prompt = PromptTemplate(
        input_variables=["structure", "doc_name"],
        template="""ä»¥ä¸‹ã¯æ–‡æ›¸ã€Œ{doc_name}ã€ã‹ã‚‰æŠ½å‡ºã—ãŸæ§‹æˆãƒ»è¦ç‚¹ã§ã™ã€‚
ã“ã®æƒ…å ±ã‚’åŸºã«æ–‡æ›¸å…¨ä½“ã®è¦ç´„ã‚’ä½œæˆã—ã¦ãã ã•ã„ã€‚

**è¦ç´„æ–¹é‡ï¼š**
- æŠ½å‡ºã•ã‚ŒãŸæ§‹æˆãƒ»è¦ç‚¹ã‹ã‚‰æ–‡æ›¸ã®å…¨ä½“åƒã‚’æŠŠæ¡
- ä¸»è¦ãªãƒ†ãƒ¼ãƒã‚„è«–ç‚¹ã‚’ç‰¹å®š
- æ–‡æ›¸ã®ç›®çš„ã‚„å†…å®¹ã®æ¦‚è¦ã‚’ç°¡æ½”ã«ã¾ã¨ã‚ã‚‹
- å®Ÿéš›ã«è¨˜è¼‰ã•ã‚Œã¦ã„ã‚‹å†…å®¹ã®ã¿ã‚’ä½¿ç”¨

**åˆ¶ç´„ï¼š**
- æ¨æ¸¬ã‚„è£œå®Œã¯è¡Œã‚ãªã„
- æ§‹æˆãƒ»è¦ç‚¹ã‹ã‚‰èª­ã¿å–ã‚Œã‚‹ç¯„å›²ã§ã®è¦ç´„ã«ç•™ã‚ã‚‹
- ç°¡æ½”ã§åˆ†ã‹ã‚Šã‚„ã™ã„æ–‡ç« ã«ã™ã‚‹

æŠ½å‡ºã•ã‚ŒãŸæ§‹æˆãƒ»è¦ç‚¹ï¼š
{structure}

**å‡ºåŠ›å½¢å¼ï¼š**
ã€Œ{doc_name}ã€ï¼š[è¦ç´„å†…å®¹]
    """)
    
    chain = summary_prompt | llm
    result = chain.invoke({"structure": structure_content, "doc_name": document_name})
    return result.content.strip()


def detect_document_type(texts: list[str]) -> tuple[str, str, str, dict]:
    """æ–‡æ›¸ã‚¿ã‚¤ãƒ—ã‚’åˆ¤å®šã™ã‚‹
    
    Args:
        texts: PDFã‹ã‚‰æŠ½å‡ºã•ã‚ŒãŸãƒšãƒ¼ã‚¸åˆ¥ãƒ†ã‚­ã‚¹ãƒˆã®ãƒªã‚¹ãƒˆ
        
    Returns:
        tuple[str, str, str, dict]: (åˆ¤å®šçµæœ, åˆ¤å®šç†ç”±, æ ¹æ‹ ãƒ†ã‚­ã‚¹ãƒˆ, è©³ç´°æƒ…å ±)
            åˆ¤å®šçµæœ: "word" | "powerpoint" | "agenda" | "participants" | "other"
            åˆ¤å®šç†ç”±: é¸æŠã•ã‚ŒãŸã‚«ãƒ†ã‚´ãƒªãƒ¼ã®åˆ¤å®šç†ç”±
            æ ¹æ‹ ãƒ†ã‚­ã‚¹ãƒˆ: åˆ¤å®šã®æ ¹æ‹ ã¨ãªã‚‹ãƒ†ã‚­ã‚¹ãƒˆ
            è©³ç´°æƒ…å ±: {"scores": {...}, "reasoning": {...}, "conclusion": str}
    """
    from langchain.output_parsers import PydanticOutputParser
    from pydantic import BaseModel, Field
    
    class CategoryAnalysis(BaseModel):
        score: int = Field(description="é‡è¦åº¦ã‚¹ã‚³ã‚¢ï¼ˆ1-5ç‚¹ï¼‰", ge=1, le=5)
        reason: str = Field(description="ã‚¹ã‚³ã‚¢ã®ç†ç”±")
        evidence: str = Field(description="æ ¹æ‹ ãƒ†ã‚­ã‚¹ãƒˆä¾‹")
    
    class DocumentTypeAnalysis(BaseModel):
        word: CategoryAnalysis = Field(description="Wordæ–‡æ›¸ã®åˆ†æ")
        powerpoint: CategoryAnalysis = Field(description="PowerPointæ–‡æ›¸ã®åˆ†æ")
        agenda: CategoryAnalysis = Field(description="è­°äº‹æ¬¡ç¬¬ã®åˆ†æ")
        participants: CategoryAnalysis = Field(description="å‚åŠ è€…ä¸€è¦§ã®åˆ†æ")
        news: CategoryAnalysis = Field(description="ãƒ‹ãƒ¥ãƒ¼ã‚¹ãƒ»ãŠçŸ¥ã‚‰ã›ã®åˆ†æ")
        survey: CategoryAnalysis = Field(description="èª¿æŸ»ãƒ»ã‚¢ãƒ³ã‚±ãƒ¼ãƒˆã®åˆ†æ")
        other: CategoryAnalysis = Field(description="ãã®ä»–ã®åˆ†æ")
        conclusion: str = Field(description="æœ€ã‚‚å¯èƒ½æ€§ãŒé«˜ã„ã¨åˆ¤æ–­ã•ã‚Œã‚‹å½¢å¼")
    
    llm = Model().llm()
    parser = PydanticOutputParser(pydantic_object=DocumentTypeAnalysis)
    # æœ€åˆã®æ•°ãƒšãƒ¼ã‚¸ã‚’åˆ†æç”¨ã«å–å¾—ï¼ˆæœ€å¤§5ãƒšãƒ¼ã‚¸ï¼‰
    pages_to_analyze = min(5, len(texts))
    sample_texts = texts[:pages_to_analyze]
    logger.info(f"å…ˆé ­{len(sample_texts)}ãƒšãƒ¼ã‚¸ã‚’åˆ†æã—ã¦æ–‡æ›¸ã‚¿ã‚¤ãƒ—ã‚’åˆ¤å®šã—ã¾ã™")

    # ãƒšãƒ¼ã‚¸æ•°ã«å¿œã˜ã¦ã‚µãƒ³ãƒ—ãƒ«ãƒ†ã‚­ã‚¹ãƒˆã‚’æº–å‚™
    if pages_to_analyze == 1:
        merged_text = f"ãƒšãƒ¼ã‚¸1:\n{sample_texts[0]}"
    else:
        merged_text = "\n\n".join([f"ãƒšãƒ¼ã‚¸{i+1}:\n{text}" for i, text in enumerate(sample_texts)])
    
    # æ–‡æ›¸åˆ¤å®šãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
    detection_prompt = PromptTemplate(
        input_variables=["text", "total_pages", "pages_count", "format_instructions"],
        template="""### ç›®çš„
åˆ†æå¯¾è±¡ã®PDFã‹ã‚‰æŠ½å‡ºã•ã‚ŒãŸãƒ†ã‚­ã‚¹ãƒˆã‚’åˆ†æã—ã€åˆ¤å®šã‚«ãƒ†ã‚´ãƒªãƒ¼ã«æ²¿ã£ã¦å…ƒã®æ–‡æ›¸ã‚¿ã‚¤ãƒ—ã‚’åˆ¤å®šã—ã¦ãã ã•ã„ã€‚

### åˆ¤å®šã‚«ãƒ†ã‚´ãƒªãƒ¼ï¼ˆ7åˆ†é¡ï¼‰

**1. Wordæ–‡æ›¸ (word)**
- é€£ç¶šã—ãŸé•·æ–‡ã‚„æ®µè½æ§‹é€ ã‚’æŒã¤å®Ÿè³ªçš„ãªå†…å®¹
- è­°äº‹éŒ²ï¼ˆç™ºè¨€å†…å®¹ãƒ»è­°è«–ã®è¨˜éŒ²ï¼‰
- è©³ç´°ãªèª¬æ˜ã€åˆ†æã€å ±å‘Šæ›¸
- ã‚¬ã‚¤ãƒ‰ãƒ©ã‚¤ãƒ³ã€ä»•æ§˜æ›¸ã€åˆ¶åº¦èª¬æ˜

**2. PowerPointè³‡æ–™ (powerpoint)**
- ã‚¹ãƒ©ã‚¤ãƒ‰å½¢å¼ã®å®Ÿè³ªçš„ãªãƒ—ãƒ¬ã‚¼ãƒ³ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³å†…å®¹
- åˆ†æçµæœã€ææ¡ˆã€èª¬æ˜è³‡æ–™
- å›³è¡¨ã¨èª¬æ˜ã®çµ„ã¿åˆã‚ã›
- æ¦‚è¦è³‡æ–™ã€ã‚µãƒãƒªãƒ¼è³‡æ–™

**3. è­°äº‹æ¬¡ç¬¬ (agenda)**
- ä¼šè­°ã®è­°äº‹æ¬¡ç¬¬ã€è­°äº‹æ—¥ç¨‹ã€ã‚¢ã‚¸ã‚§ãƒ³ãƒ€
- é–‹å‚¬æ—¥æ™‚ã€å ´æ‰€ã€è­°é¡Œãƒªã‚¹ãƒˆ
- é…ä»˜è³‡æ–™ä¸€è¦§ã€ä¼šè­°é€²è¡Œã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«
- ã€Œè­°äº‹æ¬¡ç¬¬ã€ã€Œã‚¢ã‚¸ã‚§ãƒ³ãƒ€ã€ç­‰ã®ã‚¿ã‚¤ãƒˆãƒ«

**4. å‚åŠ è€…ä¸€è¦§ (participants)**
- å§”å“¡åç°¿ã€å‚åŠ è€…ãƒªã‚¹ãƒˆãŒæ–‡æ›¸ã®ä¸»è¦å†…å®¹ï¼ˆ50%ä»¥ä¸Šï¼‰
- **1ãƒšãƒ¼ã‚¸ç›®ãŒå‚åŠ è€…ä¸€è¦§ã®å ´åˆ**: æ–‡æ›¸ã®ä¸»ç›®çš„ãŒå‚åŠ è€…æƒ…å ±
- **2ãƒšãƒ¼ã‚¸ç›®ä»¥é™ã«å‚åŠ è€…ä¸€è¦§ãŒã‚ã‚‹å ´åˆ**: ä»˜éšè³‡æ–™ã®å¯èƒ½æ€§ãŒé«˜ã„ â†’ agenda ã¨ã—ã¦åˆ¤å®š
- åå‰ã€æ‰€å±ã€å½¹è·ã®ä¸€è¦§ãŒæ–‡æ›¸ã®å¤§éƒ¨åˆ†ã‚’å ã‚ã‚‹
- ã€Œå§”å“¡åç°¿ã€ã€Œå‚åŠ è€…ä¸€è¦§ã€ã€Œå‡ºå¸­è€…ã€ç­‰ã®ã‚¿ã‚¤ãƒˆãƒ«ã§ã€ã‹ã¤å®Ÿéš›ã«å‚åŠ è€…æƒ…å ±ãŒä¸»è¦ã‚³ãƒ³ãƒ†ãƒ³ãƒ„
- **æ³¨æ„**: è­°äº‹æ¬¡ç¬¬ã«å‚åŠ è€…ä¸€è¦§ãŒä»˜éšã—ã¦ã„ã‚‹å ´åˆã¯ã€Œagendaã€ã¨ã—ã¦åˆ¤å®š

**5. ãƒ‹ãƒ¥ãƒ¼ã‚¹ãƒ»ãŠçŸ¥ã‚‰ã› (news)**
- å ±é“ç™ºè¡¨ã€ãƒ—ãƒ¬ã‚¹ãƒªãƒªãƒ¼ã‚¹ã€ãŠçŸ¥ã‚‰ã›
- æ—¥ä»˜ã€éƒ¨ç½²åã€è¦‹å‡ºã—ã€æœ¬æ–‡ã€å•ã„åˆã‚ã›å…ˆã®æ§‹é€ 
- æ”¿ç­–ç™ºè¡¨ã€äº‹æ¥­æ¡ˆå†…ã€å–ã‚Šçµ„ã¿ç´¹ä»‹
- ã€Œå ±é“ç™ºè¡¨ã€ã€Œãƒ—ãƒ¬ã‚¹ãƒªãƒªãƒ¼ã‚¹ã€ã€ŒãŠçŸ¥ã‚‰ã›ã€ç­‰ã®ã‚¿ã‚¤ãƒˆãƒ«
- æ·»ä»˜è³‡æ–™ä¸€è¦§ã€åŒæ™‚ç™ºè¡¨å…ˆã®è¨˜è¼‰

**6. èª¿æŸ»ãƒ»ã‚¢ãƒ³ã‚±ãƒ¼ãƒˆçµæœ (survey)**
- ã‚¢ãƒ³ã‚±ãƒ¼ãƒˆé›†è¨ˆçµæœã€èª¿æŸ»çµæœã€æ„è­˜èª¿æŸ»å ±å‘Š
- è¡¨å½¢å¼ã®ãƒ‡ãƒ¼ã‚¿ãŒæ–‡æ›¸ã®å¤§éƒ¨åˆ†ï¼ˆ70%ä»¥ä¸Šï¼‰ã‚’å ã‚ã‚‹
- è³ªå•é …ç›®ã¨å›ç­”ãƒ‡ãƒ¼ã‚¿ã®æ˜ç¢ºãªå¯¾å¿œé–¢ä¿‚
- ã€Œã‚¢ãƒ³ã‚±ãƒ¼ãƒˆçµæœã€ã€Œèª¿æŸ»çµæœã€ã€Œé›†è¨ˆçµæœã€ã€Œæ„è­˜èª¿æŸ»ã€ã€Œãƒ’ã‚¢ãƒªãƒ³ã‚°ã‚·ãƒ¼ãƒˆã€ç­‰ã®ã‚¿ã‚¤ãƒˆãƒ«
- æ•°å€¤ãƒ‡ãƒ¼ã‚¿ã€ã‚°ãƒ©ãƒ•ã€çµ±è¨ˆæƒ…å ±ãŒä¸»è¦ã‚³ãƒ³ãƒ†ãƒ³ãƒ„
- é¸æŠè‚¢åˆ¥å›ç­”æ•°ã€å›ç­”ç‡ã€é›†è¨ˆå€¤ãªã©ã®å®šé‡çš„ãƒ‡ãƒ¼ã‚¿
- è³ªå•ç•ªå·ï¼ˆQ1ã€å•1ã€è¨­å•1ç­‰ï¼‰ã¨å›ç­”ã®çµ„ã¿åˆã‚ã›
- ç½«ç·šã‚„è¡¨çµ„ã¿ãŒå¤šç”¨ã•ã‚Œã¦ã„ã‚‹æ–‡æ›¸æ§‹é€ 
- ãƒ‡ãƒ¼ã‚¿ã®åˆ†æã‚„è€ƒå¯Ÿã‚ˆã‚Šã‚‚ã€é›†è¨ˆçµæœã®æç¤ºãŒä¸»ç›®çš„

**7. ãã®ä»– (other)**
- æ‰‹æ›¸ãæ–‡æ›¸ã®ã‚¹ã‚­ãƒ£ãƒ³ã€è¤‡é›‘ãªãƒ¬ã‚¤ã‚¢ã‚¦ãƒˆ
- å›³è¡¨ãƒ»ã‚°ãƒ©ãƒ•ã®ã¿ã§èª¬æ˜æ–‡ãŒå°‘ãªã„ï¼ˆèª¿æŸ»çµæœä»¥å¤–ï¼‰
- é–‹å‚¬æ¡ˆå†…ã€äº‹å‹™é€£çµ¡ã€åˆ¤èª­å›°é›£ãªæ–‡æ›¸
- ä¸Šè¨˜6åˆ†é¡ã«æ˜ç¢ºã«è©²å½“ã—ãªã„æ–‡æ›¸

### é‡è¦ãªåŒºåˆ¥ãƒã‚¤ãƒ³ãƒˆ
- è­°äº‹éŒ²ï¼ˆå®Ÿéš›ã®ç™ºè¨€è¨˜éŒ²ï¼‰â†’ word
- è­°äº‹æ¬¡ç¬¬ï¼ˆã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«ã®ã¿ï¼‰â†’ agenda
- è­°äº‹æ¬¡ç¬¬+å‚åŠ è€…ä¸€è¦§ï¼ˆæ··åˆæ–‡æ›¸ï¼‰â†’ agendaï¼ˆè­°äº‹æ¬¡ç¬¬ãŒä¸»ç›®çš„ï¼‰
- å‚åŠ è€…ä¸€è¦§ã®ã¿ï¼ˆåç°¿ãŒä¸»ç›®çš„ï¼‰â†’ participants
- **èª¿æŸ»ãƒ»ã‚¢ãƒ³ã‚±ãƒ¼ãƒˆçµæœï¼ˆè³ªå•ã¨å›ç­”ãƒ‡ãƒ¼ã‚¿ï¼‰â†’ survey**
- **å˜ãªã‚‹ãƒ‡ãƒ¼ã‚¿è¡¨ç¤ºï¼ˆå‚åŠ è€…ä¸€è¦§ãªã©ï¼‰â†’ participants**
- **å®Ÿè³ªçš„ãªèª¿æŸ»çµæœï¼ˆè³ªå•ã¨å›ç­”ã®å¯¾å¿œï¼‰â†’ survey**
- å®Ÿè³ªçš„å†…å®¹ã®æœ‰ç„¡ãŒé‡è¦ãªåˆ¤æ–­åŸºæº–
- **æ–‡æ›¸ã®ä¸»è¦ç›®çš„ã‚’åˆ¤å®šåŸºæº–ã¨ã—ã€ä»˜éšçš„ãªæƒ…å ±ã¯ç„¡è¦–ã™ã‚‹**
- **è¡¨ã®ç›®çš„ãŒé‡è¦ï¼šåç°¿ãƒ»ãƒªã‚¹ãƒˆ vs èª¿æŸ»çµæœãƒ»ãƒ‡ãƒ¼ã‚¿åˆ†æ**

### åˆ¤å®šã®ãƒã‚¤ãƒ³ãƒˆ
- æ–‡ç« ã®é•·ã•ï¼šWordâ†’é•·ã„æ®µè½ã€PowerPointâ†’çŸ­ã„ãƒ•ãƒ¬ãƒ¼ã‚º
- æ§‹é€ ï¼šWordâ†’ç« ç¯€æ§‹é€ ã€PowerPointâ†’ã‚¹ãƒ©ã‚¤ãƒ‰æ§‹é€ 
- ç®‡æ¡æ›¸ãã®ä½¿ç”¨é »åº¦ï¼šPowerPointã§é »ç¹ã«ä½¿ç”¨
- ã‚¿ã‚¤ãƒˆãƒ«ã®æ‰±ã„ï¼šPowerPointã§ã¯å„ãƒšãƒ¼ã‚¸ã«æ˜ç¢ºãªã‚¿ã‚¤ãƒˆãƒ«
- **è¡¨ã®å†…å®¹ï¼šsurveyâ†’èª¿æŸ»ãƒ‡ãƒ¼ã‚¿ãƒ»çµ±è¨ˆã€participantsâ†’äººåãƒ»çµ„ç¹”ã®ãƒªã‚¹ãƒˆ**
- **æ•°å€¤ã®ç¨®é¡ï¼šsurveyâ†’é›†è¨ˆå€¤ãƒ»å‰²åˆãƒ»è©•ä¾¡ç‚¹ã€othersâ†’é€£çµ¡å…ˆãƒ»ç•ªå·**
- **æ–‡æ›¸ã®ç›®çš„ï¼šsurveyâ†’èª¿æŸ»çµæœã®å ±å‘Šã€othersâ†’æƒ…å ±ã®æ•´ç†ãƒ»æä¾›**
- **è³ªå•å½¢å¼ï¼šsurveyâ†’æ˜ç¢ºãªè³ªå•é …ç›®ã¨å›ç­”ã€othersâ†’é …ç›®åã¨ãƒ‡ãƒ¼ã‚¿**

### è¡¨æ§‹é€ ãƒ»ãƒ‡ãƒ¼ã‚¿æ–‡æ›¸ã®ç‰¹åˆ¥ãªåˆ¤å®šåŸºæº–
ä»¥ä¸‹ã®ç‰¹å¾´ãŒå¤šãè¦‹ã‚‰ã‚Œã‚‹å ´åˆã¯ survey ã‚«ãƒ†ã‚´ãƒªãƒ¼ã‚’æ¤œè¨ï¼š
- è³ªå•ç•ªå·ã‚„è¨­å•ç•ªå·ï¼ˆQ1ã€å•1ã€è¨­å•1ã€è³ªå•1ç­‰ï¼‰
- å›ç­”é¸æŠè‚¢ï¼ˆã¯ã„/ã„ã„ãˆã€æº€è¶³/ä¸æº€è¶³ã€1-5æ®µéšè©•ä¾¡ç­‰ï¼‰
- é›†è¨ˆç”¨èªï¼ˆåˆè¨ˆã€å¹³å‡ã€å‰²åˆã€ä»¶æ•°ã€å›ç­”è€…æ•°ã€ãƒ‘ãƒ¼ã‚»ãƒ³ãƒˆç­‰ï¼‰
- èª¿æŸ»é–¢é€£ç”¨èªï¼ˆå›ç­”ã€å›ç­”è€…ã€å¯¾è±¡è€…ã€ã‚µãƒ³ãƒ—ãƒ«ã€èª¿æŸ»æœŸé–“ç­‰ï¼‰
- è¡¨å½¢å¼ã®å›ç­”ãƒ‡ãƒ¼ã‚¿ï¼ˆæ•°å€¤ã€å‰²åˆã€ã‚°ãƒ©ãƒ•ç­‰ï¼‰
- ç½«ç·šã‚„è¡¨çµ„ã¿ãŒæ–‡æ›¸ã®å¤§éƒ¨åˆ†ã‚’å ã‚ã‚‹æ§‹é€ 

### å‡ºåŠ›å½¢å¼ï¼ˆå¿…ãšä»¥ä¸‹ã®å½¢å¼ã§å‡ºåŠ›ã—ã¦ãã ã•ã„ï¼‰
**ã‚¹ã‚³ã‚¢åˆ†æ:**

**Word:**
**ã‚¹ã‚³ã‚¢**: 3
**ç†ç”±**: æ–‡æ›¸ã«å«ã¾ã‚Œã‚‹æ–‡å­—æƒ…å ±ã®ç‰¹å¾´ã‚’å…·ä½“çš„ã«èª¬æ˜
**æ ¹æ‹ ãƒ†ã‚­ã‚¹ãƒˆä¾‹**: ã€Œæ–‡æ›¸ã‹ã‚‰å¼•ç”¨ã—ãŸå…·ä½“çš„ãªãƒ†ã‚­ã‚¹ãƒˆã€

**PowerPoint:**
**ã‚¹ã‚³ã‚¢**: 5
**ç†ç”±**: ãƒ—ãƒ¬ã‚¼ãƒ³ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³å½¢å¼ã®ç‰¹å¾´ã‚’å…·ä½“çš„ã«èª¬æ˜
**æ ¹æ‹ ãƒ†ã‚­ã‚¹ãƒˆä¾‹**: ã€Œæ–‡æ›¸ã‹ã‚‰å¼•ç”¨ã—ãŸå…·ä½“çš„ãªãƒ†ã‚­ã‚¹ãƒˆã€

**Agenda:**
**ã‚¹ã‚³ã‚¢**: 1
**ç†ç”±**: è­°é¡Œå½¢å¼ã®ç‰¹å¾´ã‚’å…·ä½“çš„ã«èª¬æ˜
**æ ¹æ‹ ãƒ†ã‚­ã‚¹ãƒˆä¾‹**: ã€Œæ–‡æ›¸ã‹ã‚‰å¼•ç”¨ã—ãŸå…·ä½“çš„ãªãƒ†ã‚­ã‚¹ãƒˆã€

**Participants:**
**ã‚¹ã‚³ã‚¢**: 2
**ç†ç”±**: å‚åŠ è€…æƒ…å ±ã®ç‰¹å¾´ã‚’å…·ä½“çš„ã«èª¬æ˜
**æ ¹æ‹ ãƒ†ã‚­ã‚¹ãƒˆä¾‹**: ã€Œæ–‡æ›¸ã‹ã‚‰å¼•ç”¨ã—ãŸå…·ä½“çš„ãªãƒ†ã‚­ã‚¹ãƒˆã€

**News:**
**ã‚¹ã‚³ã‚¢**: 1
**ç†ç”±**: ãƒ‹ãƒ¥ãƒ¼ã‚¹å½¢å¼ã®ç‰¹å¾´ã‚’å…·ä½“çš„ã«èª¬æ˜
**æ ¹æ‹ ãƒ†ã‚­ã‚¹ãƒˆä¾‹**: ã€Œæ–‡æ›¸ã‹ã‚‰å¼•ç”¨ã—ãŸå…·ä½“çš„ãªãƒ†ã‚­ã‚¹ãƒˆã€

**Survey:**
**ã‚¹ã‚³ã‚¢**: 1
**ç†ç”±**: ã‚¢ãƒ³ã‚±ãƒ¼ãƒˆå½¢å¼ã®ç‰¹å¾´ã‚’å…·ä½“çš„ã«èª¬æ˜
**æ ¹æ‹ ãƒ†ã‚­ã‚¹ãƒˆä¾‹**: ã€Œæ–‡æ›¸ã‹ã‚‰å¼•ç”¨ã—ãŸå…·ä½“çš„ãªãƒ†ã‚­ã‚¹ãƒˆã€

**Other:**
**ã‚¹ã‚³ã‚¢**: 2
**ç†ç”±**: ãã®ä»–ã®å½¢å¼ã®ç‰¹å¾´ã‚’å…·ä½“çš„ã«èª¬æ˜
**æ ¹æ‹ ãƒ†ã‚­ã‚¹ãƒˆä¾‹**: ã€Œæ–‡æ›¸ã‹ã‚‰å¼•ç”¨ã—ãŸå…·ä½“çš„ãªãƒ†ã‚­ã‚¹ãƒˆã€

**çµè«–:**
æœ€ã‚‚å¯èƒ½æ€§ãŒé«˜ã„ã¨åˆ¤æ–­ã•ã‚Œã‚‹å½¢å¼: powerpoint

### é‡è¦ãªæ³¨æ„äº‹é …
- å¿…ãš7ã¤ã®ã‚«ãƒ†ã‚´ãƒªãƒ¼å…¨ã¦ã«ã€Œ**ã‚¹ã‚³ã‚¢**:ã€ã€Œ**ç†ç”±**:ã€ã€Œ**æ ¹æ‹ ãƒ†ã‚­ã‚¹ãƒˆä¾‹**:ã€ã‚’è¨˜è¼‰ã—ã¦ãã ã•ã„
- ã‚¹ã‚³ã‚¢ã¯1-5ã®æ•°å­—ã®ã¿ã‚’è¨˜è¼‰ã—ã¦ãã ã•ã„
- ç†ç”±ã¯æ–‡æ›¸ã®ç‰¹å¾´ã‚’å…·ä½“çš„ã«èª¬æ˜ã—ã¦ãã ã•ã„
- æ ¹æ‹ ãƒ†ã‚­ã‚¹ãƒˆä¾‹ã¯æ–‡æ›¸ã®å†…å®¹ã‹ã‚‰å®Ÿéš›ã®ãƒ†ã‚­ã‚¹ãƒˆã‚’å¼•ç”¨ã—ã¦ãã ã•ã„

### å‡ºåŠ›è¦ä»¶
- å„ã‚¹ã‚³ã‚¢ï¼ˆ1ï½5ï¼‰ã¯ã€è¨˜è¿°ã•ã‚ŒãŸç†ç”±ã¨ä¸€è²«æ€§ã‚’ä¿ã£ã¦ãã ã•ã„ã€‚
- è¤‡æ•°ã‚«ãƒ†ã‚´ãƒªãƒ¼ãŒåŒã˜é«˜ã‚¹ã‚³ã‚¢ã«ãªã‚‰ãªã„ã‚ˆã†æ³¨æ„ã—ã¦ãã ã•ã„ã€‚
- æœ€å¾Œã«ã€æœ€ã‚‚å¯èƒ½æ€§ãŒé«˜ã„ã‚«ãƒ†ã‚´ãƒªãƒ¼ã‚’1ã¤æ˜ç¤ºã—ã¦ãã ã•ã„ã€‚

### åˆ†æå¯¾è±¡
ç·ãƒšãƒ¼ã‚¸æ•°: {total_pages}ãƒšãƒ¼ã‚¸
åˆ†æå¯¾è±¡: æœ€åˆã®{pages_count}ãƒšãƒ¼ã‚¸

PDFãƒ†ã‚­ã‚¹ãƒˆ:
{text}

### å‡ºåŠ›ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ
ä»¥ä¸‹ã®JSONå½¢å¼ã§å‡ºåŠ›ã—ã¦ãã ã•ã„ï¼š

{format_instructions}
    """)
    
    chain = detection_prompt | llm | parser
    result = chain.invoke({
        "text": merged_text, 
        "total_pages": len(texts),
        "pages_count": pages_to_analyze,
        "format_instructions": parser.get_format_instructions()
    })
    
    # Pydanticã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‹ã‚‰æƒ…å ±ã‚’æŠ½å‡º
    scores = {
        "Word": result.word.score,
        "PowerPoint": result.powerpoint.score,
        "Agenda": result.agenda.score,
        "Participants": result.participants.score,
        "News": result.news.score,
        "Survey": result.survey.score,
        "Other": result.other.score
    }
    
    reasoning = {
        "Word": result.word.reason,
        "PowerPoint": result.powerpoint.reason,
        "Agenda": result.agenda.reason,
        "Participants": result.participants.reason,
        "News": result.news.reason,
        "Survey": result.survey.reason,
        "Other": result.other.reason
    }
    
    evidence = {
        "Word": result.word.evidence,
        "PowerPoint": result.powerpoint.evidence,
        "Agenda": result.agenda.evidence,
        "Participants": result.participants.evidence,
        "News": result.news.evidence,
        "Survey": result.survey.evidence,
        "Other": result.other.evidence
    }
    
    conclusion = result.conclusion
    
    # åˆ¤å®šçµæœã‚’ãƒãƒƒãƒ”ãƒ³ã‚°ï¼ˆ7ã‚«ãƒ†ã‚´ãƒªï¼‰
    doc_type = None
    doc_reason = ""
    
    # ã‚«ãƒ†ã‚´ãƒªãƒ¼åã¨ã‚¿ã‚¤ãƒ—ã®ãƒãƒƒãƒ”ãƒ³ã‚°
    category_mapping = {
        "word": "Word",
        "powerpoint": "PowerPoint", 
        "agenda": "Agenda",
        "participants": "Participants",
        "news": "News",
        "survey": "Survey",
        "other": "Other"
    }
    
    if scores:
        # æœ€é«˜ã‚¹ã‚³ã‚¢ã®ã‚«ãƒ†ã‚´ãƒªãƒ¼ã‚’ç‰¹å®š
        max_score = max(scores.values())
        max_categories = [cat for cat, score in scores.items() if score == max_score]
        
        if max_categories:
            top_category = max_categories[0]  # è¤‡æ•°ã‚ã‚‹å ´åˆã¯æœ€åˆã®ä¸€ã¤
            
            # ã‚«ãƒ†ã‚´ãƒªãƒ¼åã‹ã‚‰ doc_type ã‚’æ±ºå®š
            for doc_type_key, category_name in category_mapping.items():
                if category_name in top_category:
                    doc_type = doc_type_key
                    doc_reason = reasoning.get(category_name, "ä¸æ˜")
                    selected_evidence = evidence.get(category_name, "ä¸æ˜")
                    break

    if doc_type is None:
        # ã‚¹ã‚³ã‚¢ã‹ã‚‰æ–‡æ›¸ã‚¿ã‚¤ãƒ—ã‚’å¾—ã‚‰ã‚Œãªã‹ã£ãŸå ´åˆã¯çµè«–ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰ç›´æ¥åˆ¤å®š
        conclusion_lower = conclusion.lower()
        for doc_type_key in category_mapping.keys():
            if doc_type_key in conclusion_lower:
                doc_type = doc_type_key
                doc_reason = "çµè«–ã‹ã‚‰åˆ¤å®š"
                selected_evidence = "ãªã—"
                break
    
    # è©³ç´°æƒ…å ±ã‚’ã¾ã¨ã‚ã‚‹
    detail_info = {
        "scores": scores,
        "reasoning": reasoning,
        "conclusion": conclusion,
        "total_pages": len(texts),
        "analyzed_pages": pages_to_analyze
    }

    sorted_detail_info = sorted(detail_info["scores"].items(), key=lambda x: x[1], reverse=True)
    logger.info(f"æ–‡æ›¸ã‚¿ã‚¤ãƒ—ã‚’æ¨å®šã—ã¾ã—ãŸ: {', '.join([f'{cat}:{score}' for cat, score in sorted_detail_info])}")
    logger.info(f"{doc_type}ã®æ¨å®šç†ç”±: {doc_reason}")
    logger.info(f"{doc_type}ã®æ ¹æ‹ : {selected_evidence}")

    return doc_type, doc_reason, selected_evidence, detail_info


def extract_word_title(texts: list[str]) -> str:
    """Wordæ–‡æ›¸ã®ã‚¿ã‚¤ãƒˆãƒ«ã‚’æŠ½å‡ºã™ã‚‹
    
    Args:
        texts: PDFã‹ã‚‰æŠ½å‡ºã•ã‚ŒãŸãƒ†ã‚­ã‚¹ãƒˆã®ãƒªã‚¹ãƒˆ
        
    Returns:
        str: æŠ½å‡ºã•ã‚ŒãŸã‚¿ã‚¤ãƒˆãƒ«
    """
    llm = Model().llm()
    
    # æœ€åˆã®5ãƒšãƒ¼ã‚¸ã‚’å–å¾—
    title_pages = min(5, len(texts))
    merged_text = "\n\n".join([f"--- ãƒšãƒ¼ã‚¸ {i+1} ---\n{text}" for i, text in enumerate(texts[:title_pages])])
    
    title_prompt = PromptTemplate(
        input_variables=["text", "pages"],
        template="""ä»¥ä¸‹ã¯Wordæ–‡æ›¸ã®æœ€åˆã®{pages}ãƒšãƒ¼ã‚¸ã§ã™ã€‚æ–‡æ›¸ã®ã‚¿ã‚¤ãƒˆãƒ«ã‚’æŠ½å‡ºã—ã¦ãã ã•ã„ã€‚

{text}

### ã‚¿ã‚¤ãƒˆãƒ«æŠ½å‡ºåŸºæº–
- è¡¨ç´™ã‚„å†’é ­ã«è¨˜è¼‰ã•ã‚ŒãŸæ–‡æ›¸ã®ä¸»ã‚¿ã‚¤ãƒˆãƒ«
- å‰¯é¡ŒãŒã‚ã‚‹å ´åˆã¯ä¸»ã‚¿ã‚¤ãƒˆãƒ«ã®ã¿
- ç« ã‚¿ã‚¤ãƒˆãƒ«ã§ã¯ãªãæ–‡æ›¸å…¨ä½“ã®ã‚¿ã‚¤ãƒˆãƒ«
- çµ„ç¹”åã‚„æ—¥ä»˜ã¯é™¤å¤–

### å‡ºåŠ›å½¢å¼
ã‚¿ã‚¤ãƒˆãƒ«ã®ã¿ã‚’å‡ºåŠ›ã—ã¦ãã ã•ã„ï¼ˆèª¬æ˜ã‚„å‰ç½®ãã¯ä¸è¦ï¼‰
    """)
    
    chain = title_prompt | llm
    result = chain.invoke({"text": merged_text, "pages": title_pages})
    extracted_title = result.content.strip()
    logger.info(f"Wordã‚¿ã‚¤ãƒˆãƒ«æŠ½å‡º: {extracted_title}")
    return extracted_title


def extract_word_table_of_contents(texts: list[str]) -> str:
    """Wordæ–‡æ›¸ã®ç›®æ¬¡ã‚’æŠ½å‡ºã™ã‚‹
    
    Args:
        texts: PDFã‹ã‚‰æŠ½å‡ºã•ã‚ŒãŸãƒšãƒ¼ã‚¸åˆ¥ãƒ†ã‚­ã‚¹ãƒˆã®ãƒªã‚¹ãƒˆ
        
    Returns:
        str: æŠ½å‡ºã•ã‚ŒãŸç›®æ¬¡ï¼ˆæ§‹é€ åŒ–ã•ã‚ŒãŸãƒ†ã‚­ã‚¹ãƒˆï¼‰
    """
    llm = Model().llm()
    
    # æœ€åˆã®10ãƒšãƒ¼ã‚¸ã‹ã‚‰ç›®æ¬¡ã‚’æŠ½å‡º
    toc_pages = min(10, len(texts))
    merged_text = "\n\n".join([f"--- ãƒšãƒ¼ã‚¸ {i+1} ---\n{text}" for i, text in enumerate(texts[:toc_pages])])
    
    toc_prompt = PromptTemplate(
        input_variables=["text", "pages"],
        template="""ä»¥ä¸‹ã¯Wordæ–‡æ›¸ã®æœ€åˆã®{pages}ãƒšãƒ¼ã‚¸ã§ã™ã€‚ç›®æ¬¡éƒ¨åˆ†ã‚’æŠ½å‡ºã—ã¦ãã ã•ã„ã€‚

{text}

### ç›®æ¬¡æŠ½å‡ºåŸºæº–
- ã€Œç›®æ¬¡ã€ã€ŒContentsã€ãªã©ã®è¦‹å‡ºã—ãŒã‚ã‚‹éƒ¨åˆ†
- ç« ãƒ»ç¯€ãƒ»é …ã®æ§‹é€ ã‚’æŒã¤ãƒªã‚¹ãƒˆ
- ãƒšãƒ¼ã‚¸ç•ªå·ãŒè¨˜è¼‰ã•ã‚Œã¦ã„ã‚‹é …ç›®ãƒªã‚¹ãƒˆ
- æ–‡æ›¸ã®æ§‹æˆã‚’ç¤ºã™éšå±¤çš„ãªãƒªã‚¹ãƒˆ

### æŠ½å‡ºå¯¾è±¡å¤–
- å˜ãªã‚‹ç®‡æ¡æ›¸ã
- å›³è¡¨ãƒªã‚¹ãƒˆ
- å‚è€ƒæ–‡çŒ®ãƒªã‚¹ãƒˆ
- é…ä»˜è³‡æ–™ä¸€è¦§

### å‡ºåŠ›å½¢å¼
ç›®æ¬¡ãŒè¦‹ã¤ã‹ã£ãŸå ´åˆ:
```
ç¬¬1ç«  â—‹â—‹â—‹
  1.1 â—‹â—‹â—‹
  1.2 â—‹â—‹â—‹
ç¬¬2ç«  â—‹â—‹â—‹
  2.1 â—‹â—‹â—‹
```

ç›®æ¬¡ãŒè¦‹ã¤ã‹ã‚‰ãªã„å ´åˆ:
ç›®æ¬¡ãªã—

### å‡ºåŠ›è¦ä»¶
- ç›®æ¬¡ã®éšå±¤æ§‹é€ ã‚’ä¿æŒ
- ãƒšãƒ¼ã‚¸ç•ªå·ã¯é™¤å¤–
- ç°¡æ½”ã§èª­ã¿ã‚„ã™ã„å½¢å¼
    """)
    
    chain = toc_prompt | llm
    result = chain.invoke({"text": merged_text, "pages": toc_pages})
    
    extracted_toc = result.content.strip()
    logger.info(f"ğŸ“‹ Wordç›®æ¬¡æŠ½å‡º: {len(extracted_toc)}æ–‡å­—")
    
    return extracted_toc


def create_summary_from_toc(title: str, table_of_contents: str) -> str:
    """ç›®æ¬¡ã‹ã‚‰è¦ç´„ã‚’ä½œæˆã™ã‚‹
    
    Args:
        title: æ–‡æ›¸ã®ã‚¿ã‚¤ãƒˆãƒ«
        table_of_contents: æŠ½å‡ºã•ã‚ŒãŸç›®æ¬¡
        
    Returns:
        str: ç›®æ¬¡ãƒ™ãƒ¼ã‚¹ã®è¦ç´„
    """
    llm = Model().llm()
    
    toc_summary_prompt = PromptTemplate(
        input_variables=["title", "toc"],
        template="""ä»¥ä¸‹ã®Wordæ–‡æ›¸ã®ã‚¿ã‚¤ãƒˆãƒ«ã¨ç›®æ¬¡ã‹ã‚‰ã€æ–‡æ›¸ã®è¦ç´„ã‚’ä½œæˆã—ã¦ãã ã•ã„ã€‚

### æ–‡æ›¸ã‚¿ã‚¤ãƒˆãƒ«
{title}

### ç›®æ¬¡
{toc}

### è¦ç´„ä½œæˆã®æ–¹é‡
- ç›®æ¬¡ã®æ§‹é€ ã‹ã‚‰æ–‡æ›¸ã®å…¨ä½“åƒã‚’æŠŠæ¡
- å„ç« ãƒ»ç¯€ã®å†…å®¹ã‚’æ¨æ¸¬ã—ã¦è«–ç†çš„ãªæµã‚Œã‚’æ§‹ç¯‰
- æ–‡æ›¸ã®ç›®çš„ãƒ»èƒŒæ™¯ãƒ»ä¸»è¦è«–ç‚¹ãƒ»çµè«–ã‚’æ•´ç†
- å…·ä½“çš„ãªå†…å®¹ã¯æ¨æ¸¬ã›ãšã€æ§‹é€ ã«åŸºã¥ã„ãŸæ¦‚è¦ã‚’è¨˜è¿°

### å‡ºåŠ›å½¢å¼
ã€Œ{title}ã€ï¼š
- æ–‡æ›¸ã®ç›®çš„ãƒ»ä½ç½®ã¥ã‘
- ä¸»è¦ãªæ¤œè¨é …ç›®ãƒ»è«–ç‚¹ï¼ˆç›®æ¬¡ã®ç« æ§‹æˆã«åŸºã¥ãï¼‰
- æ–‡æ›¸ã®ç‰¹å¾´ãƒ»æ€§æ ¼

### åˆ¶ç´„
- ç°¡æ½”ã§åˆ†ã‹ã‚Šã‚„ã™ãï¼ˆ3-5æ–‡ç¨‹åº¦ï¼‰
- ç›®æ¬¡ã«ãªã„å…·ä½“çš„å†…å®¹ã¯æ¨æ¸¬ã—ãªã„
- æ–‡æ›¸ã®æ§‹é€ ã¨è«–ç†çš„ãªæµã‚Œã‚’é‡è¦–
- ã€Œæ¤œè¨ã€ã€Œåˆ†æã€ã€Œææ¡ˆã€ç­‰ã®æ€§æ ¼ã‚’æ˜ç¤º
    """)
    
    chain = toc_summary_prompt | llm
    result = chain.invoke({
        "title": title,
        "toc": table_of_contents
    })
    
    summary = result.content.strip()
    logger.info(f"ğŸ“„ ç›®æ¬¡ã‹ã‚‰è¦ç´„ç”Ÿæˆ: {len(summary)}æ–‡å­—")
    
    return summary


def agenda_summarize(texts: list[str]) -> dict:
    """è­°äº‹æ¬¡ç¬¬ã®è¦ç´„å‡¦ç†
    
    Args:
        texts: PDFã‹ã‚‰æŠ½å‡ºã•ã‚ŒãŸãƒšãƒ¼ã‚¸åˆ¥ãƒ†ã‚­ã‚¹ãƒˆã®ãƒªã‚¹ãƒˆ
        
    Returns:
        dict: {"title": str, "summary": str}
    """
    llm = Model().llm()
    
    # å…¨æ–‡ã‚’çµåˆ
    merged_text = "\n\n".join([f"--- ãƒšãƒ¼ã‚¸ {i+1} ---\n{text}" for i, text in enumerate(texts)])
    
    # ã‚¿ã‚¤ãƒˆãƒ«æŠ½å‡º
    title_prompt = PromptTemplate(
        input_variables=["text"],
        template="""ä»¥ä¸‹ã®è­°äº‹æ¬¡ç¬¬ã‹ã‚‰ä¼šè­°åã‚’æŠ½å‡ºã—ã¦ãã ã•ã„ã€‚

{text}

### æŠ½å‡ºåŸºæº–
- æ–‡æ›¸ã®ä¸»ã‚¿ã‚¤ãƒˆãƒ«ã¨ãªã‚‹ä¼šè­°å
- å›æ•°ã‚„æ—¥ä»˜ã¯é™¤å¤–
- çµ„ç¹”åã¯å«ã‚ã¦ã‚‚å¯

### å‡ºåŠ›å½¢å¼
ä¼šè­°åã®ã¿ã‚’å‡ºåŠ›ã—ã¦ãã ã•ã„ï¼ˆèª¬æ˜ã‚„å‰ç½®ãã¯ä¸è¦ï¼‰
    """)
    
    title_chain = title_prompt | llm
    title_result = title_chain.invoke({"text": merged_text})
    title = title_result.content.strip()
    
    # è¦ç´„ä½œæˆ
    agenda_prompt = PromptTemplate(
        input_variables=["text"],
        template="""ä»¥ä¸‹ã¯è­°äº‹æ¬¡ç¬¬ã§ã™ã€‚åŸºæœ¬æƒ…å ±ã‚’ç°¡æ½”ã«è¦ç´„ã—ã¦ãã ã•ã„ã€‚

{text}

### æŠ½å‡ºé …ç›®
- ä¼šè­°å
- é–‹å‚¬æ—¥æ™‚
- é–‹å‚¬å ´æ‰€
- ä¸»è¦è­°é¡Œï¼ˆæœ€å¤§3é …ç›®ï¼‰

### å‡ºåŠ›å½¢å¼
ã€Œ[ä¼šè­°å]ã€ã®è­°äº‹æ¬¡ç¬¬ï¼ˆ[é–‹å‚¬æ—¥æ™‚]ã€[å ´æ‰€]ï¼‰ï¼šä¸»è¦è­°é¡Œã¯[è­°é¡Œ1]ã€[è­°é¡Œ2]ç­‰

### åˆ¶ç´„
- 1-2æ–‡ã§ç°¡æ½”ã«
- æ—¥æ™‚ã¯ã€Œä»¤å’Œâ—‹å¹´â—‹æœˆâ—‹æ—¥ã€å½¢å¼ã§
    """)
    
    chain = agenda_prompt | llm
    result = chain.invoke({"text": merged_text})
    summary = result.content.strip()
    
    return {"title": title, "summary": summary}


def news_based_summarize(texts: list[str]) -> dict:
    """ãƒ‹ãƒ¥ãƒ¼ã‚¹ãƒ»ãŠçŸ¥ã‚‰ã›ï¼ˆãƒ—ãƒ¬ã‚¹ãƒªãƒªãƒ¼ã‚¹ï¼‰ã®è¦ç´„å‡¦ç†
    
    Args:
        texts: PDFã‹ã‚‰æŠ½å‡ºã•ã‚ŒãŸãƒšãƒ¼ã‚¸åˆ¥ãƒ†ã‚­ã‚¹ãƒˆã®ãƒªã‚¹ãƒˆ
        
    Returns:
        dict: {"title": str, "summary": str}
    """
    llm = Model().llm()
    
    # å…¨æ–‡ã‚’çµåˆ
    merged_text = "\n\n".join([f"--- ãƒšãƒ¼ã‚¸ {i+1} ---\n{text}" for i, text in enumerate(texts)])
    
    # ã‚¿ã‚¤ãƒˆãƒ«æŠ½å‡º
    title_prompt = PromptTemplate(
        input_variables=["text"],
        template="""ä»¥ä¸‹ã®ãƒ—ãƒ¬ã‚¹ãƒªãƒªãƒ¼ã‚¹ãƒ»å ±é“ç™ºè¡¨ãƒ»ãŠçŸ¥ã‚‰ã›ã‹ã‚‰ä¸»è¦ã‚¿ã‚¤ãƒˆãƒ«ã‚’æŠ½å‡ºã—ã¦ãã ã•ã„ã€‚

{text}

### æŠ½å‡ºåŸºæº–
- æ–‡æ›¸ã®ä¸»ã‚¿ã‚¤ãƒˆãƒ«ï¼ˆè¦‹å‡ºã—ï¼‰
- æ—¥ä»˜ã‚„éƒ¨ç½²åã¯é™¤å¤–
- ã€Œå ±é“ç™ºè¡¨ã€ã€ŒãŠçŸ¥ã‚‰ã›ã€ç­‰ã®æ¥é ­èªã¯é™¤å¤–

### å‡ºåŠ›å½¢å¼
ã‚¿ã‚¤ãƒˆãƒ«ã®ã¿ã‚’å‡ºåŠ›ã—ã¦ãã ã•ã„ï¼ˆèª¬æ˜ã‚„å‰ç½®ãã¯ä¸è¦ï¼‰
    """)
    
    title_chain = title_prompt | llm
    title_result = title_chain.invoke({"text": merged_text})
    title = title_result.content.strip()
    
    # è¦ç´„ä½œæˆ
    news_prompt = PromptTemplate(
        input_variables=["text"],
        template="""ä»¥ä¸‹ã¯ãƒ—ãƒ¬ã‚¹ãƒªãƒªãƒ¼ã‚¹ãƒ»å ±é“ç™ºè¡¨ãƒ»ãŠçŸ¥ã‚‰ã›ã§ã™ã€‚é‡è¦ãªæƒ…å ±ã‚’ç°¡æ½”ã«è¦ç´„ã—ã¦ãã ã•ã„ã€‚

{text}

### æŠ½å‡ºé …ç›®
- ç™ºè¡¨å†…å®¹ãƒ»äº‹æ¥­å
- ç™ºè¡¨å…ƒï¼ˆéƒ¨ç½²ãƒ»çµ„ç¹”åï¼‰
- ä¸»è¦ãªå†…å®¹ãƒ»ç›®çš„
- é–‹å§‹æ™‚æœŸãƒ»å®Ÿæ–½äºˆå®šï¼ˆè©²å½“ã™ã‚‹å ´åˆï¼‰
- èƒŒæ™¯ãƒ»æ„ç¾©ï¼ˆç°¡æ½”ã«ï¼‰

### å‡ºåŠ›å½¢å¼
ã€Œ[ã‚¿ã‚¤ãƒˆãƒ«]ã€ï¼š[ç™ºè¡¨å…ƒ]ãŒ[ç™ºè¡¨å†…å®¹]ã«ã¤ã„ã¦ç™ºè¡¨ã€‚[ä¸»è¦ãªå†…å®¹ãƒ»ç›®çš„]ã‚’[æ™‚æœŸ]ã«å®Ÿæ–½äºˆå®šã€‚[èƒŒæ™¯ãƒ»æ„ç¾©]

### åˆ¶ç´„
- 2-3æ–‡ã§ç°¡æ½”ã«
- å…·ä½“çš„ãªäº‹å®Ÿã®ã¿ã‚’è¨˜è¼‰
- å•ã„åˆã‚ã›å…ˆã‚„æŠ€è¡“çš„è©³ç´°ã¯é™¤å¤–
- æ—¥ä»˜ã¯ã€Œä»¤å’Œâ—‹å¹´â—‹æœˆâ—‹æ—¥ã€å½¢å¼ã§
    """)
    
    chain = news_prompt | llm
    result = chain.invoke({"text": merged_text})
    summary = result.content.strip()
    
    return {"title": title, "summary": summary}


def participants_summarize(texts: list[str]) -> dict:
    """å‚åŠ è€…ä¸€è¦§ã®è¦ç´„å‡¦ç†
    
    Args:
        texts: PDFã‹ã‚‰æŠ½å‡ºã•ã‚ŒãŸãƒšãƒ¼ã‚¸åˆ¥ãƒ†ã‚­ã‚¹ãƒˆã®ãƒªã‚¹ãƒˆ
        
    Returns:
        dict: {"title": str, "summary": str}
    """
    llm = Model().llm()
    
    # å…¨æ–‡ã‚’çµåˆ
    merged_text = "\n\n".join([f"--- ãƒšãƒ¼ã‚¸ {i+1} ---\n{text}" for i, text in enumerate(texts)])
    
    # ã‚¿ã‚¤ãƒˆãƒ«æŠ½å‡º
    title_prompt = PromptTemplate(
        input_variables=["text"],
        template="""ä»¥ä¸‹ã®å‚åŠ è€…ä¸€è¦§ãƒ»å§”å“¡åç°¿ã‹ã‚‰å§”å“¡ä¼šãƒ»ä¼šè­°åã‚’æŠ½å‡ºã—ã¦ãã ã•ã„ã€‚

{text}

### æŠ½å‡ºåŸºæº–
- å§”å“¡ä¼šãƒ»ä¼šè­°ã®æ­£å¼åç§°
- å›æ•°ã‚„æ—¥ä»˜ã¯é™¤å¤–
- çµ„ç¹”åã¯å«ã‚ã¦ã‚‚å¯

### å‡ºåŠ›å½¢å¼
å§”å“¡ä¼šãƒ»ä¼šè­°åã®ã¿ã‚’å‡ºåŠ›ã—ã¦ãã ã•ã„ï¼ˆèª¬æ˜ã‚„å‰ç½®ãã¯ä¸è¦ï¼‰
    """)

    title_chain = title_prompt | llm
    title_result = title_chain.invoke({"text": merged_text})
    title = title_result.content.strip()
    
    # è¦ç´„ä½œæˆ
    participants_prompt = PromptTemplate(
        input_variables=["text"],
        template="""ä»¥ä¸‹ã¯å‚åŠ è€…ä¸€è¦§ãƒ»å§”å“¡åç°¿ã§ã™ã€‚åŸºæœ¬æƒ…å ±ã‚’ç°¡æ½”ã«è¦ç´„ã—ã¦ãã ã•ã„ã€‚

{text}

### æŠ½å‡ºé …ç›®
- å§”å“¡ä¼šãƒ»ä¼šè­°å
- ç·å‚åŠ è€…æ•°
- ä¸»è¦ãƒ¡ãƒ³ãƒãƒ¼ï¼ˆåº§é•·ã€å§”å“¡é•·ç­‰ã®å½¹è·è€…ï¼‰

### å‡ºåŠ›å½¢å¼
ã€Œ[å§”å“¡ä¼šå]ã€å§”å“¡åç°¿ï¼šå§”å“¡[â—‹]åã€åº§é•·ã¯[æ°å]ï¼ˆ[æ‰€å±]ï¼‰

### åˆ¶ç´„
- 1æ–‡ã§ç°¡æ½”ã«
- å½¹è·è€…ãŒè¤‡æ•°ã„ã‚‹å ´åˆã¯ä»£è¡¨è€…ã®ã¿
    """)
    
    chain = participants_prompt | llm
    result = chain.invoke({"text": merged_text})
    summary = result.content.strip()
    
    return {"title": title, "summary": summary}


def word_based_summarize(texts: list[str]) -> dict:
    """Wordãƒ™ãƒ¼ã‚¹æ–‡æ›¸ã®è¦ç´„å‡¦ç†
    
    ã‚¿ã‚¤ãƒˆãƒ«ã¨ç›®æ¬¡ã‹ã‚‰æ–‡æ›¸ã®å…¨ä½“æ§‹é€ ã‚’æŠŠæ¡ã—ã€æ§‹é€ ãƒ™ãƒ¼ã‚¹ã®è¦ç´„ã‚’ç”Ÿæˆ
    
    Args:
        texts: PDFã‹ã‚‰æŠ½å‡ºã•ã‚ŒãŸãƒšãƒ¼ã‚¸åˆ¥ãƒ†ã‚­ã‚¹ãƒˆã®ãƒªã‚¹ãƒˆ
        
    Returns:
        dict: {"title": str, "summary": str}
    """
    llm = Model().llm()
    
    # ã‚¹ãƒ†ãƒƒãƒ—1: ã‚¿ã‚¤ãƒˆãƒ«æŠ½å‡º
    title = extract_word_title(texts)
    
    # ã‚¹ãƒ†ãƒƒãƒ—2: ç›®æ¬¡æŠ½å‡º
    table_of_contents = extract_word_table_of_contents(texts)
    
    # ã‚¹ãƒ†ãƒƒãƒ—3: ç›®æ¬¡ã‹ã‚‰è¦ç´„ã‚’ä½œæˆ
    if table_of_contents and table_of_contents != "ç›®æ¬¡ãªã—":
        summary = create_summary_from_toc(title, table_of_contents)
    else:
        # ç›®æ¬¡ãŒãªã„å ´åˆã¯å¾“æ¥ãƒ­ã‚¸ãƒƒã‚¯ã‚’ä½¿ç”¨
        logger.info("ğŸ“„ ç›®æ¬¡ãŒè¦‹ã¤ã‹ã‚‰ãªã„ãŸã‚ã€å¾“æ¥ã®è¦ç´„æ–¹å¼ã‚’ä½¿ç”¨")
        summary = traditional_summarize(texts)
    
    return {"title": title, "summary": summary}


def extract_powerpoint_title(texts: list[str]) -> str:
    """powerpointã®ã‚¿ã‚¤ãƒˆãƒ«ã‚’æŠ½å‡ºã™ã‚‹
    
    Args:
        texts: PDFã‹ã‚‰æŠ½å‡ºã•ã‚ŒãŸãƒšãƒ¼ã‚¸åˆ¥ãƒ†ã‚­ã‚¹ãƒˆã®ãƒªã‚¹ãƒˆ
        
    Returns:
        str: æŠ½å‡ºã•ã‚ŒãŸã‚¿ã‚¤ãƒˆãƒ«
    """
    llm = Model().llm()
    
    # æœ€åˆã®3ãƒšãƒ¼ã‚¸ã‹ã‚‰ã‚¿ã‚¤ãƒˆãƒ«æŠ½å‡º
    pages_to_analyze = min(3, len(texts))
    sample_texts = texts[:pages_to_analyze]
    
    # ãƒšãƒ¼ã‚¸æ•°ã«å¿œã˜ã¦ã‚µãƒ³ãƒ—ãƒ«ãƒ†ã‚­ã‚¹ãƒˆã‚’æº–å‚™
    if pages_to_analyze == 1:
        merged_text = f"ãƒšãƒ¼ã‚¸1:\n{sample_texts[0]}"
    else:
        merged_text = "\n\n".join([f"ãƒšãƒ¼ã‚¸{i+1}:\n{text}" for i, text in enumerate(sample_texts)])
    
    title_prompt = PromptTemplate(
        input_variables=["text"],
        template="""ä»¥ä¸‹ã¯PowerPointè³‡æ–™ã®æœ€åˆã®æ•°ãƒšãƒ¼ã‚¸ã§ã™ã€‚
ã“ã®è³‡æ–™ã®é©åˆ‡ãªã‚¿ã‚¤ãƒˆãƒ«ã‚’æŠ½å‡ºã—ã¦ãã ã•ã„ã€‚

ãƒ†ã‚­ã‚¹ãƒˆ:
{text}

### æŠ½å‡ºæ–¹é‡
- æœ€åˆã®ãƒšãƒ¼ã‚¸ã®ãƒ¡ã‚¤ãƒ³ã‚¿ã‚¤ãƒˆãƒ«ã‚’å„ªå…ˆ
- å‰¯é¡ŒãŒã‚ã‚‹å ´åˆã¯å«ã‚ã‚‹
- çµ„ç¹”åã‚„æ—¥ä»˜ã¯é™¤å¤–
- ã‚¿ã‚¤ãƒˆãƒ«ã‚’å¤‰æ›´ã—ã¦ã¯ãªã‚‰ãªã„

### å‡ºåŠ›å½¢å¼
ã‚¿ã‚¤ãƒˆãƒ«ã®ã¿ã‚’å‡ºåŠ›ã—ã¦ãã ã•ã„ï¼ˆèª¬æ˜ã‚„å‰ç½®ãã¯ä¸è¦ï¼‰
        """)
    
    chain = title_prompt | llm
    result = chain.invoke({"text": merged_text})
    extracted_title = result.content.strip()
    return extracted_title


def extract_titles_and_score(texts: list[str], start_page: int, end_page: int):
    """10ãƒšãƒ¼ã‚¸ãšã¤ã‚¹ãƒ©ã‚¤ãƒ‰ã‚¿ã‚¤ãƒˆãƒ«ã‚’æŠ½å‡ºã—ã€é‡è¦åº¦ã‚’ã‚¹ã‚³ã‚¢ãƒªãƒ³ã‚°
    
    Args:
        texts: PDFã‹ã‚‰æŠ½å‡ºã•ã‚ŒãŸãƒ†ã‚­ã‚¹ãƒˆã®ãƒªã‚¹ãƒˆ
        start_page: é–‹å§‹ãƒšãƒ¼ã‚¸ï¼ˆ0ãƒ™ãƒ¼ã‚¹ï¼‰
        end_page: çµ‚äº†ãƒšãƒ¼ã‚¸ï¼ˆ0ãƒ™ãƒ¼ã‚¹ã€inclusiveï¼‰
    
    Returns:
        dict: {"slides": [{"page": int, "title": str, "score": int, "reason": str}]}
    """
    from langchain.output_parsers import PydanticOutputParser
    from pydantic import BaseModel, Field
    
    class SlideInfo(BaseModel):
        page: int = Field(description="ãƒšãƒ¼ã‚¸ç•ªå·")
        title: str = Field(description="ã‚¹ãƒ©ã‚¤ãƒ‰ã‚¿ã‚¤ãƒˆãƒ«")
        score: int = Field(description="é‡è¦åº¦ã‚¹ã‚³ã‚¢ï¼ˆ1-5ç‚¹ï¼‰", ge=1, le=5)
        reason: str = Field(description="ã‚¹ã‚³ã‚¢ã®ç†ç”±")
    
    class SlideAnalysis(BaseModel):
        slides: list[SlideInfo] = Field(description="ã‚¹ãƒ©ã‚¤ãƒ‰åˆ†æçµæœ")
    
    llm = Model().llm()
    parser = PydanticOutputParser(pydantic_object=SlideAnalysis)
    
    # æŒ‡å®šç¯„å›²ã®ãƒšãƒ¼ã‚¸ã‚’å–å¾—
    page_texts = texts[start_page:end_page+1]
    content = "\n\n".join([f"--- ãƒšãƒ¼ã‚¸ {start_page + i + 1} ---\n{text}" for i, text in enumerate(page_texts)])
    
    # ã‚¹ã‚³ã‚¢ãƒªãƒ³ã‚°åŸºæº–ï¼šæŒ¯ã‚Šè¿”ã‚Šã‚ˆã‚Šè«–ç‚¹ã‚’å„ªå…ˆ
    scoring_criteria = """
5ç‚¹: ã‚¢ã‚¸ã‚§ãƒ³ãƒ€ãƒ»ç›®æ¬¡ãƒ»æ¤œè¨äº‹é …ãƒ»ä¸»ãªè«–ç‚¹ãƒ»ã¾ã¨ã‚ãƒ»çµè«–ãƒ»éª¨å­
4ç‚¹: è¦ç‚¹ãƒ»ãƒã‚¤ãƒ³ãƒˆãƒ»ã¨ã‚Šã¾ã¨ã‚ãƒ»ææ¡ˆãƒ»(æ¡ˆ)ãƒ»å–çµ„ãƒ»é‡è¦èª²é¡Œãƒ»ä»Šå¾Œã®æ–¹é‡ãƒ»ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«
3ç‚¹: æŒ¯ã‚Šè¿”ã‚Šãƒ»èƒŒæ™¯ãƒ»èª²é¡Œãƒ»åˆ†æçµæœãƒ»æˆ¦ç•¥
2ç‚¹: èª¬æ˜ãƒ»è©³ç´°ãƒ»è£œè¶³ãƒ»å‚è€ƒè³‡æ–™ãƒ»äº‹ä¾‹ç´¹ä»‹
1ç‚¹: ãã®ä»–ãƒ»è¡¨ç´™ãƒ»äº‹å‹™é€£çµ¡
"""

    prompt = PromptTemplate(
        input_variables=["content", "format_instructions"],
        template=f"""ä»¥ä¸‹ã®PowerPointè³‡æ–™ã®å„ãƒšãƒ¼ã‚¸ã‹ã‚‰ã‚¹ãƒ©ã‚¤ãƒ‰ã‚¿ã‚¤ãƒˆãƒ«ã‚’æŠ½å‡ºã—ã€é‡è¦åº¦ã‚’5ç‚¹æº€ç‚¹ã§ã‚¹ã‚³ã‚¢ãƒªãƒ³ã‚°ã—ã¦ãã ã•ã„ã€‚

å†…å®¹:
{{content}}

### ã‚¹ã‚³ã‚¢ãƒªãƒ³ã‚°åŸºæº–ï¼ˆ1-5ç‚¹ï¼‰
{scoring_criteria}

### é‡è¦ãªåˆ¤å®šãƒã‚¤ãƒ³ãƒˆ
- ã€Œä¸»ãªè«–ç‚¹ã€ã€Œæ¤œè¨äº‹é …ã€ã€Œä»Šå›ã®è«–ç‚¹ã€ã¯5ç‚¹ï¼ˆæœ€é‡è¦ï¼‰
- ã€Œç¬¬â—‹å›ã®æŒ¯ã‚Šè¿”ã‚Šã€ã€Œå‰å›ã®æŒ¯ã‚Šè¿”ã‚Šã€ã¯3ç‚¹ï¼ˆä¸­ç¨‹åº¦ï¼‰
- ã‚¢ã‚¸ã‚§ãƒ³ãƒ€ã‚„ç›®æ¬¡ã®ä¸­ã«è¤‡æ•°é …ç›®ãŒå«ã¾ã‚Œã‚‹å ´åˆã€å®Ÿè³ªçš„ãªè«–ç‚¹éƒ¨åˆ†ã‚’é‡è¦–
- è«–ç‚¹æç¤º > æŒ¯ã‚Šè¿”ã‚Šå†…å®¹ã®å„ªå…ˆåº¦ã§åˆ¤å®šã—ã¦ãã ã•ã„

å„ãƒšãƒ¼ã‚¸ã«ã¤ã„ã¦ã€ãƒšãƒ¼ã‚¸ç•ªå·ã€ã‚¿ã‚¤ãƒˆãƒ«ã€ã‚¹ã‚³ã‚¢ã€ç†ç”±ã‚’æŠ½å‡ºã—ã¦ãã ã•ã„ã€‚

#### å‡ºåŠ›ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ
ä»¥ä¸‹ã®ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã§å‡ºåŠ›ã—ã¦ãã ã•ã„ã€‚æœ€å¾Œã®è¦ç´ ã«ã¯ã‚«ãƒ³ãƒã‚’ä»˜ã‘ãªã„ã§ãã ã•ã„ã€‚

{{format_instructions}}
        """)
    
    chain = prompt | llm | parser
    
    # ãƒªãƒˆãƒ©ã‚¤æ©Ÿèƒ½ä»˜ãã§JSONãƒ‘ãƒ¼ã‚¹ã‚’å®Ÿè¡Œ
    max_retries = 3
    for attempt in range(max_retries):
        try:
            if attempt > 0:
                logger.info(f"å†æ¤œç´¢({attempt+1}å›ç›®)")
            result = chain.invoke({
                "content": content,
                "format_instructions": parser.get_format_instructions(),
            })
            
            return result
            
        except Exception as e:
            if attempt == max_retries - 1:
                logger.error(f"âŒ é©åˆ‡ãªãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã«ã‚ˆã‚‹çµæœã‚’å¾—ã‚‰ã‚Œã¾ã›ã‚“ã§ã—ãŸ")
                return SlideAnalysis(slides=[])
            else:
                continue
    # ã“ã“ã«ã¯åˆ°é”ã—ãªã„ã¯ãšã ãŒã€å®‰å…¨ã®ãŸã‚
    return SlideAnalysis(slides=[])


def powerpoint_based_summarize(texts: list[str]) -> dict:
    """PowerPointãƒ™ãƒ¼ã‚¹æ–‡æ›¸ã®3æ®µéšè¦ç´„å‡¦ç†
    
    3æ®µéšå‡¦ç†ï¼š
    1. 10ãƒšãƒ¼ã‚¸ãšã¤ã‚¿ã‚¤ãƒˆãƒ«æŠ½å‡ºãƒ»ã‚¹ã‚³ã‚¢ãƒªãƒ³ã‚°ï¼ˆLLMï¼‰
    2. æœ€é«˜ã‚¹ã‚³ã‚¢ã‚¹ãƒ©ã‚¤ãƒ‰é¸æŠï¼ˆéLLMï¼‰
    3. è¦ç´„ä½œæˆï¼ˆLLMï¼‰
    
    Args:
        texts: PDFã‹ã‚‰æŠ½å‡ºã•ã‚ŒãŸãƒšãƒ¼ã‚¸åˆ¥ãƒ†ã‚­ã‚¹ãƒˆã®ãƒªã‚¹ãƒˆ
        
    Returns:
        dict: {"title": str, "summary": str}
    """
    llm = Model().llm()
    
    # ã‚¹ãƒ†ãƒƒãƒ—1: ã‚¿ã‚¤ãƒˆãƒ«æŠ½å‡º
    title = extract_powerpoint_title(texts)
    logger.info(f"PowerPointã®ã‚¿ã‚¤ãƒˆãƒ«ã¯ã€Œ{title.replace('\n', '\\n')}ã€ã§ã™")
    
    # ã‚¹ãƒ†ãƒƒãƒ—2: æŒ‡å®šãƒšãƒ¼ã‚¸æ•°ãšã¤ã‚¹ãƒ©ã‚¤ãƒ‰ã‚¿ã‚¤ãƒˆãƒ«æŠ½å‡ºãƒ»ã‚¹ã‚³ã‚¢ãƒªãƒ³ã‚°
    pages_per_batch = 20  # ä¸€åº¦ã«å‡¦ç†ã™ã‚‹ãƒšãƒ¼ã‚¸æ•°
    total_pages = len(texts)
    all_slides = []

    for start_page in range(0, total_pages, pages_per_batch):
        end_page = min(start_page + pages_per_batch - 1, total_pages - 1)
        try:
            logger.info(f"ã‚¹ãƒ©ã‚¤ãƒ‰ã‚¿ã‚¤ãƒˆãƒ«ã‹ã‚‰ã‚¹ãƒ©ã‚¤ãƒ‰ã®å†…å®¹ã‚’æ¨å®šã—ã¾ã™(ãƒšãƒ¼ã‚¸{start_page+1}-{end_page+1}/{total_pages})")
            slide_analysis = extract_titles_and_score(texts, start_page, end_page)
            for slide in slide_analysis.slides:
                logger.info(f"  ãƒšãƒ¼ã‚¸{slide.page}: {slide.title} â†’ ã‚¹ã‚³ã‚¢: {slide.score} - {slide.reason}")
            all_slides.extend(slide_analysis.slides)
        except Exception as e:
            logger.warning(f"âš ï¸ ã‚¹ãƒ©ã‚¤ãƒ‰åˆ†æã«å¤±æ•—ï¼ˆãƒšãƒ¼ã‚¸{start_page+1}-{end_page+1}ï¼‰: {e}")
    
    # ã‚¹ãƒ†ãƒƒãƒ—3: æœ€é«˜ã‚¹ã‚³ã‚¢ã®ã‚¹ãƒ©ã‚¤ãƒ‰ã‚’é¸æŠ
    if not all_slides:
        # ã‚¹ãƒ©ã‚¤ãƒ‰ãŒå–å¾—ã§ããªã„å ´åˆã¯å…¨æ–‡ã‚’ä½¿ç”¨
        merged_content = "\n\n".join([f"--- ãƒšãƒ¼ã‚¸ {i+1} ---\n{text}" for i, text in enumerate(texts)])
        page_info = f"å…¨{total_pages}ãƒšãƒ¼ã‚¸ï¼ˆã‚¹ãƒ©ã‚¤ãƒ‰åˆ†æå¤±æ•—ï¼‰"
        selected_slide_info = "åˆ†æå¤±æ•—ã®ãŸã‚å…¨ãƒšãƒ¼ã‚¸ä½¿ç”¨"
    else:
        # ã‚¹ã‚³ã‚¢ã§ã‚½ãƒ¼ãƒˆã—ã€æœ€é«˜ã‚¹ã‚³ã‚¢ã®ã‚¹ãƒ©ã‚¤ãƒ‰ã®ã¿ã‚’é¸æŠ
        sorted_slides = sorted(all_slides, key=lambda x: x.score, reverse=True)
        max_score = sorted_slides[0].score
        top_slides = [slide for slide in sorted_slides if slide.score == max_score]

        logger.info(f"ğŸ¯ é¸æŠã•ã‚ŒãŸã‚¹ãƒ©ã‚¤ãƒ‰: {', '.join([str(slide.page) for slide in top_slides])}")

        # æœ€é«˜ã‚¹ã‚³ã‚¢ã®ã‚¹ãƒ©ã‚¤ãƒ‰ã®ãƒ†ã‚­ã‚¹ãƒˆã‚’å–å¾—
        selected_texts = []
        for slide in top_slides:
            page_idx = slide.page - 1  # 1ãƒ™ãƒ¼ã‚¹ã‹ã‚‰0ãƒ™ãƒ¼ã‚¹ã«å¤‰æ›
            if 0 <= page_idx < len(texts):
                selected_texts.append(f"--- ãƒšãƒ¼ã‚¸ {slide.page} ({slide.title}) ---\n{texts[page_idx]}")
        
        merged_content = "\n\n".join(selected_texts)
        page_info = f"æœ€é«˜ã‚¹ã‚³ã‚¢{max_score}ç‚¹ã®ã‚¹ãƒ©ã‚¤ãƒ‰{len(top_slides)}æšï¼ˆç·{total_pages}ãƒšãƒ¼ã‚¸ä¸­ï¼‰"
        selected_slide_info = f"é¸æŠã•ã‚ŒãŸã‚¹ãƒ©ã‚¤ãƒ‰: " + ", ".join([f"ãƒšãƒ¼ã‚¸{s.page}({s.title})" for s in top_slides])
    
    # ã‚¹ãƒ†ãƒƒãƒ—4: è¦ç´„ä½œæˆ
    powerpoint_summary_prompt = PromptTemplate(
        input_variables=["title", "content", "page_info", "selected_slide_info"],
        template="""ä»¥ä¸‹ã¯PowerPointè³‡æ–™ã€Œ{title}ã€ã®é‡è¦ã‚¹ãƒ©ã‚¤ãƒ‰ã§ã™ã€‚

åˆ†æå¯¾è±¡: {page_info}
{selected_slide_info}

å†…å®¹:
{content}

### è¦ç´„ä½œæˆ
ä»¥ä¸‹ã®æ§‹æˆã§ç°¡æ½”ã«è¦ç´„ã—ã¦ãã ã•ã„ï¼š
- è³‡æ–™ã®ç›®çš„ãƒ»èƒŒæ™¯
- ä¸»è¦ãªæ¤œè¨äº‹é …ãƒ»è«–ç‚¹
- çµè«–ãƒ»ææ¡ˆãƒ»ä»Šå¾Œã®æ–¹å‘æ€§

### å‡ºåŠ›å½¢å¼
ã€Œ{title}ã€ï¼š[è¦ç´„å†…å®¹]

### åˆ¶ç´„
- ç°¡æ½”ã§åˆ†ã‹ã‚Šã‚„ã™ã
- æä¾›ã•ã‚ŒãŸã‚¹ãƒ©ã‚¤ãƒ‰ã®å†…å®¹ã®ã¿ä½¿ç”¨
- æ¨æ¸¬ã‚„è£œå®Œã¯è¡Œã‚ãªã„
        """)
    
    chain = powerpoint_summary_prompt | llm
    result = chain.invoke({
        "title": title,
        "content": merged_content,
        "page_info": page_info,
        "selected_slide_info": selected_slide_info
    })
    
    return {"title": title, "summary": result.content.strip()}


def traditional_summarize(texts: list[str]) -> str:
    """å¾“æ¥ã®å…¨æ–‡è¦ç´„å‡¦ç†"""
    llm = Model().llm()
    
    # ãƒ†ã‚­ã‚¹ãƒˆã‚’ç›´æ¥ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã«å¤‰æ›
    docs = [Document(page_content=t) for t in texts]

    # ã¾ãšè¦ç´„ã‚’ç”Ÿæˆã—ã€ãã®å¾ŒJSONã«å¤‰æ›ã™ã‚‹2æ®µéšã®ãƒ—ãƒ­ã‚»ã‚¹
    # ã‚¹ãƒ†ãƒƒãƒ—1: ãƒ†ã‚­ã‚¹ãƒˆã®è¦ç´„
    map_prompt = PromptTemplate(
        input_variables=["text"],
        template="""ä»¥ä¸‹ã®æ–‡ç« ã‚’åˆ†æã—ã€æ®µéšçš„ã«è¦ç´„ã‚’ä½œæˆã—ã¦ãã ã•ã„ã€‚

## ã‚¹ãƒ†ãƒƒãƒ—1: æ–‡æ›¸ç¨®é¡ã®åˆ¤å®š
ã¾ãšã€ã“ã®æ–‡ç« ãŒã©ã®ã‚ˆã†ãªç¨®é¡ã®æ–‡æ›¸ã‹ã‚’åˆ¤å®šã—ã¦ãã ã•ã„ï¼š

**åˆ¤å®šåŸºæº–ï¼š**
- ã€Œè¡¨ç´™ãƒ»ã‚¿ã‚¤ãƒˆãƒ«ãƒšãƒ¼ã‚¸ã€: ã‚¿ã‚¤ãƒˆãƒ«ã€çµ„ç¹”åã€æ—¥ä»˜ã®ã¿ã§å®Ÿè³ªçš„ãªå†…å®¹ãŒå°‘ãªã„
- ã€Œç›®æ¬¡ãƒ»æ¦‚è¦ã€: ç« ç«‹ã¦ã‚„æ¦‚è¦ã®ã¿ã§è©³ç´°ãªèª¬æ˜ãŒãªã„  
- ã€Œæœ¬æ–‡ãƒ»è©³ç´°è³‡æ–™ã€: å…·ä½“çš„ãªå†…å®¹ã€èª¬æ˜ã€ãƒ‡ãƒ¼ã‚¿ã€è­°è«–ç­‰ãŒå«ã¾ã‚Œã¦ã„ã‚‹

**åˆ¤å®šçµæœ**: [è¡¨ç´™ãƒ»ã‚¿ã‚¤ãƒˆãƒ«ãƒšãƒ¼ã‚¸/ç›®æ¬¡ãƒ»æ¦‚è¦/æœ¬æ–‡ãƒ»è©³ç´°è³‡æ–™]
**åˆ¤å®šç†ç”±**: [å…·ä½“çš„ãªæ ¹æ‹ ã‚’è¨˜è¿°]

## ã‚¹ãƒ†ãƒƒãƒ—2: è¦ç´„æ–¹é‡ã®æ±ºå®š
ã‚¹ãƒ†ãƒƒãƒ—1ã®åˆ¤å®šçµæœã«åŸºã¥ã„ã¦è¦ç´„æ–¹é‡ã‚’æ±ºå®šã—ã¦ãã ã•ã„ï¼š

- ã€Œè¡¨ç´™ãƒ»ã‚¿ã‚¤ãƒˆãƒ«ãƒšãƒ¼ã‚¸ã€â†’ ã‚¿ã‚¤ãƒˆãƒ«ã€çµ„ç¹”åã€åŸºæœ¬æƒ…å ±ã®ã¿ã‚’ç°¡æ½”ã«è¨˜è¿°
- ã€Œç›®æ¬¡ãƒ»æ¦‚è¦ã€â†’ æ§‹æˆã‚„æ¦‚è¦ã®è¦ç‚¹ã‚’æ•´ç†
- ã€Œæœ¬æ–‡ãƒ»è©³ç´°è³‡æ–™ã€â†’ é‡è¦ãªå†…å®¹ã‚’è«–ç†çš„ã«è¦ç´„

**æ¡ç”¨ã™ã‚‹æ–¹é‡**: [é¸æŠã—ãŸæ–¹é‡ã‚’è¨˜è¿°]

## ã‚¹ãƒ†ãƒƒãƒ—3: è¦ç´„ã®ä½œæˆ
ã‚¹ãƒ†ãƒƒãƒ—2ã§æ±ºå®šã—ãŸæ–¹é‡ã«å¾“ã£ã¦è¦ç´„ã‚’ä½œæˆã—ã¦ãã ã•ã„ã€‚

**é‡è¦ãªåˆ¶ç´„ï¼š**
- æ–‡ç« ã«å®Ÿéš›ã«æ›¸ã‹ã‚Œã¦ã„ã‚‹å†…å®¹ã®ã¿ã‚’ä½¿ç”¨ã—ã¦ãã ã•ã„
- æ¨æ¸¬ã‚„è£œå®Œã€å‰µä½œã¯ä¸€åˆ‡è¡Œã‚ãªã„ã§ãã ã•ã„
- è¡¨ç´™ãƒ»ã‚¿ã‚¤ãƒˆãƒ«ãƒšãƒ¼ã‚¸ã®å ´åˆã¯è©³ç´°ãªèª¬æ˜ã‚’å‰µä½œã—ãªã„ã§ãã ã•ã„
- **æŠ½å‡ºã•ã‚ŒãŸãƒ†ã‚­ã‚¹ãƒˆãŒä¸ååˆ†ã¾ãŸã¯æ„å‘³ã®ã‚ã‚‹å†…å®¹ãŒãªã„å ´åˆã¯ã€è¦ç´„ã‚’ç©ºæ–‡å­—åˆ—ã§è¿”ã—ã¦ãã ã•ã„**
- **OCRã‚¨ãƒ©ãƒ¼ã‚„æ–‡å­—åŒ–ã‘ãªã©ã€åˆ¤èª­ä¸èƒ½ãªãƒ†ã‚­ã‚¹ãƒˆã—ã‹ãªã„å ´åˆã¯ç©ºæ–‡å­—åˆ—ã‚’è¿”ã—ã¦ãã ã•ã„**

æ–‡ç« ï¼š
{text}

## å‡ºåŠ›å½¢å¼
**æ–‡æ›¸ç¨®é¡**: [åˆ¤å®šçµæœ]
**è¦ç´„**: [ä½œæˆã—ãŸè¦ç´„]
        """)

    combine_prompt = PromptTemplate(
        input_variables=["text"],
        template="""ä»¥ä¸‹ã®è¦ç´„ã‚’1ã¤ã®æ–‡ç« ã«ã¾ã¨ã‚ã¦ãã ã•ã„ã€‚

**äº‹å‰ãƒã‚§ãƒƒã‚¯ï¼ˆé‡è¦ï¼‰ï¼š**
ã¾ãšã€å…¥åŠ›ã•ã‚ŒãŸè¦ç´„ã‚’åˆ†æã—ã¦ãã ã•ã„ï¼š
- ã™ã¹ã¦ã®ãƒšãƒ¼ã‚¸ã®è¦ç´„ãŒç©ºæ–‡å­—åˆ—ã¾ãŸã¯ã€Œè¦ç´„:ã€ã ã‘ã®å ´åˆã¯ç©ºæ–‡å­—åˆ—ã‚’è¿”ã™
- ç®‡æ¡æ›¸ãè¨˜å·ï¼ˆâš«ã€â—ã€â€¢ã€-ç­‰ï¼‰ã®ã¿ã§æ§‹æˆã•ã‚Œã¦ã„ã‚‹å ´åˆã¯ç©ºæ–‡å­—åˆ—ã‚’è¿”ã™
- OCRã‚¨ãƒ©ãƒ¼ã‚„æ–‡å­—åŒ–ã‘ã—ãŸã‚¿ã‚¤ãƒˆãƒ«ï¼ˆä¾‹ï¼šã€Œpan Lç§Ÿt'aLonã€ï¼‰ã®ã¿ã®å ´åˆã¯ç©ºæ–‡å­—åˆ—ã‚’è¿”ã™
- æ„å‘³ã®ã‚ã‚‹ãƒ†ã‚­ã‚¹ãƒˆå†…å®¹ãŒä¸€åˆ‡å«ã¾ã‚Œã¦ã„ãªã„å ´åˆã¯ç©ºæ–‡å­—åˆ—ã‚’è¿”ã™

**ç‰¹åˆ¥å‡¦ç†ï¼ˆè¡¨ç´™æƒ…å ±ãŒã‚ã‚‹å ´åˆï¼‰ï¼š**
è¡¨ç´™ãƒ»ã‚¿ã‚¤ãƒˆãƒ«ãƒšãƒ¼ã‚¸ã‹ã‚‰é©åˆ‡ãªã‚¿ã‚¤ãƒˆãƒ«æƒ…å ±ãŒæŠ½å‡ºã§ãã‚‹ãŒã€æœ¬æ–‡ãƒ»è©³ç´°è³‡æ–™ã‹ã‚‰ã®è¦ç´„ãŒç©ºã¾ãŸã¯ç„¡æ„å‘³ãªå ´åˆï¼š
- ã€Œ[é©åˆ‡ãªã‚¿ã‚¤ãƒˆãƒ«]ï¼šã€ã®å½¢å¼ã§å‡ºåŠ›ã™ã‚‹
- ã‚¿ã‚¤ãƒˆãƒ«å¾Œã®ã‚³ãƒ­ãƒ³ã®å¾Œã¯ä½•ã‚‚è¿½åŠ ã›ãšã€ãã®ã¾ã¾çµ‚äº†ã™ã‚‹
- æ–‡æ›¸ã®èª¬æ˜ã‚„æ§‹æˆã®èª¬æ˜ã¯ä¸€åˆ‡è¿½åŠ ã—ãªã„

**çµ±åˆå‡¦ç†ï¼ˆå®Ÿè³ªçš„å†…å®¹ãŒã‚ã‚‹å ´åˆï¼‰ï¼š**
å®Ÿè³ªçš„ãªè­°è«–å†…å®¹ã€æ¤œè¨äº‹é …ã€çµè«–ã€ãƒ‡ãƒ¼ã‚¿ãªã©ãŒå«ã¾ã‚Œã¦ã„ã‚‹å ´åˆã®ã¿ï¼š
- è¡¨ç´™ãƒ»ã‚¿ã‚¤ãƒˆãƒ«ãƒšãƒ¼ã‚¸ã‹ã‚‰ã¯åŸºæœ¬æƒ…å ±ï¼ˆè³‡æ–™åã€çµ„ç¹”åç­‰ï¼‰ã‚’æŠ½å‡º
- ç›®æ¬¡ãƒ»æ¦‚è¦ã‹ã‚‰ã¯å…¨ä½“æ§‹æˆã‚’æŠŠæ¡
- æœ¬æ–‡ãƒ»è©³ç´°è³‡æ–™ã‹ã‚‰ã¯å…·ä½“çš„ãªå†…å®¹ã‚’è¦ç´„
- å„ãƒšãƒ¼ã‚¸ã®åˆ¤å®šçµæœã«åŸºã¥ã„ã¦é©åˆ‡ãªé‡ã¿ä»˜ã‘ã‚’è¡Œã†

**å‡ºåŠ›å½¢å¼ï¼š**
1. å®Œå…¨ã«ç„¡åŠ¹ãªå ´åˆï¼šç©ºæ–‡å­—åˆ—
2. è¡¨ç´™æƒ…å ±ã®ã¿ã®å ´åˆï¼šã€Œ[ã‚¿ã‚¤ãƒˆãƒ«]ï¼šã€
3. å®Ÿè³ªçš„å†…å®¹ãŒã‚ã‚‹å ´åˆï¼šã€Œ[ã‚¿ã‚¤ãƒˆãƒ«]ï¼š[è¦ç´„å†…å®¹]ã€

**æ‰‹é †ï¼š**
1. ã¾ãšäº‹å‰ãƒã‚§ãƒƒã‚¯ã‚’å®Ÿè¡Œã—ã€å®Œå…¨ã«ç„¡åŠ¹ã‹ã‚’åˆ¤å®š
2. å®Œå…¨ã«ç„¡åŠ¹ãªå ´åˆã¯ç©ºæ–‡å­—åˆ—ã‚’è¿”ã™
3. è¡¨ç´™æƒ…å ±ã‹ã‚‰é©åˆ‡ãªã‚¿ã‚¤ãƒˆãƒ«ã‚’ç‰¹å®š
4. æœ¬æ–‡ãƒ»è©³ç´°è³‡æ–™ã«å®Ÿè³ªçš„ãªå†…å®¹ãŒã‚ã‚‹ã‹ã‚’ç¢ºèª
5. å®Ÿè³ªçš„ãªå†…å®¹ãŒãªã„å ´åˆã¯ã€Œ[ã‚¿ã‚¤ãƒˆãƒ«]ï¼šã€ã§çµ‚äº†
6. å®Ÿè³ªçš„ãªå†…å®¹ãŒã‚ã‚‹å ´åˆã¯ã€Œ[ã‚¿ã‚¤ãƒˆãƒ«]ï¼š[å†…å®¹]ã€ã‚’ä½œæˆ

**ä¾‹ï¼š**
- å®Ÿè³ªçš„å†…å®¹ãŒã‚ã‚‹å ´åˆï¼šã€Œãƒ‡ã‚¸ã‚¿ãƒ«åºå€‹äººæƒ…å ±ä¿è­·ã‚¬ã‚¤ãƒ‰ãƒ©ã‚¤ãƒ³ã€ï¼šã“ã®ã‚¬ã‚¤ãƒ‰ãƒ©ã‚¤ãƒ³ã§ã¯å€‹äººæƒ…å ±ã®é©åˆ‡ãªå–ã‚Šæ‰±ã„ã«ã¤ã„ã¦...
- è¡¨ç´™æƒ…å ±ã®ã¿ã®å ´åˆï¼šã€Œç¬¬3å›æ¤œè¨ä¼šè³‡æ–™1-2ã€ï¼š
- å®Œå…¨ã«ç„¡åŠ¹ãªå ´åˆï¼šï¼ˆç©ºæ–‡å­—åˆ—ï¼‰

**æ³¨æ„ï¼š**
- æ¨æ¸¬ã‚„å‰µä½œã¯ä¸€åˆ‡è¡Œã‚ãšã€å®Ÿéš›ã«æ›¸ã‹ã‚Œã¦ã„ã‚‹å†…å®¹ã®ã¿ã‚’ä½¿ç”¨ã™ã‚‹
- è¡¨ç´™æƒ…å ±ã®ã¿ã®å ´åˆã¯æ–‡æ›¸ã®èª¬æ˜ã‚„æ§‹æˆã®èª¬æ˜ã¯è¿½åŠ ã—ãªã„
- ã‚¿ã‚¤ãƒˆãƒ«å¾Œã®ã‚³ãƒ­ãƒ³ã®å¾Œã«ç„¡æ„å‘³ãªèª¬æ˜ã‚’è¿½åŠ ã—ãªã„

è¦ç´„ï¼š
{text}]
    """)

    # è¦ç´„ã‚’ç”Ÿæˆ
    chain = load_summarize_chain(
        llm,
        chain_type="map_reduce",
        map_prompt=map_prompt,
        combine_prompt=combine_prompt,
        verbose=False,
    )

    summary_result = chain.invoke(docs)
    return summary_result["output_text"]


def document_summarizer(state: State) -> State:
    """PDFæ–‡æ›¸ã‚’è¦ç´„ã™ã‚‹ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ"""

    logger.info("ğŸ“ æ–‡æ›¸ã‚’è¦ç´„...")

    llm = Model().llm()
    parser = JsonOutputParser(pydantic_object=Summary)

    # ç¾åœ¨ã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’å–å¾—
    current_index = state.get("target_report_index", 0)
    # state ã« target_reports ãŒå­˜åœ¨ã—ãªã„ã‹ None ã®å ´åˆã«å‚™ãˆã¦æ­£è¦åŒ–
    target_reports = state.get("target_reports")
    if not target_reports or (hasattr(target_reports, '__len__') and len(target_reports) == 0):
        logger.info("é–¢é€£æ–‡æ›¸ãŒãªã„ãŸã‚æ–‡æ›¸è¦ç´„ã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™")
        return {
            **state,
            "messages": state.get("messages", []),
            "target_report_summaries": state.get("target_report_summaries", []),
            "target_report_index": current_index,
        }

    # åˆæœŸå€¤ã‚’è¨­å®š
    summary_obj = None
    message = None
    target_report_index = current_index + 1

    try:
        if current_index >= len(target_reports):
            return state

        # ç¾åœ¨ã®æ–‡æ›¸ã®URLã‚’å–å¾—
        current_report = target_reports[current_index]
        url = current_report.url
        name = current_report.name

        logger.info(f"{name}ã‚’è¦ç´„ã—ã¾ã™")

        # PDFã‚’èª­ã¿è¾¼ã‚“ã§ãƒ†ã‚­ã‚¹ãƒˆã‚’æŠ½å‡º
        texts = load_pdf_as_text(url)
        if not texts:
            logger.warning(f"âš ï¸ PDFã®èª­ã¿è¾¼ã¿ã«å¤±æ•—ã—ã¾ã—ãŸ: {url}")
            summary_obj = Summary(url=url, name=name, content="")
            
            message = AIMessage(content=f"""
## å€‹åˆ¥æ–‡æ›¸è¦ç´„çµæœï¼ˆèª­ã¿è¾¼ã¿å¤±æ•—ï¼‰

**å‡¦ç†å†…å®¹**: PDFæ–‡æ›¸ã®å€‹åˆ¥è¦ç´„ã‚’ç”Ÿæˆ
**è¦ç´„ã‚¿ã‚¤ãƒ—**: individual_documentï¼ˆèª­ã¿è¾¼ã¿å¤±æ•—ï¼‰
**æ–‡æ›¸å**: {name}
**æ–‡æ›¸URL**: {url}
**ã‚¨ãƒ©ãƒ¼ç†ç”±**: PDFãƒ•ã‚¡ã‚¤ãƒ«ã®èª­ã¿è¾¼ã¿ã«å¤±æ•—
**å½±éŸ¿**: è©²å½“æ–‡æ›¸ã®å†…å®¹ãŒæœ€çµ‚è¦ç´„ã«å«ã¾ã‚Œãªã„å¯èƒ½æ€§

**ç”Ÿæˆã•ã‚ŒãŸè¦ç´„**:
(PDFã‚’èª­ã¿è¾¼ã‚ã¾ã›ã‚“ã§ã—ãŸ)
""")
        else:
            logger.info(f"{name}ã‚’ãƒ†ã‚­ã‚¹ãƒˆåŒ–ã—ã¾ã—ãŸ({len(texts)}ãƒšãƒ¼ã‚¸)")
            # æ–‡æ›¸ã‚¿ã‚¤ãƒ—ã‚’åˆ¤å®š
            doc_type, doc_reason, evidence_text, detection_detail = detect_document_type(texts)
            
            # ã‚¿ã‚¤ãƒ—åˆ¥è¦ç´„å‡¦ç†
            result: dict | None = None
            if doc_type == "word":
                result = word_based_summarize(texts)
            elif doc_type == "powerpoint":
                result = powerpoint_based_summarize(texts)
            elif doc_type == "agenda":
                result = agenda_summarize(texts)
            elif doc_type == "participants":
                result = participants_summarize(texts)
            elif doc_type == "news":
                result = news_based_summarize(texts)
            else:
                # Surveyã¨Otherã¯ã‚¹ã‚­ãƒƒãƒ—
                logger.info(f"æ–‡æ›¸ã‚’ã‚¹ã‚­ãƒƒãƒ—: {name}ï¼ˆã‚¿ã‚¤ãƒ—: {doc_type}ï¼‰")
                message = HumanMessage(
                    content=f"æ–‡æ›¸: {name}\nURL: {url}\n\nè¦ç´„: (å‡¦ç†å¯¾è±¡å¤–ã®ãŸã‚ã‚¹ã‚­ãƒƒãƒ—)"
                )
                return {
                    **state,
                    "messages": [message],
                    "target_report_summaries": state.get("target_report_summaries", []),
                    "target_report_index": target_report_index,
                }

            title = result.get('title', name)
            summary = result.get('summary', '')
            # è¦ç´„å†…å®¹ã‚’ãƒ­ã‚°ã«å‡ºåŠ›
            logger.info(f"ã“ã®è³‡æ–™ã®è¦ç´„: {summary.replace('\n', '\\n').strip()}")

            # æœ€åˆã®æ–‡æ›¸ã§ã‚¿ã‚¤ãƒˆãƒ«ãŒæŠ½å‡ºã§ããŸå ´åˆã€reportã®nameã‚’æ›´æ–°
            if current_index == 0 and not name:
                if title and len(title) > 3:
                    current_report.name = title.replace('\n', ' ').strip()
                    logger.info(f"ã“ã®è³‡æ–™ã®æ­£å¼ãªã‚¿ã‚¤ãƒˆãƒ«ã¯ã€Œ{current_report.name}ã€ã§ã™")

            # ç›´æ¥Summaryã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚’ä½œæˆ
            summary_obj = Summary(
                content=summary,
                url=url,
                name=title if title else name,
                document_type=doc_type,
                detection_detail=detection_detail
            )

            # è©³ç´°èª¬æ˜ä»˜ããƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’ä½œæˆ
            message = AIMessage(content=f"""
## å€‹åˆ¥æ–‡æ›¸è¦ç´„çµæœ

**å‡¦ç†å†…å®¹**: PDFæ–‡æ›¸ã®å€‹åˆ¥è¦ç´„ã‚’ç”Ÿæˆ
**è¦ç´„ã‚¿ã‚¤ãƒ—**: individual_documentï¼ˆå€‹åˆ¥æ–‡æ›¸è¦ç´„ï¼‰
**æ–‡æ›¸å**: {name}
**æ–‡æ›¸ã‚¿ã‚¤ãƒ—**: {doc_type}
**æ–‡æ›¸URL**: {url}
**ãƒšãƒ¼ã‚¸æ•°**: {len(texts)}ãƒšãƒ¼ã‚¸
**é¸æŠç†ç”±**: ä¼šè­°ã§é…å¸ƒã•ã‚ŒãŸä¸€æ¬¡è³‡æ–™ã§ã‚ã‚Šã€è¦ç´„ä½œæˆã«ã¨ã£ã¦é‡è¦ãªå‚ç…§è³‡æ–™

**ç”Ÿæˆã•ã‚ŒãŸè¦ç´„**:
{summary_obj.content}
""")

    except Exception as e:
        logger.error(f"âŒ æ–‡æ›¸è¦ç´„ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿ: {str(e)}")
        if current_index < len(target_reports):
            current_report = target_reports[current_index]
            summary_obj = Summary(url=current_report.url, name=current_report.name, content="")
        else:
            summary_obj = Summary(url="", name="", content="")

        message = AIMessage(content=f"""
## å€‹åˆ¥æ–‡æ›¸è¦ç´„çµæœï¼ˆã‚¨ãƒ©ãƒ¼ï¼‰

**å‡¦ç†å†…å®¹**: PDFæ–‡æ›¸ã®å€‹åˆ¥è¦ç´„ã‚’ç”Ÿæˆ
**è¦ç´„ã‚¿ã‚¤ãƒ—**: individual_documentï¼ˆå‡¦ç†å¤±æ•—ï¼‰
**æ–‡æ›¸å**: {summary_obj.name}
**æ–‡æ›¸URL**: {summary_obj.url}
**ã‚¨ãƒ©ãƒ¼ç†ç”±**: æ–‡æ›¸å‡¦ç†ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿ
**å½±éŸ¿**: è©²å½“æ–‡æ›¸ã®è©³ç´°ãŒæœ€çµ‚è¦ç´„ã«å«ã¾ã‚Œãªã„å¯èƒ½æ€§

**ç”Ÿæˆã•ã‚ŒãŸè¦ç´„**:
(ã‚¨ãƒ©ãƒ¼ã®ãŸã‚è¦ç´„ã§ãã¾ã›ã‚“ã§ã—ãŸ)
""")

    # æ—¢å­˜ã®summariesã‚’å–å¾—ã—ã€æ–°ã—ã„è¦ç´„ã‚’è¿½åŠ 
    current_summaries = state.get("target_report_summaries", [])
    new_summaries = current_summaries + ([summary_obj] if summary_obj else [])

    # æ–°ã—ã„çŠ¶æ…‹ã‚’è¿”ã™
    system_message = HumanMessage(content="PDFæ–‡æ›¸ã®å†…å®¹ã‚’èª­ã¿å–ã‚Šã€è¦ç´„ã‚’ä½œæˆã—ã¦ãã ã•ã„ã€‚")

    logger.info(f"âœ… {summary_obj.name}ã®è¦ç´„ã‚’ä½œæˆã—ã¾ã—ãŸ")
    logger.info("")

    return {
        **state,
        "messages": [system_message, message] if message else [system_message],
        "target_report_summaries": new_summaries,
        "target_report_index": target_report_index,
    }

"""
HTMLProcessor sub-agent for Plan-Action architecture.

This sub-agent handles HTML loading, markdown conversion, main content extraction,
and related document discovery with isolated context.
"""

import urllib.parse

from langchain_core.messages import HumanMessage
from langchain_core.output_parsers import JsonOutputParser
from langchain_core.prompts import (
    AIMessagePromptTemplate,
    ChatPromptTemplate,
    MessagesPlaceholder,
    SystemMessagePromptTemplate,
)
from langgraph.graph import END, StateGraph

from .. import CandidateReportList, Model, logger
from ..state_v2 import HTMLProcessorState
from ..tools import load_html_as_markdown


class HTMLProcessor:
    """
    HTML processing sub-agent.

    Loads HTML pages, converts to markdown, extracts main content
    (removing headers/footers/navigation), and discovers related documents.
    """

    def __init__(self, model: Model | None = None):
        """
        Initialize HTMLProcessor sub-agent.

        Args:
            model: Model instance for LLM access. If None, uses default Model().
        """
        self.model = model if model is not None else Model()
        self.graph = self._build_graph()

    def _build_graph(self) -> StateGraph:
        """
        Build the StateGraph for HTML processing.

        Returns:
            Compiled StateGraph for HTML workflow
        """
        graph = StateGraph(HTMLProcessorState)

        # Three-stage pipeline
        graph.add_node("load_html", self._load_html)
        graph.add_node("extract_main_content", self._extract_main_content)
        graph.add_node("discover_documents", self._discover_documents)

        # Linear flow
        graph.set_entry_point("load_html")
        graph.add_edge("load_html", "extract_main_content")
        graph.add_edge("extract_main_content", "discover_documents")
        graph.add_edge("discover_documents", END)

        return graph

    def _load_html(self, state: HTMLProcessorState) -> HTMLProcessorState:
        """
        Load HTML page and convert to markdown.

        Args:
            state: Current state with url

        Returns:
            Updated state with markdown field
        """
        url = state["url"]

        logger.info(f"HTMLã‚’ãƒ­ãƒ¼ãƒ‰ä¸­: {url}")

        try:
            markdown_content = load_html_as_markdown(url)
            logger.info(f"HTMLâ†’Markdownå¤‰æ›å®Œäº† ({len(markdown_content)}æ–‡å­—)")

            return {"markdown": markdown_content}

        except Exception as e:
            logger.error(f"HTMLèª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
            return {"markdown": None}

    def _extract_main_content(self, state: HTMLProcessorState) -> HTMLProcessorState:
        """
        Extract main content from markdown (remove headers, footers, navigation).

        Args:
            state: Current state with markdown

        Returns:
            Updated state with main_content field
        """
        llm = self.model.llm()
        markdown = state.get("markdown")
        url = state["url"]

        if not markdown:
            logger.error("MarkdownãŒç©ºã®ãŸã‚ã€ãƒ¡ã‚¤ãƒ³ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’æŠ½å‡ºã§ãã¾ã›ã‚“")
            return {"main_content": None}

        logger.info("ãƒ¡ã‚¤ãƒ³ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’æŠ½å‡ºä¸­...")

        system_prompt = SystemMessagePromptTemplate.from_template(
            """ã‚ãªãŸã¯ãƒžãƒ¼ã‚¯ãƒ€ã‚¦ãƒ³ã‹ã‚‰ãƒ¡ã‚¤ãƒ³ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’æŠ½å‡ºã™ã‚‹å°‚é–€å®¶ã§ã™ã€‚

# å½¹å‰²
Webãƒšãƒ¼ã‚¸ã®ãƒžãƒ¼ã‚¯ãƒ€ã‚¦ãƒ³ã‚’åˆ†æžã—ã€ãƒ˜ãƒƒãƒ€ãƒ¼ãƒ»ãƒ•ãƒƒã‚¿ãƒ¼ãƒ»ãƒŠãƒ“ã‚²ãƒ¼ã‚·ãƒ§ãƒ³ã‚’é™¤åŽ»ã—ã¦ã€
æœ¬è³ªçš„ãªå†…å®¹ï¼ˆä¼šè­°æƒ…å ±ã€å ±å‘Šæ›¸ã€ãŠçŸ¥ã‚‰ã›ãªã©ï¼‰ã®ã¿ã‚’æŠ½å‡ºã—ã¦ãã ã•ã„ã€‚

# æŠ½å‡ºæ‰‹é †

ã‚¹ãƒ†ãƒƒãƒ—1: ãƒšãƒ¼ã‚¸æ§‹é€ ã‚’åˆ†æžã™ã‚‹
- ãƒ˜ãƒƒãƒ€ãƒ¼ï¼ˆä¸Šéƒ¨ãƒŠãƒ“ã‚²ãƒ¼ã‚·ãƒ§ãƒ³ã€ãƒ­ã‚´ã€æ¤œç´¢ãƒœãƒƒã‚¯ã‚¹ï¼‰ã‚’ç‰¹å®š
- ãƒ•ãƒƒã‚¿ãƒ¼ï¼ˆè‘—ä½œæ¨©ã€ãƒ—ãƒ©ã‚¤ãƒã‚·ãƒ¼ãƒãƒªã‚·ãƒ¼ã€ãŠå•ã„åˆã‚ã›ï¼‰ã‚’ç‰¹å®š
- ã‚µã‚¤ãƒ‰ãƒãƒ¼ï¼ˆé–¢é€£ãƒªãƒ³ã‚¯ã€ãƒ¡ãƒ‹ãƒ¥ãƒ¼ï¼‰ã‚’ç‰¹å®š
- ãƒ¡ã‚¤ãƒ³ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ï¼ˆä¼šè­°æƒ…å ±ã€å ±å‘Šæ›¸ã€è³‡æ–™ãƒªã‚¹ãƒˆï¼‰ã‚’ç‰¹å®š

ã‚¹ãƒ†ãƒƒãƒ—2: ä¸è¦ãªéƒ¨åˆ†ã‚’é™¤åŽ»ã™ã‚‹
ä»¥ä¸‹ã‚’å‰Šé™¤ï¼š
- ãƒ˜ãƒƒãƒ€ãƒ¼ã€ãƒ•ãƒƒã‚¿ãƒ¼ã€ãƒŠãƒ“ã‚²ãƒ¼ã‚·ãƒ§ãƒ³
- ãƒ‘ãƒ³ããšãƒªã‚¹ãƒˆ
- åºƒå‘Šã€ãƒãƒŠãƒ¼ã€é€šçŸ¥
- ã‚µã‚¤ãƒˆãƒžãƒƒãƒ—ã€ãƒ—ãƒ©ã‚¤ãƒã‚·ãƒ¼ãƒãƒªã‚·ãƒ¼ã¸ã®ãƒªãƒ³ã‚¯
- ã€Œãƒšãƒ¼ã‚¸ã®å…ˆé ­ã¸ã€ãªã©ã®ãƒŠãƒ“ã‚²ãƒ¼ã‚·ãƒ§ãƒ³è¦ç´ 

ã‚¹ãƒ†ãƒƒãƒ—3: é‡è¦ãªéƒ¨åˆ†ã‚’ä¿æŒã™ã‚‹
ä»¥ä¸‹ã‚’ä¿æŒï¼š
- ä¼šè­°ãƒ»å ±å‘Šæ›¸ãƒ»ã¨ã‚Šã¾ã¨ã‚ã®æ¦‚è¦
- è­°é¡Œãƒ»è­°äº‹éŒ²ãƒ»æ±ºå®šäº‹é …
- è³‡æ–™ãƒªã‚¹ãƒˆï¼ˆé…ä»˜è³‡æ–™ã€å‚è€ƒè³‡æ–™ï¼‰
- æ—¥æ™‚ãƒ»å ´æ‰€ãƒ»å‡ºå¸­è€…ãªã©ã®ä¼šè­°æƒ…å ±
- ãŠçŸ¥ã‚‰ã›ãƒ»å‹Ÿé›†ã®æœ¬æ–‡

# å‡ºåŠ›å½¢å¼
æŠ½å‡ºã—ãŸãƒ¡ã‚¤ãƒ³ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’ãƒžãƒ¼ã‚¯ãƒ€ã‚¦ãƒ³å½¢å¼ã§å‡ºåŠ›ã—ã¦ãã ã•ã„ã€‚
æ§‹é€ ï¼ˆè¦‹å‡ºã—ã€ãƒªã‚¹ãƒˆã€è¡¨ã€ãƒªãƒ³ã‚¯ï¼‰ã¯ä¿æŒã—ã¦ãã ã•ã„ã€‚

# ã‚¨ãƒ©ãƒ¼å‡¦ç†
ãƒ¡ã‚¤ãƒ³ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ãŒæŠ½å‡ºã§ããªã„å ´åˆã¯ã€å¿…ãšã€Œ[HTML_PARSING_ERROR]ã€ã¨å‡ºåŠ›ã—ã¦ãã ã•ã„ã€‚
            """
        )

        assistant_prompt = AIMessagePromptTemplate.from_template(
            """ãƒžãƒ¼ã‚¯ãƒ€ã‚¦ãƒ³ã‹ã‚‰ãƒ¡ã‚¤ãƒ³ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’æŠ½å‡ºã—ã¦ãã ã•ã„ã€‚

# åˆ¶ç´„äº‹é …
- ãƒ˜ãƒƒãƒ€ãƒ¼ã€ãƒ•ãƒƒã‚¿ãƒ¼ã€ãƒŠãƒ“ã‚²ãƒ¼ã‚·ãƒ§ãƒ³ã¯é™¤åŽ»
- ãƒ¡ã‚¤ãƒ³ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®æ§‹é€ ã¯ä¿æŒ
- ãƒªãƒ³ã‚¯ï¼ˆãƒ¡ã‚¤ãƒ³ã‚³ãƒ³ãƒ†ãƒ³ãƒ„å†…ï¼‰ã¯ä¿æŒ
- æŠ½å‡ºã§ããªã„å ´åˆã¯ã€Œ[HTML_PARSING_ERROR]ã€
            """
        )

        prompt = ChatPromptTemplate.from_messages(
            [system_prompt, assistant_prompt, MessagesPlaceholder(variable_name="messages")]
        )

        chain = prompt | llm

        # Create messages
        messages = [
            HumanMessage(content=f'ä¼šè­°ã®URLã¯"{url}"ã§ã™ã€‚'),
            HumanMessage(content=f"ãƒžãƒ¼ã‚¯ãƒ€ã‚¦ãƒ³ã¯ä»¥ä¸‹ã®é€šã‚Šã§ã™ï¼š\n\n{markdown}"),
        ]

        result = chain.invoke({"messages": messages})

        # Check for HTML parsing error
        if "[HTML_PARSING_ERROR]" in result.content:
            logger.warning("âš ï¸ HTMLãƒ‘ãƒ¼ã‚¹ã‚¨ãƒ©ãƒ¼ãŒæ¤œå‡ºã•ã‚Œã¾ã—ãŸã€‚lxmlã§è‡ªå‹•ä¿®æ­£ã‚’è©¦ã¿ã¾ã™...")

            try:
                # Retry with lxml normalization
                normalized_markdown = load_html_as_markdown(url)
                logger.info("ðŸ”§ HTMLã‚’æ­£è¦åŒ–ã—ã¦å†å¤‰æ›ã—ã¾ã—ãŸ")

                # Retry extraction
                retry_messages = [
                    HumanMessage(content=f'ä¼šè­°ã®URLã¯"{url}"ã§ã™ã€‚'),
                    HumanMessage(content=f"ãƒžãƒ¼ã‚¯ãƒ€ã‚¦ãƒ³ã¯ä»¥ä¸‹ã®é€šã‚Šã§ã™ï¼š\n\n{normalized_markdown}"),
                ]

                retry_result = chain.invoke({"messages": retry_messages})

                if "[HTML_PARSING_ERROR]" not in retry_result.content:
                    logger.info("âœ… HTMLæ­£è¦åŒ–å¾Œã«ãƒ¡ã‚¤ãƒ³ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®æŠ½å‡ºã«æˆåŠŸã—ã¾ã—ãŸ")
                    result = retry_result
                else:
                    logger.error("âŒ HTMLæ­£è¦åŒ–å¾Œã‚‚ãƒ¡ã‚¤ãƒ³ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®æŠ½å‡ºã«å¤±æ•—ã—ã¾ã—ãŸ")
                    return {"main_content": None}

            except Exception as e:
                logger.error(f"âŒ HTMLè‡ªå‹•ä¿®æ­£ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
                return {"main_content": None}

        main_content = result.content.strip()
        logger.info(f"ãƒ¡ã‚¤ãƒ³ã‚³ãƒ³ãƒ†ãƒ³ãƒ„æŠ½å‡ºå®Œäº† ({len(main_content)}æ–‡å­—)")

        return {"main_content": main_content}

    def _discover_documents(self, state: HTMLProcessorState) -> HTMLProcessorState:
        """
        Discover related document URLs from main content.

        Args:
            state: Current state with main_content

        Returns:
            Updated state with discovered_documents field
        """
        llm = self.model.llm()
        main_content = state.get("main_content")
        url = state["url"]

        if not main_content:
            logger.error("ãƒ¡ã‚¤ãƒ³ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ãŒç©ºã®ãŸã‚ã€é–¢é€£è³‡æ–™ã‚’ç™ºè¦‹ã§ãã¾ã›ã‚“")
            return {"discovered_documents": []}

        logger.info("é–¢é€£è³‡æ–™ã‚’ç™ºè¦‹ä¸­...")

        parser = JsonOutputParser(pydantic_object=CandidateReportList)

        system_prompt = SystemMessagePromptTemplate.from_template(
            """ã‚ãªãŸã¯ãƒžãƒ¼ã‚¯ãƒ€ã‚¦ãƒ³ã‹ã‚‰é–¢é€£è³‡æ–™ã®ãƒªãƒ³ã‚¯ã‚’æŠ½å‡ºã™ã‚‹å°‚é–€å®¶ã§ã™ã€‚

# å½¹å‰²
ä¼šè­°ãƒšãƒ¼ã‚¸ã®ãƒ¡ã‚¤ãƒ³ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‹ã‚‰ã€è¦ç´„å¯¾è±¡ã¨ãªã‚‹é–¢é€£è³‡æ–™ï¼ˆPDFã€Wordæ–‡æ›¸ãªã©ï¼‰ã‚’
æ­£ç¢ºã«ç‰¹å®šã—ã€ä¸è¦ãªãƒªãƒ³ã‚¯ï¼ˆãƒŠãƒ“ã‚²ãƒ¼ã‚·ãƒ§ãƒ³ã€å¤–éƒ¨ã‚µã‚¤ãƒˆãªã©ï¼‰ã‚’é™¤å¤–ã—ã¦ãã ã•ã„ã€‚

# åˆ¤å®šæ‰‹é †

ã‚¹ãƒ†ãƒƒãƒ—1: ã™ã¹ã¦ã®ãƒªãƒ³ã‚¯ã‚’æŠ½å‡ºã™ã‚‹
- ãƒžãƒ¼ã‚¯ãƒ€ã‚¦ãƒ³å†…ã®ã™ã¹ã¦ã®ãƒªãƒ³ã‚¯ã‚’æ¼ã‚ŒãªãæŠ½å‡º
- ãƒªãƒ³ã‚¯å…ˆURLã¨ãƒªãƒ³ã‚¯ãƒ†ã‚­ã‚¹ãƒˆã‚’å–å¾—

ã‚¹ãƒ†ãƒƒãƒ—2: å„ãƒªãƒ³ã‚¯ã‚’åˆ¤å®šã™ã‚‹
ä»¥ä¸‹ã®5ã¤ã®åŸºæº–ã§é †ç•ªã«ç¢ºèªï¼š

**åŸºæº–1: é–¢é€£è³‡æ–™ã‹ï¼Ÿ**
âœ… ä»¥ä¸‹ã¯é–¢é€£è³‡æ–™ï¼š
- ä¼šè­°ã®è­°äº‹éŒ²ã€å ±å‘Šæ›¸ã€é…ä»˜è³‡æ–™
- ã¨ã‚Šã¾ã¨ã‚ã®æœ¬æ–‡ãƒ»æ¦‚è¦
- æ§‹æˆå“¡ä¸€è¦§ã€ç›®æ¬¡ã€ç´¢å¼•
- æ¡ˆå†…ãƒ»ãŠçŸ¥ã‚‰ã›ãƒ»å‹Ÿé›†ã®æœ¬æ–‡

âŒ ä»¥ä¸‹ã¯é–¢é€£è³‡æ–™ã§ã¯ãªã„ï¼š
- ãƒ—ãƒ©ã‚¤ãƒã‚·ãƒ¼ãƒãƒªã‚·ãƒ¼ã€ã‚µã‚¤ãƒˆãƒžãƒƒãƒ—
- YouTubeã€å‹•ç”»ãƒ•ã‚¡ã‚¤ãƒ«ï¼ˆmp4ã€aviï¼‰
- NDL Warpï¼ˆå›½ç«‹å›½ä¼šå›³æ›¸é¤¨ï¼‰
- ä¸€èˆ¬çš„ãªæ¡ˆå†…ãƒ»ãŠçŸ¥ã‚‰ã›

**åŸºæº–2: ä¼šè­°è³‡æ–™ãƒ»è£œè¶³è³‡æ–™ã‹ï¼Ÿ**
- ä¼šè­°ã§ä½¿ç”¨ã•ã‚ŒãŸè³‡æ–™
- å‚è€ƒè³‡æ–™ã€è¿½åŠ è³‡æ–™

**åŸºæº–3: ãƒŠãƒ“ã‚²ãƒ¼ã‚·ãƒ§ãƒ³è¦ç´ ã§ã¯ãªã„ã‹ï¼Ÿ**
- ãƒ˜ãƒƒãƒ€ãƒ¼ã€ãƒ•ãƒƒã‚¿ãƒ¼ã€ãƒ¡ãƒ‹ãƒ¥ãƒ¼
- ãƒ‘ãƒ³ããšãƒªã‚¹ãƒˆ
- ã‚µã‚¤ãƒ‰ãƒãƒ¼ã®ãƒªãƒ³ã‚¯

**åŸºæº–4: ç›¸å¯¾ãƒ‘ã‚¹ã®å‡¦ç†**
- ç›¸å¯¾ãƒ‘ã‚¹ã¯çµ¶å¯¾URLã«å¤‰æ›
- ãƒ™ãƒ¼ã‚¹URL: {url}

ã‚¹ãƒ†ãƒƒãƒ—3: å‡ºåŠ›
ã™ã¹ã¦ã®ãƒªãƒ³ã‚¯ã«ã¤ã„ã¦ä»¥ä¸‹ã‚’è¨˜è¿°ï¼š
- URLï¼ˆçµ¶å¯¾ãƒ‘ã‚¹ï¼‰
- ãƒªãƒ³ã‚¯ãƒ†ã‚­ã‚¹ãƒˆ
- åˆ¤å®šçµæžœï¼ˆtrue/falseï¼‰
- åˆ¤æ–­ç†ç”±ï¼ˆå…·ä½“çš„ã«ï¼‰

# åˆ¶ç´„äº‹é …
- ã™ã¹ã¦ã®ãƒªãƒ³ã‚¯ã‚’æ¼ã‚Œãªãå‡ºåŠ›
- åˆ¤å®šç†ç”±ã¯å…·ä½“çš„ã«è¨˜è¿°
- ä¸ç¢ºã‹ãªå ´åˆã¯åŽ³å¯†ã«åˆ¤æ–­
            """
        )

        assistant_prompt = AIMessagePromptTemplate.from_template(
            """ä»¥ä¸‹ã®ãƒ¡ã‚¤ãƒ³ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‹ã‚‰é–¢é€£è³‡æ–™ã®ãƒªãƒ³ã‚¯ã‚’æŠ½å‡ºã—ã¦ãã ã•ã„ã€‚

# ãƒ¡ã‚¤ãƒ³ã‚³ãƒ³ãƒ†ãƒ³ãƒ„
{main_content}

# å‡¦ç†æ‰‹é †
1. ã™ã¹ã¦ã®ãƒªãƒ³ã‚¯ã‚’æŠ½å‡º
2. 5ã¤ã®åŸºæº–ã§åˆ¤å®šï¼ˆé–¢é€£è³‡æ–™ã€ä¼šè­°è³‡æ–™ã€ãƒŠãƒ“ã‚²ãƒ¼ã‚·ãƒ§ãƒ³ã€ç›¸å¯¾ãƒ‘ã‚¹ï¼‰
3. åˆ¤å®šçµæžœã¨ç†ç”±ã‚’å‡ºåŠ›

# å‡ºåŠ›ãƒ•ã‚©ãƒ¼ãƒžãƒƒãƒˆ
{format_instructions}
            """
        )

        prompt = ChatPromptTemplate.from_messages([system_prompt, assistant_prompt])

        chain = prompt | llm | parser

        try:
            result = chain.invoke({
                "url": url,
                "main_content": main_content,
                "format_instructions": parser.get_format_instructions()
            })

            # Extract document URLs (only those marked as related)
            discovered_urls = []
            if hasattr(result, "reports"):
                for report in result.reports:
                    if report.is_related_document:
                        # Convert relative URLs to absolute
                        absolute_url = urllib.parse.urljoin(url, report.url)
                        discovered_urls.append(absolute_url)

            logger.info(f"é–¢é€£è³‡æ–™ã‚’{len(discovered_urls)}ä»¶ç™ºè¦‹ã—ã¾ã—ãŸ")

            return {"discovered_documents": discovered_urls}

        except Exception as e:
            logger.error(f"é–¢é€£è³‡æ–™ç™ºè¦‹ä¸­ã«ã‚¨ãƒ©ãƒ¼: {e}")
            return {"discovered_documents": []}

    def invoke(self, input_data: dict) -> dict:
        """
        Execute HTML processing.

        Args:
            input_data: Dict with keys:
                - url: str - HTML page URL

        Returns:
            Dict with keys:
                - markdown: str | None - Converted markdown
                - main_content: str | None - Extracted main content
                - discovered_documents: list[str] - URLs of related documents
        """
        compiled = self.graph.compile()
        result = compiled.invoke(input_data)
        return result
